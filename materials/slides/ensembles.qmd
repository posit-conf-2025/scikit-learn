---
title: "Tree-based and ensemble models"
format: 
  live-revealjs:
    slide-number: true
    theme: [default, styles.scss]
    incremental: true
    scrollable: true
resources:
  - data
jupyter: python3
execute:
  echo: true
  warning: false
---

## Tree-based methods {.smaller}

```{pyodide}
#| include: false
#| autorun: true
import pandas as pd
pd.set_option('display.max_rows', 5)
```

::: {.nonincremental}
- Algorithms that segment the predictor space into a number of simple regions.
  
- Named decision trees because they can be summarized as a tree.

:::: {.columns}
::: {.column width="50%"}
![](img/ensembles/decision-tree-regions.png){width=75%}
::: 
::: {.column width="50%"}
![](img/ensembles/decision-tree.png){width=75%}
:::
::::

Source: [*An Introduction to Statistical Learning with Applications in Python*](https://www.statlearning.com/) by James, Witten, Hastie, Tibshirani & Taylor

:::

## Tree-based methods

- Decision trees on their own, are very explainable and intuitive,
  but not very powerful at predicting.

- However, there are extensions of decision trees,
  such as random forest and boosted trees,
  which are very powerful at predicting.
  We will demonstrate two of these in this session.

## Decision trees

::: {.nonincremental}
- Excellent [decision Trees](https://mlu-explain.github.io/decision-tree/) explainer web app
  by Jared Wilber & Lucía Santamaría
  
![](img/ensembles/mlu-explain-dt.png){width=75%}
:::

## Classification Decision trees

::: {.nonincremental}
- Use recursive binary splitting to grow a classification tree
  (splitting of the predictor space into $J$ distinct, non-overlapping regions).

![](img/ensembles/mlu-explain-dt.png){width=75%}
:::

## Classification Decision trees

::: {.nonincremental}
- For every observation that falls into the region $R_j$ ,
  we make the same prediction,
  which is the majority vote for the training observations in $R_j$.

![](img/ensembles/mlu-explain-dt.png){width=75%}
:::

## Classification Decision trees

::: {.nonincremental}
- Where to split the predictor space is done in a top-down and greedy manner,
  and in practice for classification, the best split at any point in the algorithm
  is one that minimizes the Gini index (a measure of node purity).

![](img/ensembles/mlu-explain-dt.png){width=75%}
:::

## Classification Decision trees

::: {.nonincremental}
- A limitation of decision trees is that they tend to overfit,
  so in practice we use cross-validation to tune a hyperparameter,
  $\alpha$, to find the optimal, pruned tree.

![](img/ensembles/mlu-explain-dt.png){width=75%}
:::

## Example: the heart data set

::: {.nonincremental}
- Let's consider a situation where we'd like to be able to predict
  the presence of heart disease (`AHD`) in patients,
  based off 13 measured characteristics.

- The [heart data set](https://www.statlearning.com/s/Heart.csv)
  contains a binary outcome for heart disease
  for patients who presented with chest pain.
:::

## Example: the heart data set (cont'd) {.smaller}

An angiographic test was performed and a label for `AHD` of Yes
was labelled to indicate the presence of heart disease,
otherwise the label was No.

```{pyodide}
import pandas as pd


heart = pd.read_csv("data/Heart.csv", index_col=0)
heart.head()
```

## Do we have a class imbalance? {.smaller}

It's always important to check this, as it may impact your splitting
and/or modeling decisions.

```{pyodide}
heart['AHD'].value_counts(normalize=True)
```

## Data splitting {.smaller}

Let's split the data into training and test sets:

```{pyodide}
import numpy as np
from sklearn.model_selection import train_test_split

np.random.seed(2025)

heart_train, heart_test = train_test_split(
    heart, train_size=0.8, stratify=heart["AHD"]
)

X_train = heart_train.drop(columns=['AHD'])
y_train = heart_train['AHD']
X_test = heart_test.drop(columns=['AHD'])
y_test = heart_test['AHD']
```

## Categorical variables

:::: {.columns}
::: {.column width="35%"}
::: {.nonincremental}
- This is our first case of seeing categorical predictor variables,
can we treat them the same as numerical ones? **No!**

- In `scikit-learn` we must perform **one-hot encoding**
:::
:::

::: {.column width="65%"}
![](img/ensembles/onehot1-1-1.png)

*Source: <https://scales.arabpsychology.com/stats/how-can-i-perform-one-hot-encoding-in-r/>*
:::
::::


## Look at the data again {.smaller}

Which columns do we need to standardize?

Which do we need to one-hot encode?

```{pyodide}
heart.head()
```

## One hot encoding & pre-processing {.smaller}

```{pyodide}
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer, make_column_selector

numeric_feats = ['Age', 'Chol', 'RestECG', 'MaxHR', 'Oldpeak','Slope', 'Ca']
passthrough_feats = ['Sex', 'Fbs', 'ExAng']
categorical_feats = ['ChestPain', 'Thal', 'RestECG']

heart_preprocessor = make_column_transformer(
    (StandardScaler(), numeric_feats),
    ("passthrough", passthrough_feats),
    (OneHotEncoder(handle_unknown = "ignore"), categorical_feats),
)
```

> `handle_unknown = "ignore"` handles the case where
> categories exist in the test data, which were missing in the training set.
> Specifically, it sets the value for those to 0 for all cases of the category.

## Fitting a dummy classifier {.smaller}

```{pyodide}
from sklearn.dummy import DummyClassifier
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_validate

dummy = DummyClassifier()
dummy_pipeline = make_pipeline(heart_preprocessor, dummy)
cv_10_dummy = pd.DataFrame(
    cross_validate(
        estimator=dummy_pipeline,
        cv=10,
        X=X_train,
        y=y_train
    )
)
```

## Put the mean cross-validated error in a data frame {.smaller}

```{pyodide}
cv_10_dummy_metrics = cv_10_dummy.agg(["mean"])
results = pd.DataFrame({'mean' : [cv_10_dummy_metrics.test_score.iloc[0]]},
  index = ['Dummy classifier']
)
results
```


## Fitting a decision tree {.smaller}

```{pyodide}
from sklearn.tree import DecisionTreeClassifier

decision_tree = DecisionTreeClassifier(random_state=2026)

dt_pipeline = make_pipeline(heart_preprocessor, decision_tree)
cv_10_dt = pd.DataFrame(
    cross_validate(
        estimator=dt_pipeline,
        cv=10,
        X=X_train,
        y=y_train
    )
)
```

## Add the mean cross-validated error to our results data frame {.smaller}

```{pyodide}
cv_10_dt_metrics = cv_10_dt.agg(["mean"])
results_dt = pd.DataFrame({'mean' : [cv_10_dt_metrics.test_score.iloc[0]]},
  index = ['Decision tree']
)
results = pd.concat([results, results_dt])
results
```

## Can we do better?

- We could tune some decision tree parameters
  (e.g., alpha, maximum tree depth, etc)...

- We could also try a different tree-based method!

- Another great explainer: [the Random Forest Algorithm](https://mlu-explain.github.io/random-forest/)
  by Jenny Yeon & Jared Wilber

## The Random Forest Algorithm

1. Build a number of decision trees on bootstrapped training samples.

2. When building the trees from the bootstrapped samples,
  at each stage of splitting,
  the best splitting is computed using a randomly selected subset of the features.

3. Take the majority votes across all the trees for the final prediction.

## Random forest in `scikit-learn` {.smaller}

```{pyodide}
from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier(random_state=2025)
rf_pipeline = make_pipeline(heart_preprocessor, random_forest)
cv_10_rf = pd.DataFrame(
    cross_validate(
        estimator=rf_pipeline,
        cv=10,
        X=X_train,
        y=y_train
    )
)
```

## Add the mean cross-validated error to our results data frame {.smaller}

```{pyodide}
cv_10_rf_metrics = cv_10_rf.agg(["mean"])
results_rf = pd.DataFrame({'mean' : [cv_10_rf_metrics.test_score.iloc[0]]},
  index = ['Random forest']
)
results = pd.concat([results, results_rf])
results
```

## Can we do better?

- Random forest can be tuned a several important parameters, including:

  - `n_estimators`: number of decision trees (higher = more complexity)

  - `max_depth`: max depth of each decision tree (higher = more complexity)

  - `max_features`: the number of features you get to look at each split
  (higher = more complexity)

- We can use `GridSearchCV` to search for the optimal parameters for these,
  as we did for $K$ in $K$-nearest neighbors.

## Tuning random forest in `scikit-learn` {.smaller}

```{pyodide}
from sklearn.model_selection import GridSearchCV

rf_param_grid = {'randomforestclassifier__n_estimators': [50],
              'randomforestclassifier__max_depth': [1, 3, 5, 7, 9],
              'randomforestclassifier__max_features': [1, 2, 3, 4, 5, 6, 7]}

rf_tune_grid = GridSearchCV(
    estimator=rf_pipeline,
    param_grid=rf_param_grid,
    cv=10,
    n_jobs=-1 # tells computer to use all available CPUs
)
rf_tune_grid.fit(
    X_train,
    y_train
)
```

## Comparing to our other models {.smaller}

How did the tuned Random Forest compare
against the other models we tried?

```{pyodide}
cv_10_rf_tuned_metrics = pd.DataFrame(rf_tune_grid.cv_results_)
results_rf_tuned = pd.DataFrame({'mean' : rf_tune_grid.best_score_},
  index = ['Random forest tuned']
)
results = pd.concat([results, results_rf_tuned])
results
```

## Boosting

- No randomization.

- The key idea is combining many simple models called weak learners,
  to create a strong learner.

- They combine multiple shallow (depth 1 to 5) decision trees.

- They build trees in a serial manner,
  where each tree tries to correct the mistakes of the previous one.

## Tuning Boosted Classifiers

- `HistGradientBoostingClassifier` can be tuned a several important parameters, including:

  - `max_iter`: number of decision trees (higher = more complexity)

  - `max_depth`: max depth of each decision tree (higher = more complexity)

  - `learning_rate`: the shrinkage parameter which controls the rate
  at which boosting learns. Values between 0.01 or 0.001 are typical.

- We can use `GridSearchCV` to search for the optimal parameters for these,
  as we did for the parameters in Random Forest.

## Tuning Boosted Classifiers (cont'd) {.smaller}

```{pyodide}
from sklearn.ensemble import HistGradientBoostingClassifier

gradient_boosted_classifier = HistGradientBoostingClassifier(random_state=2025)
gb_pipeline = make_pipeline(heart_preprocessor, gradient_boosted_classifier)
gb_param_grid = {'histgradientboostingclassifier__max_iter': [200],
              'histgradientboostingclassifier__max_depth': [1, 3, 5, 7, 9],
              'histgradientboostingclassifier__learning_rate': [0.001, 0.005, 0.01]}
gb_tune_grid = GridSearchCV(
    estimator=gb_pipeline,
    param_grid=gb_param_grid,
    cv=10,
    n_jobs=-1 # tells computer to use all available CPUs
)
gb_tune_grid.fit(
    X_train,
    y_train
)
```

## Boosted Classifiers results

How did the `HistGradientBoostingClassifier` compare
against the other models we tried?

```{pyodide}
cv_10_gb_tuned_metrics = pd.DataFrame(gb_tune_grid.cv_results_)
results_gb_tuned = pd.DataFrame({'mean' : gb_tune_grid.best_score_},
  index = ['Histogram Gradient boosted classifier tuned']
)
results = pd.concat([results, results_gb_tuned])
results
```

## How do we choose the final model?

- Remember, what is your question or application?

- A good rule when models are not very different,
  what is the simplest model that does well?

- Look at other metrics that are important to you
  (not just the metric you used for tuning your model),
  remember precision & recall, for example.

- Remember - no peaking at the test set until you choose!
  And then, you should only look at the test set for one model!

## Precision and recall {.smaller}

```{pyodide}
from sklearn.metrics import make_scorer, precision_score, recall_score

scoring = {
    'accuracy': 'accuracy',
    'precision': make_scorer(precision_score, pos_label='Yes'),
    'recall': make_scorer(recall_score, pos_label='Yes')
}

rf_tune_grid = GridSearchCV(
    estimator=rf_pipeline,
    param_grid=rf_param_grid,
    cv=10,
    n_jobs=-1,
    scoring=scoring,
    refit='accuracy'
)

rf_tune_grid.fit(X_train, y_train)
```

## Precision and recall cont'd {.smaller}

::: {.nonincremental}
- What do we think? Is this model ready for production in a diagnostic setting?

- How could we improve it further?

```{pyodide}
cv_results = pd.DataFrame(rf_tune_grid.cv_results_)

results_rf_tuned = pd.DataFrame({
    'mean': [rf_tune_grid.best_score_,
             cv_results['mean_test_precision'].iloc[rf_tune_grid.best_index_,
             cv_results['mean_test_recall'].iloc[rf_tune_grid.best_index_],
}, index=['accuracy', 'precision', 'recall'])
results_rf_tuned
```
:::

## Feature importances: key points

- Decision trees are very interpretable (decision rules!), however in ensemble
  models (e.g., Random Forest and Boosting) there are many trees -
  individual decision rules are not as meaningful...

- Instead, we can calculate feature importances as
  the total decrease in impurity for all splits involving that feature,
  weighted by the number of samples involved in those splits,
  normalized and averaged over all the trees.

- These are calculated on the training set,
  as that is the set the model is trained on.

## Feature importances: Notes of caution!

- Feature importances can be unreliable with both highly cardinal,
  and multicollinear features.

- Unlike the linear model coefficients, feature importances do not have a sign!
  They tell us about importance, but not an “up or down”.

- Increasing a feature may cause the prediction to first go up, and then go down.

- Alternatives to feature importance to understanding models exist, 
such as post-hoc explanations (sometimes called "explainable AI", 
see [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) 
by Christoph Molnar for an introduction)

## Feature importances in `scikit-learn`

```{pyodide}
# Access the best pipeline
best_pipeline = rf_tune_grid.best_estimator_

# Extract the trained RandomForestClassifier from the pipeline
best_rf = best_pipeline.named_steps['randomforestclassifier']

# Extract feature names after preprocessing
# Get the names of features from each transformer in the pipeline
numeric_features = numeric_feats
categorical_feature_names = best_pipeline.named_steps['columntransformer'].transformers_[2][1].get_feature_names_out(categorical_feats)
passthrough_features = passthrough_feats

# Combine all feature names into a single list
feature_names = np.concatenate([numeric_features, passthrough_features, categorical_feature_names])
```

## Feature importances in `scikit-learn` (cont'd) {.smaller}

```{pyodide}
# Calculate feature importances
feature_importances = best_rf.feature_importances_

# Create a DataFrame to display feature importances
importances_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
})

# Sort by importance (descending order)
importances_df = importances_df.sort_values(by='Importance', ascending=False)
importances_df.head()
```

## Visualizing the results

```{pyodide}
import altair as alt

bar_chart = alt.Chart(importances_df).mark_bar().encode(
    x=alt.X('Importance:Q', title='Feature Importance'),
    y=alt.Y('Feature:N', sort='-x', title='Feature'),
    tooltip=['Feature', 'Importance']
).properties(
    title='Feature Importances from Random Forest Model',
    width=600,
    height=400
)
```

## Visualizing the results

```{pyodide}
bar_chart
```

## Evaluating on the test set {.smaller}

Predict on the test set:

```{pyodide}
heart_test_drop_na = heart_test.dropna()
X_test = heart_test.drop(columns=['AHD'])
y_test = heart_test['AHD']

heart_test["predicted"] = rf_tune_grid.predict(
    X_test
)

heart_test.head()
```

## Evaluating on the test set

Examine accuracy, precision and recall:

```{pyodide}
accuracy = rf_tune_grid.score(
    X_test, y_test
)

precision = precision_score(
    y_true=heart_test["AHD"], y_pred=heart_test["predicted"],
    pos_label='Yes'
)

recall = recall_score(
    y_true=heart_test["AHD"], y_pred=heart_test["predicted"],
    pos_label='Yes'
)

print(f"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}")
```

## Evaluating on the test set {.smaller}

```{pyodide}
conf_matrix = pd.crosstab(
    heart_test["AHD"],
    heart_test["predicted"]
)
conf_matrix
```

## Keep learning!

:::: {.columns}
::: {.column width="50%"}
![](img/frontmatter/ds-a-first-intro-cover-python.jpeg){height=25%}
<https://python.datasciencebook.ca/>
:::


::: {.column width="50%"}
![](img/frontmatter/ISLP_cover.png){height=25%}
<https://www.statlearning.com/>
:::
::::

## Local installation

::: {.nonincremental}
1. Using Docker:
[Data Science: A First Introduction (Python Edition) Installation Instructions](https://python.datasciencebook.ca/setup.html)

2. Using conda:
[UBC MDS Installation Instructions](https://ubc-mds.github.io/resources_pages/installation_instructions/)
:::

## Additional resources

::: {.nonincremental}
- The [UBC DSCI 573 (Feature and Model Selection notes)](https://ubc-mds.github.io/DSCI_573_feat-model-select)
  chapter of Data Science: A First Introduction (Python Edition) by
  Varada Kolhatkar and Joel Ostblom. These notes cover classification and regression metrics,
  advanced variable selection and more on ensembles.
- The [`scikit-learn` website](https://scikit-learn.org/stable/) is an excellent
  reference for more details on, and advanced usage of, the functions and
  packages in the past two chapters. Aside from that, it also offers many
  useful [tutorials](https://scikit-learn.org/stable/tutorial/index.html)
  to get you started.
- [*An Introduction to Statistical Learning*](https://www.statlearning.com/) {cite:p}`james2013introduction` provides
  a great next stop in the process of
  learning about classification. Chapter 4 discusses additional basic techniques
  for classification that we do not cover, such as logistic regression, linear
  discriminant analysis, and naive Bayes.
:::

## References

::: {.nonincremental}
Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani and Jonathan Taylor. An Introduction to Statistical Learning with Applications in Python. Springer, 1st edition, 2023. URL: https://www.statlearning.com/.

Kolhatkar, V., and Ostblom, J. (2024). UBC DSCI 573: Feature and Model Selection course notes. URL: https://ubc-mds.github.io/DSCI_573_feat-model-select

Pedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825–2830.
:::

## Questions?

![](img/ensembles/close-up-portrait-woman-question.jpg)
