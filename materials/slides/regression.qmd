---
title: "Regression"
subtitle: "K-NN Regression, Linear Regression and L1 regularization"
format:
  revealjs:
    slide-number: true
    smaller: true
    theme: [default, styles.scss]
jupyter: python3

execute:
  echo: true
  warning: false

editor:
  render-on-save: true
---

```{python}
#| include: false
import pandas as pd
pd.set_option('display.max_rows', 5)
```

## Session learning objectives

- Recognize situations where a regression analysis would be appropriate for making predictions.
- Explain the K-nearest neighbors (K-NN) regression algorithm and describe how it differs from K-NN classification.
- Describe the advantages and disadvantages of K-nearest neighbors and linear regression.
- Use Python to fit linear regression models on training data.
- Evaluate the linear regression model on test data.
- Describe how linear regression is affected by outliers and multicollinearity.
- Learn how to apply LASSO regression (L1 regularization) for feature selection and regularization to improve model performance.



## The regression problem

- Predictive problem
- Use past information to predict future observations
- Predict *numerical* values instead of *categorical* values

Examples:

- Race time in the Boston marathon
- size of a house to predict its sale price

## Regression Methods

In this workshop:

- K-nearest neighbors (brief overview)
- Linear regression
- L1 regularization

## Classification similarities to regression

Concepts from classification map over to the setting of regression

- Predict a new observation's response variable based on past observations
- Split the data into training and test sets
- Use cross-validation to evaluate different choices of model parameters

## Difference

Predicting numerical variables instead of categorical variables

## Explore a data set

[932 real estate transactions in Sacramento, California](https://support.spatialkey.com/spatialkey-sample-csv-data/)

> Can we use the size of a house in the Sacramento, CA area to predict its sale price?

## Data and package setup

```{python}
import altair as alt
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import set_config

# Output dataframes instead of arrays
set_config(transform_output='pandas')

sacramento = pd.read_csv('data/sacramento.csv')
print(sacramento)

```

## Price vs Sq.Ft

```{python}
#| echo: false

scatter = alt.Chart(sacramento).mark_circle().encode(
    x=alt.X("sqft")
        .scale(zero=False)
        .title("House size (square feet)"),
    y=alt.Y("price")
        .axis(format="$,.0f")
        .title("Price (USD)")
)

scatter

```

## Sample of data

```{python}
# look at a small sample of data
np.random.seed(10)

small_sacramento = sacramento.sample(n=30)
print(small_sacramento)
```

## Sample: K-NN Example

::: {.columns}
::: {.column}

House price of 2000

```{python}
# | echo: false

small_plot = (
	alt.Chart(small_sacramento)
	.mark_circle(opacity=1)
	.encode(
		x=alt.X('sqft').scale(zero=False).title('House size (square feet)'),
		y=alt.Y('price').axis(format='$,.0f').title('Price (USD)'),
	)
)

# add an overlay to the base plot
line_df = pd.DataFrame({'x': [2000]})
rule = (
	alt.Chart(line_df)
	.mark_rule(strokeDash=[6], size=1.5, color='black')
	.encode(x='x')
)

(small_plot + rule)
```

:::
::: {.column}

5 closest neighbors

```{python}
# | echo: false

small_sacramento['dist'] = (2000 - small_sacramento['sqft']).abs()
nearest_neighbors = small_sacramento.nsmallest(5, 'dist')
nearest_neighbors

nn_plot = small_plot + rule

# plot horizontal lines which is perpendicular to x=2000
h_lines = []
for i in range(5):
	h_line_df = pd.DataFrame(
		{
			'sqft': [nearest_neighbors.iloc[i, 4], 2000],
			'price': [nearest_neighbors.iloc[i, 6]] * 2,
		}
	)
	h_lines.append(
		alt.Chart(h_line_df)
		.mark_line(color='black')
		.encode(x='sqft', y='price')
	)

# highlight the nearest neighbors in orange
orange_neighbrs = (
	alt.Chart(nearest_neighbors)
	.mark_circle(opacity=1, color='#ff7f0e')
	.encode(
		x=alt.X('sqft').scale(zero=False).title('House size (square feet)'),
		y=alt.Y('price').axis(format='$,.0f').title('Price (USD)'),
	)
)

nn_plot = alt.layer(*h_lines, small_plot, orange_neighbrs, rule)

nn_plot
```

:::
:::

## Make and visualize prediction

```{python}
small_sacramento['dist'] = (2000 - small_sacramento['sqft']).abs()
nearest_neighbors = small_sacramento.nsmallest(5, 'dist')
prediction = nearest_neighbors['price'].mean()
print(prediction)
```


```{python}
# | echo: false

nn_plot_pred = nn_plot + alt.Chart(
    pd.DataFrame({"sqft": [2000], "price": [prediction]})
).mark_circle(size=80, opacity=1, color="#d62728").encode(x="sqft", y="price")

nn_plot_pred
```


## K-NN regression in `scikit-learn`

We wonâ€™t be covering K-NN regression in detail, as the process is very similar to K-NN classification. The main differences are summarized below:

| Feature               | K-NN Classification               | K-NN Regression                          |
|-----------------------|-----------------------------------|-------------------------------------------|
| **Target Type**       | Categorical (e.g., 'A', 'B')      | Continuous (e.g., 3.5, 27.0)              |
| **Prediction Logic**  | Majority vote                     | Mean (or weighted average)                |
| **scikit-learn Class**| `KNeighborsClassifier`            | `KNeighborsRegressor`                     |
| **Output**            | Class label                       | Numeric value                             |
| **Use Case**          | Classification problems           | Regression problems                       |
| **Metrics**           | Accuracy, Precision, Recall etc.                | RMS(P)E (details to follow)   


## Splitting the data
 
```{python}
np.random.seed(1)

sacramento_train, sacramento_test = train_test_split(
    sacramento, train_size=0.75
)
```

:::{.callout-note}
We are not specifying the stratify argument.
The `train_test_split()` function cannot stratify on a quantitative variable
:::

## Metric: RMS(P)E

Root Mean Square (Prediction) Error

$$\text{RMSPE} = \sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2}$$

where:

- $n$ is the number of observations,
- $y_i$ is the observed value for the $i^\text{th}$ observation, and
- $\hat{y}_i$ is the forecasted/predicted value for the $i^\text{th}$ observation.

## Metric: Visualize

```{python}
# | echo: false


from sklearn.neighbors import KNeighborsRegressor

# (synthetic) new prediction points
pts = pd.DataFrame(
	{'sqft': [1200, 1850, 2250], 'price': [300000, 200000, 500000]}
)
finegrid = pd.DataFrame({'sqft': np.arange(600, 3901, 10)})

# preprocess the data, make the pipeline
sacr_preprocessor = make_column_transformer((StandardScaler(), ['sqft']))
sacr_pipeline = make_pipeline(
	sacr_preprocessor, KNeighborsRegressor(n_neighbors=4)
)

# fit the model
X = small_sacramento[['sqft']]
y = small_sacramento[['price']]
sacr_pipeline.fit(X, y)

# predict on the full grid and new data pts
sacr_full_preds_hid = pd.concat(
	(
		finegrid,
		pd.DataFrame(sacr_pipeline.predict(finegrid), columns=['predicted']),
	),
	axis=1,
)

sacr_new_preds_hid = pd.concat(
	(
		small_sacramento[['sqft', 'price']].reset_index(),
		pd.DataFrame(
			sacr_pipeline.predict(small_sacramento[['sqft', 'price']]),
			columns=['predicted'],
		),
	),
	axis=1,
).drop(columns=['index'])

# to make altair mark_line works, need to create separate dataframes for each vertical error line
errors_plot = (
	small_plot
	+ alt.Chart(sacr_full_preds_hid)
	.mark_line(color='#ff7f0e')
	.encode(x='sqft', y='predicted')
	+ alt.Chart(sacr_new_preds_hid)
	.mark_circle(opacity=1)
	.encode(x='sqft', y='price')
)
sacr_new_preds_melted_df = sacr_new_preds_hid.melt(id_vars=['sqft'])
v_lines = []
for i in sacr_new_preds_hid['sqft']:
	line_df = sacr_new_preds_melted_df.query(f'sqft == {i}')
	v_lines.append(
		alt.Chart(line_df).mark_line(color='black').encode(x='sqft', y='value')
	)

errors_plot = alt.layer(*v_lines, errors_plot)
errors_plot
```

## RMSPE vs RMSE

Root Mean Square (Prediction) Error

- RMSPE: the error calculated on the non-training dataset
- RMSE: the error calcualted on the training dataset

This notation is a statistics distinction,
you will most likely see RMSPE written as RMSE.

## Choosing $k$

As we did previously, we can use cross-validation and select the value of $k$ that yields the lowest RMSE to choose the best model.

```{python}
# | echo: false

from sklearn.neighbors import KNeighborsRegressor

# preprocess the data, make the pipeline
sacr_preprocessor = make_column_transformer((StandardScaler(), ['sqft']))
sacr_pipeline = make_pipeline(sacr_preprocessor, KNeighborsRegressor())

# create the 5-fold GridSearchCV object
param_grid = {
	'kneighborsregressor__n_neighbors': range(1, 201, 3),
}
sacr_gridsearch = GridSearchCV(
	estimator=sacr_pipeline,
	param_grid=param_grid,
	cv=5,
	scoring='neg_root_mean_squared_error',  # we will deal with this later
)

# fit the GridSearchCV object
sacr_gridsearch.fit(
	sacramento_train[['sqft']],  # A single-column data frame
	sacramento_train['price'],  # A series
)

# sacr_gridsearch.best_params_

# Retrieve the CV scores
sacr_results = pd.DataFrame(sacr_gridsearch.cv_results_)
sacr_results = sacr_results[
	[
		'param_kneighborsregressor__n_neighbors',
		'mean_test_score'
	]
].rename(columns={'param_kneighborsregressor__n_neighbors': 'n_neighbors'})


best_k_sacr = sacr_results["n_neighbors"][sacr_results["mean_test_score"].idxmin()]
best_cv_RMSPE = min(sacr_results["mean_test_score"])

gridvals = [
	1,
	3,
	25,
	best_k_sacr,
	250,
	len(sacramento_train),
]

plots = list()

sacr_preprocessor = make_column_transformer((StandardScaler(), ['sqft']))
X = sacramento_train[['sqft']]
y = sacramento_train[['price']]

base_plot = (
	alt.Chart(sacramento_train)
	.mark_circle()
	.encode(
		x=alt.X(
			'sqft',
			title='House size (square feet)',
			scale=alt.Scale(zero=False),
		),
		y=alt.Y('price', title='Price (USD)', axis=alt.Axis(format='$,.0f')),
	)
)
for i in range(len(gridvals)):
	# make the pipeline based on n_neighbors
	sacr_pipeline = make_pipeline(
		sacr_preprocessor, KNeighborsRegressor(n_neighbors=gridvals[i])
	)
	sacr_pipeline.fit(X, y)
	# predictions
	sacr_preds = sacramento_train
	sacr_preds = sacr_preds.assign(
		predicted=sacr_pipeline.predict(sacramento_train)
	)
	# overlay the plots
	plots.append(
		base_plot
		+ alt.Chart(sacr_preds, title=f'K = {gridvals[i]}')
		.mark_line(color='#ff7f0e')
		.encode(x='sqft', y='predicted')
	)

(plots[0] | plots[1] | plots[2]) & (plots[3] | plots[4] | plots[5])
```


## Evaluating on the test set


- Then we retrain the K-NN regression model on the entire training data set using best $k$.

```{python}
from sklearn.metrics import mean_squared_error

sacramento_test['predicted'] = sacr_gridsearch.predict(sacramento_test)
RMSPE = mean_squared_error(
	y_true=sacramento_test['price'], y_pred=sacramento_test['predicted']
) ** (1 / 2)

RMSPE
```

> *The code for running cross-validation to get `sacr_gridsearch` is included at the end of the slides for reference.*

## Final best K model

Predicted values of house price (orange line) for the final K-NN regression model.

```{python}
# | echo: false

# Create a grid of evenly spaced values along the range of the sqft data
sqft_prediction_grid = pd.DataFrame(
	{'sqft': np.arange(sacramento['sqft'].min(), sacramento['sqft'].max(), 10)}
)
# Predict the price for each of the sqft values in the grid
sqft_prediction_grid['predicted'] = sacr_gridsearch.predict(
	sqft_prediction_grid
)

# Plot all the houses
base_plot = (
	alt.Chart(sacramento)
	.mark_circle(opacity=0.4)
	.encode(
		x=alt.X('sqft').scale(zero=False).title('House size (square feet)'),
		y=alt.Y('price').axis(format='$,.0f').title('Price (USD)'),
	)
)

# Add the predictions as a line
sacr_preds_plot = base_plot + alt.Chart(
	sqft_prediction_grid, title=f'K = {best_k_sacr}'
).mark_line(color='#ff7f0e').encode(x='sqft', y='predicted')

sacr_preds_plot
```

> *K-NN regression supports multiple predictors - see the end of the slides for Python code demonstrating multivariable K-NN regression.*

## Strengths and limitations of K-NN regression

Strengths:

- simple, intuitive algorithm
- requires few assumptions about what the data must look like
- works well with non-linear relationships (i.e., if the relationship is not a straight line)

Weaknesses:

- very slow as the training data gets larger
- may not perform well with a large number of predictors
- may not predict well beyond the range of values input in your training data

## Linear Regression

- Addresses the limitations from KNN regression
- Provides an interpretable mathematical equation that describes
the relationship between the predictor and response variables

- Creates a straight line of best fit through the training data

:::{.callout-note}
Logistic regression is the linear model we can use for binary classification
:::

## Sacramento real estate

```{python}
import pandas as pd

sacramento = pd.read_csv('data/sacramento.csv')

np.random.seed(42)
small_sacramento = sacramento.sample(n=30)

print(small_sacramento)
```

## Sacramento real estate: best fit line

::: {.columns}
::: {.column}

```{python}
# | echo: false

small_plot = (
	alt.Chart(small_sacramento)
	.mark_circle(opacity=1)
	.encode(
		x=alt.X('sqft').scale(zero=False).title('House size (square feet)'),
		y=alt.Y('price')
		.axis(format='$,.0f')
		.scale(zero=False)
		.title('Price (USD)'),
	)
)


# create df_lines with one fake/empty line (for starting at 2nd color later)
df_lines = {'x': [500, 500], 'y': [100000, 100000], 'number': ['-1', '-1']}

# set the domains (range of x values) of lines
min_x = small_sacramento['sqft'].min()
max_x = small_sacramento['sqft'].max()

# add the line of best fit
from sklearn.linear_model import LinearRegression

lm = LinearRegression()
lm.fit(small_sacramento[['sqft']], small_sacramento[['price']])
pred_min = float(lm.predict(pd.DataFrame({'sqft': [min_x]})))
pred_max = float(lm.predict(pd.DataFrame({'sqft': [max_x]})))

df_lines['x'].extend([min_x, max_x])
df_lines['y'].extend([pred_min, pred_max])
df_lines['number'].extend(['0', '0'])

# add other similar looking lines
intercept_l = [-64542.23, -6900, -64542.23]
slope_l = [190, 175, 160]
for i in range(len(slope_l)):
	df_lines['x'].extend([min_x, max_x])
	df_lines['y'].extend(
		[
			intercept_l[i] + slope_l[i] * min_x,
			intercept_l[i] + slope_l[i] * max_x,
		]
	)
	df_lines['number'].extend([f'{i+1}', f'{i+1}'])

df_lines = pd.DataFrame(df_lines)

# plot the bogus line to skip the same color as the scatter
small_plot += (
	alt.Chart(df_lines[df_lines['number'] == '-1'])
	.mark_line()
	.encode(x='x', y='y', color=alt.Color('number', legend=None))
)
# plot the real line with 2nd color
small_plot += (
	alt.Chart(df_lines[df_lines['number'] == '0'])
	.mark_line()
	.encode(x='x', y='y', color=alt.Color('number', legend=None))
)

small_plot
```

:::
::: {.column}

The equation for the line is:

$$\text{house sale price} = \beta_0 + \beta_1 \cdot (\text{house size})+\varepsilon,$$
where

- $\beta_0$ is the *vertical intercept* of the line (the price when house size is 0)
- $\beta_1$ is the *slope* of the line (how quickly the price increases as you increase house size)
- $\varepsilon$ is the random error term


:::
:::

## Sacramento real estate: Prediction

```{python}
# | echo: false

from sklearn.linear_model import LinearRegression

lm = LinearRegression()
lm.fit(small_sacramento[['sqft']], small_sacramento[['price']])
prediction = float(lm.predict(pd.DataFrame({'sqft': [2000]})))

# the vertical dotted line
line_df = pd.DataFrame({'x': [2000]})
rule = alt.Chart(line_df).mark_rule(strokeDash=[6], size=1.5).encode(x='x')

# the red point
point_df = pd.DataFrame({'x': [2000], 'y': [prediction]})
point = (
	alt.Chart(point_df)
	.mark_circle(color='red', size=80, opacity=1)
	.encode(x='x', y='y')
)

# overlay all plots
small_plot_2000_pred = (
	small_plot
	+ rule
	+ point
	# add the text
	+ alt.Chart(
		pd.DataFrame(
			{
				'x': [2450],
				'y': [prediction - 41000],
				'prediction': ['$' + '{0:,.0f}'.format(prediction)],
			}
		)
	)
	.mark_text(dy=-5, size=15)
	.encode(x='x', y='y', text='prediction')
)

small_plot_2000_pred

```

## Estimating a line


```{python}
#| echo: false


several_lines_plot = small_plot.copy()

several_lines_plot += alt.Chart(
    df_lines[df_lines["number"] != "0"]
).mark_line().encode(x="x", y="y", color=alt.Color("number",legend=None))

several_lines_plot

```


## What makes the best line?

To define the **best line** we need to know how to measure the distance of the points to the line! 
    
**Which of the following criteria would you choose to define "distance of a point to the line"?** 

![](img/regression/dist.png)

> Figure by Dr. Joel Ostblom

## Least squares

Least Squares method minimizes the sum of the squares of the residuals. The **residuals** are the difference between the observed value of the response ($y_i$) and the predicted value of the response ($\hat{y}_i$):

$$r_i = y_i-\hat{y}_i$$

```{python}
# | echo: false


small_sacramento_pred = small_sacramento
# get prediction
small_sacramento_pred = small_sacramento_pred.assign(
	predicted=lm.predict(small_sacramento[['sqft']])
)
# melt the dataframe to create separate df to create lines
small_sacramento_pred = small_sacramento_pred[
	['sqft', 'price', 'predicted']
].melt(id_vars=['sqft'])

v_lines = []
for i in range(len(small_sacramento)):
	sqft_val = small_sacramento.iloc[i]['sqft']
	line_df = small_sacramento_pred.query('sqft == @sqft_val')
	v_lines.append(
		alt.Chart(line_df).mark_line(color='black').encode(x='sqft', y='value')
	)

error_plot = alt.layer(*v_lines, small_plot).configure_circle(opacity=1)
error_plot

```

> The residuals are the vertical distances of each point to the estimated line

## Sum of squared errors

When estimating the regression equation, we minimize the sum of squared errors (SSE), defined
as

$$SSE = \sum_{i=1}^n (y_i-\hat{y_i})^2=\sum_{i=1}^n r_i^2.$$


## Linear regression in Python

The `scikit-learn` pattern still applies:

- Create a training and test set
- Instantiate a model
- Fit the model on training
- Use model on testing set

## Linear regression: Train test split

```{python}
import numpy as np
import altair as alt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn import set_config

# Output dataframes instead of arrays
set_config(transform_output='pandas')

np.random.seed(1)

sacramento = pd.read_csv('data/sacramento.csv')

sacramento_train, sacramento_test = train_test_split(
	sacramento, train_size=0.75
)

```

## Linear regression: Fit the model

```{python}
# fit the linear regression model
lm = LinearRegression()
lm.fit(
	sacramento_train[['sqft']],  # A single-column data frame
	sacramento_train['price'],  # A series
)

# make a dataframe containing slope and intercept coefficients
results_df = pd.DataFrame({'slope': [lm.coef_[0]], 'intercept': [lm.intercept_]})
print(results_df)
```
<!--TODO: numbers here are hard-coded -->
$\text{house sale price} =$ 137.29 $+$ 15642.31 $\cdot (\text{house size}).$

## Linear regression: Predictions

```{python}
# make predictions
sacramento_test["predicted"] = lm.predict(sacramento_test[["sqft"]])

# calculate RMSPE
RMSPE = mean_squared_error(
    y_true=sacramento_test["price"],
    y_pred=sacramento_test["predicted"]
)**(1/2)

RMSPE
```

## Linear regression: Plot

```{python}
# | echo: false

sqft_prediction_grid = sacramento[['sqft']].agg(['min', 'max'])
sqft_prediction_grid['predicted'] = lm.predict(sqft_prediction_grid)

all_points = (
	alt.Chart(sacramento)
	.mark_circle()
	.encode(
		x=alt.X('sqft').scale(zero=False).title('House size (square feet)'),
		y=alt.Y('price')
		.axis(format='$,.0f')
		.scale(zero=False)
		.title('Price (USD)'),
	)
)

sacr_preds_plot = all_points + alt.Chart(sqft_prediction_grid).mark_line(
	color='#ff7f0e'
).encode(x='sqft', y='predicted')

sacr_preds_plot

```

## Standarization

- We did not need to standarize like we did for KNN.
- In linear regression, if we standarize, we convert all the units to unit-less standard deviations
- Standarization in linear regression does not change the fit of the model
    - It will, however, change the coefficients!

## Comparing simple linear and K-NN regression

```{python}
# | echo: false

from sklearn.model_selection import GridSearchCV
from sklearn.compose import make_column_transformer
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# preprocess the data, make the pipeline
sacr_preprocessor = make_column_transformer((StandardScaler(), ['sqft']))
sacr_pipeline_knn = make_pipeline(
	sacr_preprocessor, KNeighborsRegressor(n_neighbors=55)
)  # 55 is the best parameter obtained through cross validation in regression1 chapter

sacr_pipeline_knn.fit(sacramento_train[['sqft']], sacramento_train[['price']])

# knn in-sample predictions (on training split)
sacr_preds_knn = sacramento_train
sacr_preds_knn = sacr_preds_knn.assign(
	knn_predicted=sacr_pipeline_knn.predict(sacramento_train)
)

# knn out-of-sample predictions (on test split)
sacr_preds_knn_test = sacramento_test
sacr_preds_knn_test = sacr_preds_knn_test.assign(
	knn_predicted=sacr_pipeline_knn.predict(sacramento_test)
)

sacr_rmspe_knn = np.sqrt(
	mean_squared_error(
		y_true=sacr_preds_knn_test['price'],
		y_pred=sacr_preds_knn_test['knn_predicted'],
	)
)

# plot knn in-sample predictions overlaid on scatter plot
knn_plot_final = (
	alt.Chart(sacr_preds_knn, title='K-NN regression')
	.mark_circle()
	.encode(
		x=alt.X(
			'sqft',
			title='House size (square feet)',
			scale=alt.Scale(zero=False),
		),
		y=alt.Y(
			'price',
			title='Price (USD)',
			axis=alt.Axis(format='$,.0f'),
			scale=alt.Scale(zero=False),
		),
	)
)

knn_plot_final = (
	knn_plot_final
	+ knn_plot_final.mark_line(color='#ff7f0e').encode(
		x='sqft', y='knn_predicted'
	)
	+ alt.Chart(  # add the text
		pd.DataFrame(
			{
				'x': [3500],
				'y': [100000],
				'rmspe': [f'RMSPE = {round(sacr_rmspe_knn)}'],
			}
		)
	)
	.mark_text(dy=-5, size=15)
	.encode(x='x', y='y', text='rmspe')
)


# add more components to lm_plot_final
lm_plot_final = (
	alt.Chart(sacramento_train, title='linear regression')
	.mark_circle()
	.encode(
		x=alt.X(
			'sqft',
			title='House size (square feet)',
			scale=alt.Scale(zero=False),
		),
		y=alt.Y(
			'price',
			title='Price (USD)',
			axis=alt.Axis(format='$,.0f'),
			scale=alt.Scale(zero=False),
		),
	)
)

lm_plot_final = (
	lm_plot_final
	+ lm_plot_final.transform_regression('sqft', 'price').mark_line(
		color='#ff7f0e'
	)
	+ alt.Chart(  # add the text
		pd.DataFrame(
			{
				'x': [3500],
				'y': [100000],
				'rmspe': [f'RMSPE = {round(RMSPE)}'],
			}
		)
	)
	.mark_text(dy=-5, size=15)
	.encode(x='x', y='y', text='rmspe')
)

(lm_plot_final | knn_plot_final)
```

## Multiple linear regression

- More predictor variables! (More does not always mean better...)
- When $p$, the number of variables, is greater than 1, we have *multiple* linear regression.

> *Note: We will talk about categorical predictors later in the workshop*

## Sacramento real estate: 2 predictors

```{python}
mlm = LinearRegression()
mlm.fit(sacramento_train[['sqft', 'beds']], sacramento_train['price'])

sacramento_test['predicted'] = mlm.predict(sacramento_test[['sqft', 'beds']])
```

## Sacramento real estate: Coefficients

```{python}
mlm.coef_
```

```{python}
mlm.intercept_
```

$$\text{house sale price} = 53180.27 + 154.59\cdot(\text{house size}) -20333.43\cdot(\text{bedrooms}),$$
where:

- Intercept ($\hat{\beta}_0=53180.27$): Predicted sale price when house size = 0 and bedrooms = 0 (not meaningful in reality, but needed for the equation).

- House size ($\hat{\beta}_1=154.59$): Holding bedrooms constant, each extra unit of size (e.g., square foot) increases the price by $154.59 on average.

- Number of bedrooms ($\hat{\beta}_2=-20333.43$): Holding size constant, each additional bedroom reduces price by about $20,333.

## More variables make it harder to visualize

```{python}
# | echo: false

# create a prediction pt grid
xvals = np.linspace(
	sacramento_train['sqft'].min(), sacramento_train['sqft'].max(), 50
)
yvals = np.linspace(
	sacramento_train['beds'].min(), sacramento_train['beds'].max(), 50
)
xygrid = np.array(np.meshgrid(xvals, yvals)).reshape(2, -1).T
xygrid = pd.DataFrame(xygrid, columns=['sqft', 'beds'])

# add prediction
mlmPredGrid = mlm.predict(xygrid)

fig = px.scatter_3d(
	sacramento_train,
	x='sqft',
	y='beds',
	z='price',
	opacity=0.4,
	labels={
		'sqft': 'Size (sq ft)',
		'beds': 'Bedrooms',
		'price': 'Price (USD)',
	},
)

fig.update_traces(marker={'size': 2, 'color': 'red'})

fig.add_trace(
	go.Surface(
		x=xvals,
		y=yvals,
		z=mlmPredGrid.reshape(50, -1),
		name='Predictions',
		colorscale='viridis',
		colorbar={'title': 'Price (USD)'},
	)
)

fig.update_layout(
	margin=dict(l=0, r=0, b=0, t=1),
	template='plotly_white',
)

fig
```

## Sacramento real estate: mlm rmspe

```{python}
lm_mult_test_RMSPE = mean_squared_error(
	y_true=sacramento_test['price'], y_pred=sacramento_test['predicted']
) ** (1 / 2)

lm_mult_test_RMSPE

```

## Outliers and Multicollinearity

- Outliers: extreme values that can move the best fit line
- Multicollinearity: variables that are highly correlated to one another

## Outliers

::: {.columns}
::: {.column}
Subset

```{python}
# | echo: false

sacramento_train_small = sacramento_train.sample(100, random_state=2)
sacramento_outlier = pd.DataFrame({'sqft': [5000], 'price': [50000]})
sacramento_concat_df = pd.concat((sacramento_train_small, sacramento_outlier))

lm_plot_outlier = (
	alt.Chart(sacramento_train_small)
	.mark_circle()
	.encode(
		x=alt.X(
			'sqft',
			title='House size (square feet)',
			scale=alt.Scale(zero=False),
		),
		y=alt.Y(
			'price',
			title='Price (USD)',
			axis=alt.Axis(format='$,.0f'),
			scale=alt.Scale(zero=False),
		),
	)
)
lm_plot_outlier += lm_plot_outlier.transform_regression(
	'sqft', 'price'
).mark_line(color='#ff7f0e')

outlier_pt = (
	alt.Chart(sacramento_outlier)
	.mark_circle(color='#d62728', size=100)
	.encode(x='sqft', y='price')
)

outlier_line = (
	(
		alt.Chart(sacramento_concat_df)
		.mark_circle()
		.encode(
			x=alt.X(
				'sqft',
				title='House size (square feet)',
				scale=alt.Scale(zero=False),
			),
			y=alt.Y(
				'price',
				title='Price (USD)',
				axis=alt.Axis(format='$,.0f'),
				scale=alt.Scale(zero=False),
			),
		)
	)
	.transform_regression('sqft', 'price')
	.mark_line(color='#d62728')
)

lm_plot_outlier += outlier_pt + outlier_line

lm_plot_outlier
```


:::
::: {.column}
Full data

```{python}
#| echo: false

sacramento_concat_df = pd.concat((sacramento_train, sacramento_outlier))

lm_plot_outlier_large = (
    alt.Chart(sacramento_train)
    .mark_circle()
    .encode(
        x=alt.X("sqft", title="House size (square feet)", scale=alt.Scale(zero=False)),
        y=alt.Y(
            "price",
            title="Price (USD)",
            axis=alt.Axis(format="$,.0f"),
            scale=alt.Scale(zero=False),
        ),
    )
)
lm_plot_outlier_large += lm_plot_outlier_large.transform_regression(
    "sqft", "price"
).mark_line(color="#ff7f0e")

outlier_line = (
    (
        alt.Chart(sacramento_concat_df)
        .mark_circle()
        .encode(
            x=alt.X(
                "sqft", title="House size (square feet)", scale=alt.Scale(zero=False)
            ),
            y=alt.Y(
                "price",
                title="Price (USD)",
                axis=alt.Axis(format="$,.0f"),
                scale=alt.Scale(zero=False),
            ),
        )
    )
    .transform_regression("sqft", "price")
    .mark_line(color="#d62728")
)

lm_plot_outlier_large += outlier_pt + outlier_line

lm_plot_outlier_large
```

:::
:::

## Multicollinearity

- Multicollinearity means that some (or all) of the *explanatory* variables are linearly related! 

- When this happens, the coefficient estimates are very "unstable" and the contribution of one variable gets mixed with that of another variable correlated with it.

- Essentially, the plane of best fit has regression coefficients that are very sensitive to the exact values in the data.

```{python}
# | echo: false

np.random.seed(1)
sacramento_train = sacramento_train.assign(
	sqft1=sacramento_train['sqft']
	+ 100
	* np.random.choice(
		range(1000000), size=len(sacramento_train), replace=True
	)
	/ 1000000
)
sacramento_train = sacramento_train.assign(
	sqft2=sacramento_train['sqft']
	+ 100
	* np.random.choice(
		range(1000000), size=len(sacramento_train), replace=True
	)
	/ 1000000
)
sacramento_train = sacramento_train.assign(
	sqft3=sacramento_train['sqft']
	+ 100
	* np.random.choice(
		range(1000000), size=len(sacramento_train), replace=True
	)
	/ 1000000
)
sacramento_train

lm_plot_multicol_1 = (
	alt.Chart(sacramento_train)
	.mark_circle()
	.encode(
		x=alt.X('sqft', title='House size measurement 1 (square feet)'),
		y=alt.Y('sqft1', title='House size measurement 2 (square feet)'),
	)
)

lm_plot_multicol_1
```

## Feature selection

- In modern datasets, we often have many features (sometimes more than the number of observations).
- Not all features are informative or relevant.
- Including irrelevant predictors can lead to:
  - Overfitting
  - Multicollinearity
  - Poor generalization to new data
- We need methods that can automatically select features while fitting the model...

## L1-regularization 

- L1-regularization, often referred to as LASSO (Least Absolute Shrinkage and Selection Operator), adds an L1 penalty to the least squares loss:

$$
\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} \left\{
\sum_{i=1}^n (y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p |\beta_j|
\right\}
$$

- $\lambda \geq 0$ is a tuning parameter that controls the strength of the penalty
- With the L1 penalty, many coefficients get set equal to zero. 
- Thus, LASSO performs variable selection *and* regularization.

:::{.callout-note}
Standardization is necessary in penalized regression because penalty terms are scale-sensitive.
:::

## L1-regularization in `scikit-learn`

```{python}

from sklearn.linear_model import Lasso

# Example: Use only numeric columns for X 
features = ['beds', 'baths', 'sqft']

X = sacramento_train[features]
y = sacramento_train['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2025)

sacr_preprocessor = make_column_transformer(
    (StandardScaler(), features),
    remainder= 'drop'
)

sacr_pipeline = make_pipeline(
    sacr_preprocessor,
    Lasso(alpha=2000.0, max_iter=10000) # fixing alpha for now
)

# Fit the pipeline on training data
sacr_pipeline.fit(X_train, y_train)

lasso_model = sacr_pipeline.named_steps['lasso']

coefs = lasso_model.coef_

# Count non-zero coefficients
num_selected = np.sum(coefs != 0)
print(f"Number of features selected by Lasso: {num_selected}")

# List selected features
selected_features = [feature for feature, coef in zip(features, coefs) if coef != 0]
print("Selected features:", selected_features)
```

## Choosing the shrinkage parameter 

- The $\alpha$ parameter controls the strength of the L1 penalty:
  - Larger $\alpha$ â†’ stronger regularization â†’ more coefficients shrunk to zero
  - Smaller $\alpha$ â†’ less regularization â†’ more complex model

- Proper selection of $\alpha$ is critical for balancing **bias** and **variance**.

## Tuning $\alpha$ with cross-validation

- Just like tuning $k$ in KNN, we can select $\alpha$ by evaluating performance on validation sets.

- Example workflow with **GridSearchCV**:


```{python}
pipeline = make_pipeline(
    sacr_preprocessor,
    Lasso(max_iter=10000)
)

param_grid = {
    'lasso__alpha': np.arange(1, 3000, 10.0)
}

grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    cv=5,
    scoring='neg_root_mean_squared_error'
)

grid_search.fit(X_train, y_train)

print("Best alpha:", grid_search.best_params_['lasso__alpha'])
print("Coefficients:", coefs)
```

## 

```{python}
best_model = grid_search.best_estimator_

# Predict on test set
y_pred = best_model.predict(X_test)

# Show first 5 predictions
print("First 5 predictions:", y_pred[:5])

# Evaluate model
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** (1/2)

print(f"Test RMSE: {rmse:.2f}")
```

## Clicker questions

- [https://www.menti.com/algmf2xzfs8g9](https://www.menti.com/algmf2xzfs8g)

- Alternatively, go to [menti.com](menti.com) and use code 1668 5101

## Additional Resources

- The [Regression I: K-nearest neighbors](https://python.datasciencebook.ca/regression1.html)
  and [Regression II: linear regression](https://python.datasciencebook.ca/regression2.html)
  chapters of Data Science: A First Introduction (Python Edition) by
  Tiffany Timbers, Trevor Campbell, Melissa Lee, Joel Ostblom, Lindsey Heagy
  contains all the content presented here with a detailed narrative.
- The [`scikit-learn` website](https://scikit-learn.org/stable/) is an excellent
  reference for more details on, and advanced usage of, the functions and
  packages in this lesson. Aside from that, it also offers many
  useful [tutorials](https://scikit-learn.org/stable/tutorial/index.html)
  to get you started.
- [*An Introduction to Statistical Learning*](https://www.statlearning.com/) by
  Gareth James Daniela Witten Trevor Hastie, and Robert Tibshirani provides
  a great next stop in the process of
  learning about classification. Chapter 3 discusses lienar regression in more depth.
  As well as how it comares to K-nearest neighbors.


## References

Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21â€“27, 1967.

Evelyn Fix and Joseph Hodges. Discriminatory analysis. nonparametric discrimination: consistency properties. Technical Report, USAF School of Aviation Medicine, Randolph Field, Texas, 1951.

## Code for CV in K-NN regression

```{python}
from sklearn.neighbors import KNeighborsRegressor

# preprocess the data, make the pipeline
sacr_preprocessor = make_column_transformer((StandardScaler(), ['sqft']))
sacr_pipeline = make_pipeline(sacr_preprocessor, KNeighborsRegressor())

# create the 5-fold GridSearchCV object
param_grid = {
	'kneighborsregressor__n_neighbors': range(1, 201, 3),
}
sacr_gridsearch = GridSearchCV(
	estimator=sacr_pipeline,
	param_grid=param_grid,
	cv=5,
	scoring='neg_root_mean_squared_error'
)

# fit the GridSearchCV object
sacr_gridsearch.fit(
	sacramento_train[['sqft']],  
	sacramento_train['price']
)

sacr_gridsearch.best_params_

# Retrieve the CV scores
sacr_results = pd.DataFrame(sacr_gridsearch.cv_results_)
sacr_results = sacr_results[
	[
		'param_kneighborsregressor__n_neighbors',
		'mean_test_score'
	]
].rename(columns={'param_kneighborsregressor__n_neighbors': 'n_neighbors'})


best_k_sacr = sacr_results["n_neighbors"][sacr_results["mean_test_score"].idxmin()]
best_cv_RMSPE = min(sacr_results["mean_test_score"])
```

## Multivariable K-NN regression: Preprocessor

```{python}
sacr_preprocessor = make_column_transformer(
    (StandardScaler(), ['sqft', 'beds'])
)
sacr_pipeline = make_pipeline(sacr_preprocessor, KNeighborsRegressor())
```

## Multivariable K-NN regression: CV

```{python}
# create the 5-fold GridSearchCV object
param_grid = {
    'kneighborsregressor__n_neighbors': range(1, 50),
}

sacr_gridsearch = GridSearchCV(
    estimator=sacr_pipeline,
    param_grid=param_grid,
    cv=5,
    scoring='neg_root_mean_squared_error',
)

sacr_gridsearch.fit(
    sacramento_train[['sqft', 'beds']], sacramento_train['price']
)
```

## Multivariable K-NN regression: Best K

```{python}
# retrieve the CV scores
sacr_results = pd.DataFrame(sacr_gridsearch.cv_results_)
sacr_results['mean_test_score'] = -sacr_results['mean_test_score']
sacr_results = sacr_results[
    [
        'param_kneighborsregressor__n_neighbors',
        'mean_test_score'
    ]
].rename(columns={'param_kneighborsregressor__n_neighbors': 'n_neighbors'})

# show only the row of minimum RMSPE
sacr_results.nsmallest(1, 'mean_test_score')
```

## Multivariable K-NN regression: Best model

```{python}
best_k_sacr_multi = sacr_results["n_neighbors"][sacr_results["mean_test_score"].idxmin()]
min_rmspe_sacr_multi = min(sacr_results["mean_test_score"])
```

Best K

```{python}
best_k_sacr_multi
```

Best RMSPE

```{python}
min_rmspe_sacr_multi
```



## Multivariable K-NN regression: Test data

```{python}
sacramento_test["predicted"] = sacr_gridsearch.predict(sacramento_test)
RMSPE_mult = mean_squared_error(
    y_true=sacramento_test["price"],
    y_pred=sacramento_test["predicted"]
)**(1/2)

RMSPE_mult
```



## Multivariable K-NN regression: Visualize

```{python}
# | echo: false

# create a prediction pt grid
xvals = np.linspace(
    sacramento_train['sqft'].min(), sacramento_train['sqft'].max(), 50
)
yvals = np.linspace(
    sacramento_train['beds'].min(), sacramento_train['beds'].max(), 50
)
xygrid = np.array(np.meshgrid(xvals, yvals)).reshape(2, -1).T
xygrid = pd.DataFrame(xygrid, columns=['sqft', 'beds'])

# add prediction
knnPredGrid = sacr_gridsearch.predict(xygrid)

fig = px.scatter_3d(
    sacramento_train,
    x='sqft',
    y='beds',
    z='price',
    opacity=0.4,
    labels={
        'sqft': 'Size (sq ft)',
        'beds': 'Bedrooms',
        'price': 'Price (USD)',
    },
)

fig.update_traces(marker={'size': 2, 'color': 'red'})

fig.add_trace(
    go.Surface(
        x=xvals,
        y=yvals,
        z=knnPredGrid.reshape(50, -1),
        name='Predictions',
        colorscale='viridis',
        colorbar={'title': 'Price (USD)'},
    )
)

fig.update_layout(
    margin=dict(l=0, r=0, b=0, t=1),
    template='plotly_white',
)

fig
```

