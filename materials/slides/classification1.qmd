---
title: "Classification I: training & predicting"
format: 
  live-revealjs:
    slide-number: true
    theme: [default, styles.scss]
    incremental: false
    scrollable: true
resources:
  - data
jupyter: python3
execute:
  echo: true
  warning: false
---


```{python}
# | include: false
import pandas as pd
import numpy as np
import altair as alt

pd.set_option("display.max_rows", 7)
```

## Introductions

<!-- Top row: two profile intros -->
::: {style="display: flex; gap: 2rem; justify-content: center; align-items: flex-start;"}


::: {style="text-align: center; flex: 0 0 auto;"}
<img src="https://canssiontario.utoronto.ca/wp-content/uploads/2021/03/Timbers-Tiffany_5x7.png" 
     alt="Tiffany Timbers" 
     style="width: 200px; height: 200px; object-fit: cover; border-radius: 50%;"/>

**Tiffany Timbers**  
Associate Professor of Teaching, <br>
Dept. of Statistics, UBC
:::


::: {style="text-align: center; flex: 0 0 auto;"}
<img src="https://katieburak.github.io/profile.png" 
     alt="Katie Burak" 
     style="width: 200px; height: 200px; object-fit: cover; border-radius: 50%;"/>

**Katie Burak**  
Assistant Professor of Teaching, <br>
Dept. of Statistics, UBC
:::

:::

## Housekeeping 

-   Gender-neutral bathrooms:
    - LL1 (Under escalators, to the right of the Learning Center)
    - LL2 (Next to Chicago A)
-   Meditation/prayer room: LL2 - Chicago A 
-   Mother's/lactation room: LL2 - Chicago B
-   Red lanyards = No photos
-   Code of Conduct: <https://posit.co/code-of-conduct>. Please review carefully. You can report Code of Conduct violations in person, by email, or by phone. Please see the policy linked above for contact information.
-   Coffee breaks 10:30-11 and 15-15:30
-   Lunch 12:30-13:30

## Helpful resources

- Workshop material: [https://posit-conf-2025.github.io/scikit-learn/](https://posit-conf-2025.github.io/scikit-learn/)
- [https://python.datasciencebook.ca/](https://python.datasciencebook.ca/)
- [`scikit-learn` docs](https://scikit-learn.org/stable/)
- [KCDS ML course](https://ubc-mds.github.io/introduction-machine-learning/modules/index.html)


## Session learning objectives

By the end of the session, learners will be able to do the following:

* Recognize situations where a simple classifier would be appropriate for making predictions.
* Explain the $K$-nearest neighbor classification algorithm.
* Interpret the output of a classifier.
* Describe what a training data set is and how it is used in classification.
* Given a dataset with two explanatory variables/predictors,
  use $K$-nearest neighbor classification in Python using
  the `scikit-learn` framework to predict the class of a single new observation.

## The classification problem

> predicting a categorical class (sometimes called a *label*) for an observation given its
other variables (sometimes called *features*)

- Diagnose a patient as healthy or sick
- Tag an email as "spam" or "not spam"
- Predict whether a purchase is fraudulent

## Training set

> Observations with known classes that we use as a basis for prediction

- Assign an observation without a known class (e.g., a new patient)
- To a class (e.g., diseased or healthy)

How?

- By how similar it is to other observations for which we do know the class
    - (e.g., previous patients with known diseases and symptoms)

## K-nearest neighbors

- One of many possible classification methods
    - KNN, decision trees, support vector machines (SVMs),
logistic regression, neural networks, and more;

> Predict observations based on other observations "close" to it

## Exploring a data set

Data:

- [digitized breast cancer image features](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29),
created by Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian

- Each row:
    - diagnosis (benign or malignant)
    - several other measurements (nucleus texture, perimeter, area, and more)

- Diagnosis for each image was conducted by physicians.

## ML Question

<br></br>

*Formulate a predictive question*:

> *Can we use the tumor image measurements available to us to predict whether a future tumor image
(with unknown diagnosis) shows a benign or malignant tumor?*

## Loading the cancer data 

```{pyodide}
import pandas as pd
import altair as alt

cancer = pd.read_csv("data/wdbc.csv")
print(cancer)
```

> these values have been *standardized (centered and scaled)*

## Describing the variables in the cancer data set 

1. ID: identification number
2. Class: the diagnosis (M = malignant or B = benign)
3. Radius: the mean of distances from center to points on the perimeter
4. Texture: the standard deviation of gray-scale values
5. Perimeter: the length of the surrounding contour
6. Area: the area inside the contour
7. Smoothness: the local variation in radius lengths
8. Compactness: the ratio of squared perimeter and area
9. Concavity: severity of concave portions of the contour
10. Concave Points: the number of concave portions of the contour
11. Symmetry: how similar the nucleus is when mirrored
12. Fractal Dimension: a measurement of how "rough" the perimeter is

## DataFrame; info

```{pyodide}
cancer.info()
```

## Series; unique

```{pyodide}
cancer["Class"].unique()
```


## Series; replace

```{pyodide}
cancer["Class"] = cancer["Class"].replace({
    "M" : "Malignant",
    "B" : "Benign"
})

cancer["Class"].unique()
```

## Exploring the cancer data

:::: {.columns}

::: {.column width="50%"}
```{pyodide}
cancer["Class"].value_counts()
```
:::


::: {.column width="50%"}

```{pyodide}
cancer["Class"].value_counts(normalize=True)
```
:::

::::

## Visualization; scatter

```{pyodide}
perim_concav = alt.Chart(cancer).mark_circle().encode(
    x=alt.X("Perimeter").title("Perimeter (standardized)"),
    y=alt.Y("Concavity").title("Concavity (standardized)"),
    color=alt.Color("Class").title("Diagnosis")
)
perim_concav
```

- Malignant: upper right-hand corner
- Benign: lower left-hand corner


## K-nearest neighbors; classification

1. find the $K$ "nearest" or "most similar" observations in our training set
2. predict new observation based on closest points

## KNN Example: new point

![](img/classification1/perim_concav_with_new_point.png){fig-align="center"}

## KNN example: closest point

> if a point is close to another in the scatter plot,
> then the perimeter and concavity values are similar,
> and so we may expect that they would have the same diagnosis

![](img/classification1/perim_concav_with_new_point-line.png){fig-align="center"}



## KNN Example: another new point

![](img/classification1/perim_concav_with_new_point2 .png){fig-align="center"}

## KNN: improve the prediction with `k`

we can consider several neighboring points, `k=3`

![](img/classification1/perim_concav_with_new_point2-lines.png){fig-align="center"}

## Distance between points

$$\mathrm{Distance} = \sqrt{(a_x -b_x)^2 + (a_y - b_y)^2}$$

## Distance between points: `k=5`

>  3 of the 5 nearest neighbors to our new observation are malignant

![](img/classification1/3-of-5.png){fig-align="center"}


## Classification with K-nearest neighbors

```{pyodide}
new_point = [2, 4]
attrs = ["Perimeter", "Concavity"]

points_df = pd.DataFrame(
    {"Perimeter": new_point[0], "Concavity": new_point[1], "Class": ["Unknown"]}
)

perim_concav_with_new_point_df = pd.concat((cancer, points_df), ignore_index=True)
print(perim_concav_with_new_point_df.iloc[[-1]])
```


## Distances 

- Compute the distance matrix between each pair from a vector array X and Y using `euclidean_distances`

```{pyodide}
from sklearn.metrics.pairwise import euclidean_distances

# distance of new point to all other points
my_distances = euclidean_distances(perim_concav_with_new_point_df[attrs])[len(cancer)][:-1]
```

```{pyodide}
len(my_distances)
```

```{pyodide}
# distance of new point to all other points
my_distances
```

## More than two explanatory variables: distance formula

The distance formula becomes

$$\mathrm{Distance} = \sqrt{(a_{1} -b_{1})^2 + (a_{2} - b_{2})^2 + \dots + (a_{m} - b_{m})^2}.$$


## More than two explanatory variables: visualize

![](img/classification1/3d.png){fig-align="center"}

## Summary of K-nearest neighbors algorithm

The K-nearest neighbors algorithm works as follows:

1. Compute the distance between the new observation and each observation in the training set
2. Find the $K$ rows corresponding to the $K$ smallest distances
3. Classify the new observation based on a majority vote of the neighbor classes

## K-nearest neighbors with `scikit-learn`

- K-nearest neighbors algorithm is implemented in [`scikit-learn`](https://scikit-learn.org/stable/index.html)

```{pyodide}
from sklearn import set_config

# Output dataframes instead of arrays
set_config(transform_output="pandas")
```

Now we can get started with `sklearn` and `KNeighborsClassifier()`

```{pyodide}
from sklearn.neighbors import KNeighborsClassifier
```

## Review cancer data

```{pyodide}
cancer_train = cancer[["Class", "Perimeter", "Concavity"]]
print(cancer_train)
```

## `scikit-learn`: Create Model Object

```{pyodide}
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn
```

## `scikit-learn`: Fit the model

```{pyodide}
knn.fit(
  X=cancer_train[["Perimeter", "Concavity"]],
  y=cancer_train["Class"]
)
```

:::{.callout-note}
1. We do not re-assign the variable
2. The arguments are `X` and `y` (note the capitialization).
   This comes from matrix notation.
:::

## `scikit-learn`: Predict

```{pyodide}
new_obs = pd.DataFrame({"Perimeter": [0], "Concavity": [3.5]})
print(new_obs)
```

```{pyodide}
knn.predict(new_obs)
```

## Data preprocessing: Scaling

For KNN:

- the *scale* of each variable (i.e., its size and range of values) matters
- distance based algorithm

Compare these 2 scenarios:

- Person A (200 lbs, 6ft tall) vs Person B (202 lbs, 6ft tall)
- Person A (200 lbs, 6ft tall) vs Person B (200 lbs, 8ft tall)

All have a distance of 2

## Data preprocessing: Centering

Many other models:

- *center* of each variable (e.g., its mean) matters as well

- Does not matter as much in KNN:

- Person A (200 lbs, 6ft tall) vs Person B (202 lbs, 6ft tall)
- Person A (200 lbs, 6ft tall) vs Person B (200 lbs, 8ft tall)

Difference in weight is in the 10s, difference in height is fractions of a foot.

## Data preprocessing: Standardization

- The mean is used to center, the standard deviation is used to scale
- Standardization: transform the data such that the mean is 0, and a standard deviation is 1

```{pyodide}
unscaled_cancer = pd.read_csv("data/wdbc_unscaled.csv")[["Class", "Area", "Smoothness"]]
unscaled_cancer["Class"] = unscaled_cancer["Class"].replace({
   "M" : "Malignant",
   "B" : "Benign"
})
unscaled_cancer
```

## `scikit-learn`: `ColumnTransformer`

- `scikit-learn` has a [`preprocessing` module](https://scikit-learn.org/stable/modules/preprocessing.html)
  - `StandardScaler()`: scale our data
- [`make_column_transformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html#sklearn.compose.make_column_selector):
  creates a [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer)
  to select columns

```{pyodide}
from sklearn.preprocessing import StandardScaler
from sklearn.compose import make_column_transformer

preprocessor = make_column_transformer(
    (StandardScaler(), ["Area", "Smoothness"]),
)
preprocessor
```

## `scikit-learn`: Select numeric columns

```{pyodide}
from sklearn.compose import make_column_selector

preprocessor = make_column_transformer(
    (StandardScaler(), make_column_selector(dtype_include="number")),
)
preprocessor
```

## `scikit-learn`: transform

Scale the data

```{pyodide}
preprocessor.fit(unscaled_cancer)
scaled_cancer = preprocessor.transform(unscaled_cancer)
```

Compare unscaled vs scaled

:::: {.columns}

::: {.column width="50%"}
```{pyodide}
print(unscaled_cancer)
```
:::


::: {.column width="50%"}
```{pyodide}
print(scaled_cancer)
```
:::

::::

## Visualize unstandarized vs standarized data

![](img/classification1/unstandardizedcomparison.png){fig-align="center"}

## Why `scikit-learn` pipelines?

- Manually standarizing is error prone
- Does not automatically account for new data
- Prevent data leakage by processing on training data to use on test data (later)
- Need same mean and standarization from training to use on test / new data

## Balancing + class imbalance

What if we have class imbalance? i.e., if the response variable has a big difference
in frequency counts between classes?

```{pyodide}
rare_cancer = pd.concat((
    cancer[cancer["Class"] == "Benign"],
    cancer[cancer["Class"] == "Malignant"].head(3) # only 3 total
))
print(rare_cancer)
```

## Visualizing class imbalance

```{pyodide}
rare_cancer["Class"].value_counts()
```

![](img/classification1/rarecancer.png){fig-align="center"}

## Predicting with class imbalance

:::: {.columns}

::: {.column width="50%"}
![](img/classification1/imbalance1.png)
:::

::: {.column width="50%"}
![](img/classification1/imbalance2.png)
:::

::::

## Upsampling

Rebalance the data by *oversampling* the rare class

1. Separate the classes out into their own data frames by filtering
2. Use the `.sample()` method on the rare class data frame
    - Sample with replacement so the classes are the same size
3. Use the `.value_counts()` method to see that our classes are now balanced

## Upsampling: code

Set seed
```{pyodide}
import numpy as np

np.random.seed(42)
```

Upsample the rare class

```{pyodide}
malignant_cancer = rare_cancer[rare_cancer["Class"] == "Malignant"]
benign_cancer = rare_cancer[rare_cancer["Class"] == "Benign"]
malignant_cancer_upsample = malignant_cancer.sample(
    n=benign_cancer.shape[0], replace=True
)
upsampled_cancer = pd.concat((malignant_cancer_upsample, benign_cancer))
upsampled_cancer["Class"].value_counts()

```

## Upsampling: Re-train KNN `k=7`

![](img/classification1/upsampled.png){fig-align="center"}

## Missing data

Assume we are only looking at "randomly missing" data

```{pyodide}
missing_cancer = pd.read_csv("data/wdbc_missing.csv")[
    ["Class", "Radius", "Texture", "Perimeter"]
]
missing_cancer["Class"] = missing_cancer["Class"].replace(
    {"M": "Malignant", "B": "Benign"}
)
print(missing_cancer)
```

## Missing data: `.dropna()`

KNN computes distances across all the features, it needs complete observations

```{pyodide}
# drop incomplete observations
no_missing_cancer = missing_cancer.dropna()
print(no_missing_cancer)
```

## Missing data: `SimpleImputer()`

We can impute missing data (with the mean) if there's too many missing values

```{pyodide}
from sklearn.impute import SimpleImputer

preprocessor = make_column_transformer(
    (SimpleImputer(), ["Radius", "Texture", "Perimeter"]),
    verbose_feature_names_out=False,
)
preprocessor
```

## Imputed data

```{pyodide}
preprocessor.fit(missing_cancer)
imputed_cancer = preprocessor.transform(missing_cancer)

```

::: {.columns}
::: {.column}

```{pyodide}
print(missing_cancer)
```

:::
::: {.column}

```{pyodide}
print(imputed_cancer)
```

:::
:::

## Put it all together: Preprocessor

```{pyodide}
# load the unscaled cancer data, make Class readable
unscaled_cancer = pd.read_csv("data/wdbc_unscaled.csv")
unscaled_cancer["Class"] = unscaled_cancer["Class"].replace(
    {"M": "Malignant", "B": "Benign"}
)

# create the K-NN model
knn = KNeighborsClassifier(n_neighbors=7)

# create the centering / scaling preprocessor
preprocessor = make_column_transformer(
    (StandardScaler(), ["Area", "Smoothness"]),
    # more column transformers here
)

```

## Put it all together: Pipeline

```{pyodide}
from sklearn.pipeline import make_pipeline

knn_pipeline = make_pipeline(preprocessor, knn)
knn_pipeline.fit(
    X=unscaled_cancer,
    y=unscaled_cancer["Class"]
)
knn_pipeline
```

## Put it all together: Predict

```{pyodide}
new_observation = pd.DataFrame(
    {"Area": [500, 1500], "Smoothness": [0.075, 0.1]}
)
prediction = knn_pipeline.predict(new_observation)
prediction
```

## Prediction Area

Model prediction area.

![](img/classification1/prediction_plot.png){fig-align="center"}

- Points are on original unscaled data
- Area is using the pipeline model

## Clicker questions

- [https://www.menti.com/alpi9oanfsf9](https://www.menti.com/alpi9oanfsf9)

- Alternatively, go to [menti.com](menti.com) and use code 8481 0955

## Additional resources

- The [Classification I: training & predicting](https://python.datasciencebook.ca/classification1.html)
  chapter of Data Science: A First Introduction (Python Edition) by
  Tiffany Timbers, Trevor Campbell, Melissa Lee, Joel Ostblom, Lindsey Heagy
  contains all the content presented here with a detailed narrative.
- The [`scikit-learn` website](https://scikit-learn.org/stable/) is an excellent
  reference for more details on, and advanced usage of, the functions and
  packages in this lesson. Aside from that, it also offers many
  useful [tutorials](https://scikit-learn.org/stable/tutorial/index.html)
  to get you started.
- [*An Introduction to Statistical Learning*](https://www.statlearning.com/) by
  Gareth James Daniela Witten Trevor Hastie, and Robert Tibshirani provides
  a great next stop in the process of
  learning about classification. Chapter 4 discusses additional basic techniques
  for classification that we do not cover, such as logistic regression, linear
  discriminant analysis, and naive Bayes.


## References

Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Gaël Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, 108–122. 2013.

Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21–27, 1967.

Evelyn Fix and Joseph Hodges. Discriminatory analysis. nonparametric discrimination: consistency properties. Technical Report, USAF School of Aviation Medicine, Randolph Field, Texas, 1951.

William Nick Street, William Wolberg, and Olvi Mangasarian. Nuclear feature extraction for breast tumor diagnosis. In International Symposium on Electronic Imaging: Science and Technology. 1993.

Stanford Health Care. What is cancer? 2021. URL: https://stanfordhealthcare.org/medical-conditions/cancer/cancer.html.
