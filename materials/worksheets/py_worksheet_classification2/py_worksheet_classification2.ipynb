{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce3fc2f7fe8c4446d8bd7fb16e7587a6",
     "grade": false,
     "grade_id": "cell-83e402ae256bacca",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Worksheet - Classification (Part II)\n",
    "\n",
    "## Learning Goals:\n",
    "\n",
    "After completing this workshop session, you will be able to:\n",
    "\n",
    "* Describe what a test data set is and how it is used in classification.\n",
    "* Understand several ways of representing classifier performance: accuracy, precision, and recall, and the confusion matrix.\n",
    "* Using Python, evaluate classifier performance using a test data set and appropriate metrics.\n",
    "* Using Python, execute cross-validation in Python to choose the number of neighbours.\n",
    "* Identify when it is necessary to scale variables before classification and do this using Python\n",
    "* In a dataset with > 2 attributes, perform k-nearest neighbour classification in Python using the `scikit-learn` package to predict the class of a test dataset.\n",
    "* Describe advantages and disadvantages of the k-nearest neighbour classification algorithm.\n",
    "\n",
    "This worksheet covers parts of [Chapter 6](https://python.datasciencebook.ca/classification2) of the online textbook. You can refer to and read this chapter to help you answer the worksheet. Any place you see `___`, you must fill in the function, variable, or data to complete the code before running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "492c0919a51f6fe3859c14f2af85c265",
     "grade": false,
     "grade_id": "cell-e3518ed3d64fb67c",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Run this cell before continuing.\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Simplify working with large datasets in Altair\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# Output dataframes instead of arrays\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdeca2f54241b43f374f8c841ee0afc7",
     "grade": false,
     "grade_id": "cell-0ae53a5320742a59",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Fruit Data Example\n",
    "\n",
    "In the agricultural industry, cleaning, sorting, grading, and packaging food products are all necessary tasks in the post-harvest process. Products are classified based on appearance, size and shape, attributes which helps determine the quality of the food. Sorting can be done by humans, but it is tedious and time consuming. Automatic sorting could help save time and money. Images of the food products are captured and analysed to determine visual characteristics. \n",
    "\n",
    "The [dataset](https://www.kaggle.com/mjamilmoughal/k-nearest-neighbor-classifier-to-predict-fruits/notebook) contains observations of fruit described with four features: (1) mass (in g), (2) width (in cm), (3) height (in cm), and (4) color score (on a scale from 0 - 1).\n",
    "\n",
    "To get started building a classifier that can classfiy a fruit based on its appearance, use `pd.read_csv` to load the file `fruit_data.csv` (found in the data folder) from the previous tutorial into your notebook.\n",
    "\n",
    "*Assign your data to an object called `fruit_data`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "# Run this cell to read the data\n",
    "fruit_data = pd.read_csv(\"data/fruit_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "561a15a05a929a995640793bdf16f4c8",
     "grade": false,
     "grade_id": "cell-9691b09f8cd7044c",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's take a look at the first few observations in the fruit dataset. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4f469982bf1cca15567b6e44725df2f",
     "grade": false,
     "grade_id": "cell-19eb282d44a51d90",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to preview the data\n",
    "fruit_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's investigate the class counts for each kind of fruit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_data['fruit_name'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change to using `normalize=True` to get the class proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_data['fruit_name'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17bf6dce34be2f50d9bae7b06a353a86",
     "grade": false,
     "grade_id": "cell-6e0a8546a65b79e8",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Randomness and Setting Seeds\n",
    "\n",
    "This worksheet uses functions from the `scikit-learn` library, which not only allows us to perform K-nearest neighbour classification, but also allows us to evaluate how well our classification worked. In order to ensure that the steps in the worksheet are reproducible, we need to set a *`random_state`* or *random seed*, i.e., a numerical \"starting value,\" which determines the sequence of random numbers Python will generate.\n",
    "\n",
    "Below in many cells we have included an argument to set the `random_state` or `np.random.seed`. They are necessary to make sure the autotesting code functions properly. In your own analysis however, it is a better practice to set the *`random_state`* or *random seed* just once at the beginning of your script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49c822bc89425d0c7a20d98f6637f2e3",
     "grade": false,
     "grade_id": "cell-0e8089ce4f710660",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2. Splitting the data into a training and test set\n",
    "\n",
    "In this exercise, we will be partitioning `fruit_data` into a training (75%) and testing (25%) set using the `scikit-learn` package. After creating the test set, we will put the test set away in a lock box and not touch it again until we have found the best k-nn classifier we can make using the training set. We will use the variable `fruit_name` as our class label. \n",
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "To create the training and test set, we would use the `train_test_split` function from `scikit-learn` package. Save the trained dataset and test dataset as `fruit_train` and `fruit_test`, respectively. To help you out, we have put a scaffold of the code in the cell below. Your job is to fill in the blanks with the correct values.\n",
    "\n",
    "> Note: by default `scikit-learn` will **not** stratify the split by the label,\n",
    "> so if we forget to set `stratify` to `fruit_data['fruit_name']` it could be possible that\n",
    "> we may not end up with an observation in the test set with the label \"mandarin\"\n",
    "> (because there are only 5 mandarin observations in the entire data set),\n",
    "> and as a consequence, we would not be able to evaluate predictions on it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3a53ac95b91bca144661b01c9b4c133",
     "grade": false,
     "grade_id": "cell-c262636f2b9777e3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data into a training and test set\n",
    "\n",
    "# fruit_train, fruit_test = train_test_split(____, \n",
    "#                                            test_size=____,\n",
    "#                                            stratify=____,\n",
    "#                                            random_state=____) # set the random state to be 2020\n",
    "\n",
    "fruit_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b841e3ad8f51bda88aa7753fcf318af",
     "grade": true,
     "grade_id": "cell-7432078031e563b8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run these tests to check your answer\n",
    "\n",
    "# Check training data \n",
    "assert isinstance(fruit_train, pd.DataFrame), \"`fruit_train` is not a pandas DataFrame\"\n",
    "assert fruit_train.shape == (44, 7), f\"Expected shape (44, 7), but got {fruit_train.shape}\"\n",
    "assert (fruit_train['fruit_name'] == 'mandarin').any(), \"`mandarin` not found in `fruit_name`\"\n",
    "np.testing.assert_array_equal(np.array([14, 14, 12,  4]), fruit_train['fruit_name'].value_counts().values,\n",
    "                             err_msg='Some unexpected observations are in your training set.')\n",
    "assert fruit_train['fruit_name'].value_counts().index.tolist() == ['orange', 'apple', 'lemon', 'mandarin'], 'Some unexpected observations are in your training set.'\n",
    "\n",
    "# Check test data\n",
    "assert isinstance(fruit_test, pd.DataFrame), \"`fruit_test` is not a pandas DataFrame\"\n",
    "assert fruit_test.shape == (15, 7), f\"Expected shape (15, 7), but got {fruit_train.shape}\"\n",
    "assert (fruit_test['fruit_name'] == 'mandarin').any(), \"`mandarin` not found in `fruit_name`\"\n",
    "np.testing.assert_array_equal(np.array([5, 5, 4, 1]), fruit_test['fruit_name'].value_counts().values,\n",
    "                             err_msg='Some unexpected observations are in your test set.')\n",
    "assert fruit_test['fruit_name'].value_counts().index.tolist() == ['apple', 'orange', 'lemon', 'mandarin'], 'Some unexpected observations are in your test set.'\n",
    "\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3459c19223373b19d6a4f86268fd48f2",
     "grade": false,
     "grade_id": "cell-9f6c24da5042200c",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 2 \n",
    "\n",
    "K-nearest neighbors is sensitive to the scale of the predictors so we should do some preprocessing to standardize them. Remember that standardization is *part of your training procedure*, so you can't use your test data to compute the centered / scaled values for each variable. Once we have created the standardization preprocessor, we can then later on apply it separately to both the training and test data sets.\n",
    "\n",
    "For this worksheet, let's see if `mass`, `width`, `height` and `color_score` can predict `fruit_name`. \n",
    "\n",
    "To scale and center the data, first, pass the predictors to the `make_column_transformer` function to make the preprocessor. To help you out, we have put a scaffold of the code in the cell below. Your job is to fill in the blanks with the correct values.\n",
    "\n",
    "*Assign your answer to an object called `fruit_preprocessor`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74ac5c1fc9e911ba2da76c8ed2fa4d99",
     "grade": false,
     "grade_id": "cell-aca4cc578b245211",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a preprocessor using the training data\n",
    "\n",
    "# fruit_preprocessor = make_column_transformer(\n",
    "#     (____, ____,\n",
    "#     verbose_feature_names_out=____\n",
    "# )\n",
    "\n",
    "fruit_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ad26ad7fcf1b58f4a49c65612a43b93",
     "grade": true,
     "grade_id": "cell-2453aaa31ba794b6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run these tests to check your answer\n",
    "\n",
    "assert isinstance(fruit_preprocessor, ColumnTransformer), \"fruit_preprocessor is not a ColumnTransformer\"\n",
    "expected_columns = {'mass', 'width', 'height', 'color_score'}\n",
    "actual_columns = set(fruit_preprocessor.transformers[0][2])\n",
    "assert actual_columns == expected_columns, f\"Expected columns {expected_columns}, but got {actual_columns}\"\n",
    "assert isinstance(fruit_preprocessor.transformers[0][1], StandardScaler), \"Transformer is not an instance of StandardScaler\"\n",
    "\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the data, we can do things like exploratory data analysis and model fitting!!!\n",
    "\n",
    "A common way to explore the data when peforming classification is to create pairwise plots of the predictor variables,\n",
    "and color the points by the class label. Remember, that our model is sensitive to scale, \n",
    "and so we should also scale the data when visualizing it for modeling insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to scale the data and create a pairwise scatter plot\n",
    "\n",
    "# Create a scaled version of the data\n",
    "fruit_preprocessor.fit(fruit_train)\n",
    "scaled_fruit_train = fruit_preprocessor.transform(fruit_train)\n",
    "\n",
    "# Create the scatterplot\n",
    "fruit_pairwise = alt.Chart(fruit_train).mark_point(filled=True, stroke='black', strokeWidth=0.5, size=75, opacity=0.9).encode(\n",
    "    x=alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
    "    y=alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
    "    color=alt.Color(\"fruit_name:N\", scale=alt.Scale(range=['green', 'yellow', '#f26a02', '#ffd1a6'])),\n",
    "    shape=alt.Shape(\"fruit_name:N\", scale=alt.Scale(range=['circle', 'diamond', 'circle', 'square'])),\n",
    ").properties(\n",
    "    width=175,\n",
    "    height=175\n",
    ").repeat(\n",
    "    row=['mass', 'width', 'height', 'color_score'],\n",
    "    column=['color_score', 'height','width', 'mass']\n",
    ").interactive()\n",
    "\n",
    "fruit_pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6356bfde003430bf8048d1c516c8cddb",
     "grade": false,
     "grade_id": "cell-6693330a0f077083",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3. Cross-validation and parameter value selection\n",
    "\n",
    "### Question 3\n",
    "\n",
    "We want to pick the number of neighbours to maximize the performance of our classifier on data *it hasn’t seen yet*. One way to do this is to cross-validation. To do this in Python we should do the following:\n",
    "\n",
    "1. Setup a pipeline object that specifies the data and preprocessor, as well as the type of model we want to use,\n",
    "2. Create a dictionary with the range of values we want to search through for the parameter(s)\n",
    "3. Pass the pipline object, and parameter grid dictionary object to a `GridSearchCV`.\n",
    "4. Run `fit` on the `GridSearchCV` object to carryout the tuning process.\n",
    "5. Use `pd.DataFrame` to convert the output to a dataframe for convenience.\n",
    "\n",
    "Perform the above using to run 5-fold cross validation to choose $K$ \n",
    "for K-nearest neighbors classification.\n",
    "Use the `fruit_preprocessor` object you created in question 2,\n",
    "and search across the values from 1 to 15 (incrementing by 1).\n",
    "\n",
    "*Assign your final data frame to an object called `fruit_vfold_score` so you can check your work.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)  # DO NOT REMOVE\n",
    "\n",
    "# # 1. Setup pipeline object\n",
    "# fruit_tune_pipeline = make_pipeline(____, \n",
    "#                                     ____)\n",
    "# \n",
    "# # 2. Define parameter grid\n",
    "# parameter_grid = {\n",
    "#     ____: range(____, ____, ____),\n",
    "# }\n",
    "# \n",
    "# # 3. Create `GridSearchCV` object\n",
    "# fruit_tune_grid = GridSearchCV(\n",
    "#     estimator=____,\n",
    "#     param_grid=____,\n",
    "#     cv=____\n",
    "# )\n",
    "# \n",
    "# # 4. Fit `GridSearchCV` object\n",
    "# fruit_tune_grid.fit(\n",
    "#     ____,\n",
    "#     ____\n",
    "# )\n",
    "# \n",
    "# # 5. Convert results to data frame\n",
    "# fruit_vfold_score = pd.DataFrame(____)\n",
    "# \n",
    "fruit_vfold_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_vfold_score['rank_test_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these tests to check your answer\n",
    "\n",
    "assert isinstance(fruit_vfold_score, pd.DataFrame), \"`fruit_test` is not a pandas DataFrame\"\n",
    "assert fruit_vfold_score.shape == (14, 13), f\"Expected shape (14, 13), but got {fruit_vfold_score.shape}\"\n",
    "expected_columns = {'mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
    "       'param_kneighborsclassifier__n_neighbors', 'params',\n",
    "       'split0_test_score', 'split1_test_score', 'split2_test_score',\n",
    "       'split3_test_score', 'mean_test_score', 'std_test_score',\n",
    "       'rank_test_score'}\n",
    "actual_columns = set(fruit_vfold_score.columns)\n",
    "assert actual_columns == expected_columns, f\"Expected columns {expected_columns}, but got {actual_columns}\"\n",
    "\n",
    "np.testing.assert_array_equal(np.array([3, 2, 2, 1, 1, 1, 1, 1, 1, 1]), fruit_vfold_score['rank_test_score'].value_counts().values,\n",
    "                             err_msg='Something unexpected happened with the tuning of your classifier. Check your model and cross-validation specs.')\n",
    "assert fruit_vfold_score['rank_test_score'].value_counts().index.tolist() == [4, 1, 9, 3, 7, 8, 12, 11, 14, 13], 'Something unexpected happened with the tuning of your classifier. Check your model and cross-validation specs.'\n",
    "\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the results a bit easier to interpret, \n",
    "let's select just the columns we are interested in:\n",
    "\n",
    "- `param_kneighborsclassifier__n_neighbors`\n",
    "- `mean_test_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see a table of just the parameter values and\n",
    "# the mean cross-validation accuracy\n",
    "\n",
    "fruit_vfold_score[['param_kneighborsclassifier__n_neighbors',\n",
    "                  'mean_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a parameter's effect on model performance\n",
    "\n",
    "Visually inspecting the grid search results can help us find, \n",
    "and choose the best value for the number of neighbors parameter.\n",
    "\n",
    "Below, we use `altair` (a Python visualization library) \n",
    "to create a line plot using the `accuracies_grid` dataframe \n",
    "with `param_kneighborsclassifier__n_neighbors` on the x-axis \n",
    "and the `mean_test_score` on the y-axis. Use `point=True` to include a point for each value of $K$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize the parameters vs mean cross-validation accuracy \n",
    "# as a line chart\n",
    "\n",
    "accuracy_versus_k_grid = alt.Chart(fruit_vfold_score).mark_line(point=True).encode(\n",
    "    x=alt.X(\"param_kneighborsclassifier__n_neighbors\")\n",
    "        .title(\"Neighbors\")\n",
    "        .scale(zero=False),\n",
    "    y=alt.Y(\"mean_test_score\")\n",
    "        .title(\"Mean Test Score\")\n",
    "        .scale(zero=False)\n",
    ")\n",
    "\n",
    "accuracy_versus_k_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding prediction errors\n",
    "\n",
    "Now, let's look at the *confusion matrix* for the classifier. This will show us a table comparing the predicted labels with the true labels. \n",
    "\n",
    "Although we can create a confusion matrix by using the `crosstab` function from `pandas`, with many observations, it can be difficult to interpret the confusion matrix when it is presented as a table like above. In these cases, we could instead use the `ConfusionMatrixDisplay` function of the `scikit-learn` package to visualize the confusion matrix as a heatmap. Please run the cell below to see the fruit confusion matrix as a heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    estimator=fruit_tune_grid,  # We are directly passing the pipeline and let sklearn do the predictions for us\n",
    "    X=fruit_train.drop(columns=['fruit_name']),\n",
    "    y=fruit_train['fruit_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What mistakes is it making? \n",
    "Revisit the pairwise scatter plot you created after question 2, \n",
    "can you guess why its making these mistakes? \n",
    "Do you have any ideas of what you could do to improve the model\n",
    "(hint, think about the data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model generalizability\n",
    "\n",
    "Once you have finished selecting your single, final model (**and only then!**) you may wish to estimate how well your model will generalize to unseen data. You have kept your test set for just that purpose, and that purpose alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "The first step in assessing model generalizability is to use your model to predict labels for the test set observations. To do this with Python, use `predict` on the `fruit_tune_grid` object (that contains your trained model) to add a column named `predicted` to the `fruit_test` data frame. By default, scikit-learn will use the \"best estimator\", here $K$ with the highest accuracy, to do this. \n",
    "To help you out, we have put a scaffold of the code in the cell below. Your job is to fill in the blanks with the correct values.\n",
    "\n",
    "*Assign your answer to an object called `fruit_test_w_predictions`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a deep copy of `fruit_test` and name it `fruit_test_w_predictions`\n",
    "fruit_test_w_predictions = fruit_test.copy()\n",
    "\n",
    "# Use trained classifier to predict labels for test data\n",
    "\n",
    "# fruit_test_w_predictions['predicted'] = fruit_tune_grid.predict(\n",
    "#     ____)\n",
    "# )\n",
    "\n",
    "fruit_test_w_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these tests to check your answer\n",
    "\n",
    "assert isinstance(fruit_test_w_predictions, pd.DataFrame), '`fruit_test_w_predictions` is not a pandas DataFrame'\n",
    "assert fruit_test_w_predictions.shape == (15, 8), f\"Expected shape (15, 8), but got {fruit_test_w_predictions.shape}\"\n",
    "expected_columns = {'fruit_label', 'fruit_name', 'fruit_subtype', 'mass', 'width', 'height', 'color_score', 'predicted'}\n",
    "actual_columns = set(fruit_test_w_predictions.columns)\n",
    "assert actual_columns == expected_columns, f\"Expected columns {expected_columns}, but got {actual_columns}\"\n",
    "np.testing.assert_array_equal(np.array([5, 5, 4, 1]), fruit_test_w_predictions['predicted'].value_counts().values,\n",
    "                             err_msg='Some unexpected predictions were made from your classifier.')\n",
    "assert fruit_test_w_predictions['predicted'].value_counts().index.tolist() == ['apple', 'orange', 'lemon', 'mandarin'], 'Some unexpected predictions were made from your classifier.'\n",
    "\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Next, we can calculate our prediction metrics on the test set, as well a compute and visualize a confusion matrix. We typically expect some drop in performance from what we observed in our training set (because we used that set to train the model), but ideally the model performs similarly. \n",
    "\n",
    "Use Python to calculate accuracy, weighted precision and weighted recall for the test set. To help you out, we have put a scaffold of the code in the cell below. Your job is to fill in the blanks with the correct values. Name your objects for accuracy, weighted precision and weighted recall `accuracy`, `weighted_precision` and `weighted_recall`, respectively.\n",
    "\n",
    "> *Note: given we have many categories, we want to know the weighted average precision and recall instead of calculating it for just one class.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy for the test set\n",
    "\n",
    "# accuracy = fruit_tune_grid.score(\n",
    "#     X=____,\n",
    "#     y=____\n",
    "# )\n",
    "\n",
    "accuracy\n",
    "type(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted precision for the test set\n",
    "\n",
    "# weighted_precision = precision_score(\n",
    "#     y_true=____,\n",
    "#     y_pred=____,\n",
    "#     average='weighted'\n",
    "# )\n",
    "\n",
    "weighted_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted recall for the test set\n",
    "\n",
    "# weighted_recall = recall_score(\n",
    "#     y_true=____,\n",
    "#     y_pred=____,\n",
    "#     average='weighted'\n",
    "# )\n",
    "\n",
    "weighted_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these tests to check your answer\n",
    "\n",
    "# Check accuracy\n",
    "assert type(accuracy) == np.float64, '`accuracy` should be a single value, of type `numpy.float64`.'\n",
    "assert accuracy == 1, '`accuracy` value is incorrect'\n",
    "\n",
    "# Check weighted precision\n",
    "assert type(weighted_precision) == np.float64, '`weighted_precision` should be a single value, of type `numpy.float64`.'\n",
    "assert weighted_precision == 1, '`weighted_precision` value is incorrect'\n",
    "\n",
    "# Check weighted recall\n",
    "assert type(weighted_recall) == np.float64, '`weighted_recall` should be a single value, of type `numpy.float64`.'\n",
    "assert weighted_recall == 1, '`weighted_recall` value is incorrect'\n",
    "\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction performance as a confusion matrix\n",
    "\n",
    "# ConfusionMatrixDisplay.from_estimator(\n",
    "#     estimator=____\n",
    "#     X=____,\n",
    "#     y=____\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... no mistakes on the test set... Why might that be?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python [conda env:breast-cancer-predictor]",
   "language": "python",
   "name": "conda-env-breast-cancer-predictor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
