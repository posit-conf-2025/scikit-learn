[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine learning with scikit-learn in Python",
    "section": "",
    "text": "Machine learning with scikit-learn in Python\n\nposit::conf(2025)\nby Tiffany Timbers and Katie Burak\n\nüóìÔ∏è September 16, 2025  ‚è∞ 09:00 - 17:00 üè® ROOM TBD ‚úçÔ∏è pos.it/conf\n\n\n\n\nTime\nActivity\n\n\n\n\n09:00 - 10:30\nSession 1\n\n\n10:30 - 11:00\nCoffee break\n\n\n11:00 - 12:30\nSession 2\n\n\n12:30 - 13:30\nLunch break\n\n\n13:30 - 15:00\nSession 3\n\n\n15:00 - 15:30\nCoffee break\n\n\n15:30 - 17:00\nSession 4\n\n\n\n\n This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "materials/slides/regression.html#session-learning-objectives",
    "href": "materials/slides/regression.html#session-learning-objectives",
    "title": "Regression",
    "section": "Session learning objectives",
    "text": "Session learning objectives\n\nRecognize situations where a regression analysis would be appropriate for making predictions.\nExplain the K-nearest neighbors (K-NN) regression algorithm and describe how it differs from K-NN classification.\nDescribe the advantages and disadvantages of K-nearest neighbors and linear regression.\nUse Python to fit linear regression models on training data.\nEvaluate the linear regression model on test data.\nDescribe how linear regression is affected by outliers and multicollinearity.\nLearn how to apply LASSO regression (L1 regularization) for feature selection and regularization to improve model performance.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#the-regression-problem",
    "href": "materials/slides/regression.html#the-regression-problem",
    "title": "Regression",
    "section": "The regression problem",
    "text": "The regression problem\n\nPredictive problem\nUse past information to predict future observations\nPredict numerical values instead of categorical values\n\nExamples:\n\nRace time in the Boston marathon\nsize of a house to predict its sale price",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#regression-methods",
    "href": "materials/slides/regression.html#regression-methods",
    "title": "Regression",
    "section": "Regression Methods",
    "text": "Regression Methods\nIn this workshop:\n\nK-nearest neighbors (brief overview)\nLinear regression\nL1 regularization",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#classification-similarities-to-regression",
    "href": "materials/slides/regression.html#classification-similarities-to-regression",
    "title": "Regression",
    "section": "Classification similarities to regression",
    "text": "Classification similarities to regression\nSimilarities:\n\nPredict a new observation‚Äôs response variable based on past observations\nSplit the data into training and test sets\nUse cross-validation to evaluate different choices of model parameters \n\nDifference:\n\nPredicting numerical variables instead of categorical variables",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#explore-a-data-set",
    "href": "materials/slides/regression.html#explore-a-data-set",
    "title": "Regression",
    "section": "Explore a data set",
    "text": "Explore a data set\n932 real estate transactions in Sacramento, California\n\nCan we use the size of a house in the Sacramento, CA area to predict its sale price?",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#package-setup",
    "href": "materials/slides/regression.html#package-setup",
    "title": "Regression",
    "section": "Package setup",
    "text": "Package setup",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#data",
    "href": "materials/slides/regression.html#data",
    "title": "Regression",
    "section": "Data",
    "text": "Data",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#price-vs-sq.ft",
    "href": "materials/slides/regression.html#price-vs-sq.ft",
    "title": "Regression",
    "section": "Price vs Sq.Ft",
    "text": "Price vs Sq.Ft",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#sample-of-data",
    "href": "materials/slides/regression.html#sample-of-data",
    "title": "Regression",
    "section": "Sample of data",
    "text": "Sample of data",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#sample-k-nn-example",
    "href": "materials/slides/regression.html#sample-k-nn-example",
    "title": "Regression",
    "section": "Sample: K-NN Example",
    "text": "Sample: K-NN Example\n\n\nHouse price of 2000\n\n\n\n\n\n\n\n5 closest neighbors",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#make-and-visualize-prediction",
    "href": "materials/slides/regression.html#make-and-visualize-prediction",
    "title": "Regression",
    "section": "Make and visualize prediction",
    "text": "Make and visualize prediction",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#splitting-the-data",
    "href": "materials/slides/regression.html#splitting-the-data",
    "title": "Regression",
    "section": "Splitting the data",
    "text": "Splitting the data\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nWe are not specifying the stratify argument. The train_test_split() function cannot stratify on a quantitative variable",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#metric-rmspe",
    "href": "materials/slides/regression.html#metric-rmspe",
    "title": "Regression",
    "section": "Metric: RMS(P)E",
    "text": "Metric: RMS(P)E\nRoot Mean Square (Prediction) Error\n\\[\\text{RMSPE} = \\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\]\nwhere:\n\n\\(n\\) is the number of observations,\n\\(y_i\\) is the observed value for the \\(i^\\text{th}\\) observation, and\n\\(\\hat{y}_i\\) is the forecasted/predicted value for the \\(i^\\text{th}\\) observation.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#metric-visualize",
    "href": "materials/slides/regression.html#metric-visualize",
    "title": "Regression",
    "section": "Metric: Visualize",
    "text": "Metric: Visualize",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#rmspe-vs-rmse",
    "href": "materials/slides/regression.html#rmspe-vs-rmse",
    "title": "Regression",
    "section": "RMSPE vs RMSE",
    "text": "RMSPE vs RMSE\nRoot Mean Square (Prediction) Error\n\nRMSPE: the error calculated on the non-training dataset\nRMSE: the error calcualted on the training dataset\n\nThis notation is a statistics distinction, you will most likely see RMSPE written as RMSE.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#choosing-k",
    "href": "materials/slides/regression.html#choosing-k",
    "title": "Regression",
    "section": "Choosing \\(k\\)",
    "text": "Choosing \\(k\\)\nAs we did previously, we can use cross-validation and select the value of \\(k\\) that yields the lowest RMSE to choose the best model.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#evaluating-on-the-test-set",
    "href": "materials/slides/regression.html#evaluating-on-the-test-set",
    "title": "Regression",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\n\nThen, as we did before, we retrain the K-NN regression model on the entire training data set using the best \\(k\\).\n\n\nThe code for running cross-validation to get sacr_gridsearch is included at the end of the slides for your reference.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#final-best-k-model",
    "href": "materials/slides/regression.html#final-best-k-model",
    "title": "Regression",
    "section": "Final best K model",
    "text": "Final best K model\nPredicted values of house price (orange line) for the final K-NN regression model.\n\n\nK-NN regression supports multiple predictors - see the end of the slides for Python code demonstrating multivariable K-NN regression.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#strengths-and-limitations-of-k-nn-regression",
    "href": "materials/slides/regression.html#strengths-and-limitations-of-k-nn-regression",
    "title": "Regression",
    "section": "Strengths and limitations of K-NN regression",
    "text": "Strengths and limitations of K-NN regression\nStrengths:\n\nsimple, intuitive algorithm\nrequires few assumptions about what the data must look like\nworks well with non-linear relationships (i.e., if the relationship is not a straight line)\n\nWeaknesses:\n\nvery slow as the training data gets larger\nmay not perform well with a large number of predictors\nmay not predict well beyond the range of values input in your training data",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#k-nn-regression-in-scikit-learn",
    "href": "materials/slides/regression.html#k-nn-regression-in-scikit-learn",
    "title": "Regression",
    "section": "K-NN regression in scikit-learn",
    "text": "K-NN regression in scikit-learn\nWe won‚Äôt be covering K-NN regression in detail, as the process is very similar to K-NN classification. The main differences are summarized below:\n\n\n\n\nFeature\nK-NN Class.\nK-NN Regr.\n\n\n\n\nTarget\nCategorical\nContinuous\n\n\nLogic\nMajority vote\nMean / w.avg\n\n\nClass\nKNeighborsClassifier\nKNeighborsRegressor\n\n\nOutput\nLabel\nValue\n\n\nUse Case\nClassification\nRegression\n\n\nMetrics\nAccuracy, Precision, Recall\nRMSE",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#linear-regression",
    "href": "materials/slides/regression.html#linear-regression",
    "title": "Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nAddresses the limitations from KNN regression\nProvides an interpretable mathematical equation that describes the relationship between the predictor and response variables\nCreates a straight line of best fit through the training data\n\n\n\n\n\n\n\nNote\n\n\nLogistic regression is the linear model we can use for binary classification",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#sacramento-real-estate",
    "href": "materials/slides/regression.html#sacramento-real-estate",
    "title": "Regression",
    "section": "Sacramento real estate",
    "text": "Sacramento real estate",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#sacramento-real-estate-best-fit-line",
    "href": "materials/slides/regression.html#sacramento-real-estate-best-fit-line",
    "title": "Regression",
    "section": "Sacramento real estate: best fit line",
    "text": "Sacramento real estate: best fit line\n\n\n\n\n\n\n\n\n\nThe equation for the line is:\n\\[\\text{house sale price} = \\beta_0 + \\beta_1 \\cdot (\\text{house size})+\\varepsilon,\\] where\n\n\\(\\beta_0\\) is the vertical intercept of the line (the price when house size is 0)\n\\(\\beta_1\\) is the slope of the line (how quickly the price increases as you increase house size)\n\\(\\varepsilon\\) is the random error term",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#sacramento-real-estate-prediction",
    "href": "materials/slides/regression.html#sacramento-real-estate-prediction",
    "title": "Regression",
    "section": "Sacramento real estate: Prediction",
    "text": "Sacramento real estate: Prediction",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#estimating-a-line",
    "href": "materials/slides/regression.html#estimating-a-line",
    "title": "Regression",
    "section": "Estimating a line",
    "text": "Estimating a line",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#what-makes-the-best-line",
    "href": "materials/slides/regression.html#what-makes-the-best-line",
    "title": "Regression",
    "section": "What makes the best line?",
    "text": "What makes the best line?\nTo define the best line we need to know how to measure the distance of the points to the line!\nWhich of the following criteria would you choose to define ‚Äúdistance of a point to the line‚Äù?\n\n\nFigure by Dr.¬†Joel Ostblom",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#least-squares",
    "href": "materials/slides/regression.html#least-squares",
    "title": "Regression",
    "section": "Least squares",
    "text": "Least squares\nLeast Squares method minimizes the sum of the squares of the residuals. The residuals are the difference between the observed value of the response (\\(y_i\\)) and the predicted value of the response (\\(\\hat{y}_i\\)):\n\\[r_i = y_i-\\hat{y}_i\\]\n\n\n\n\n\n\n\nThe residuals are the vertical distances of each point to the estimated line",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#sum-of-squared-errors",
    "href": "materials/slides/regression.html#sum-of-squared-errors",
    "title": "Regression",
    "section": "Sum of squared errors",
    "text": "Sum of squared errors\nWhen estimating the regression equation, we minimize the sum of squared errors (SSE), defined as\n\\[SSE = \\sum_{i=1}^n (y_i-\\hat{y_i})^2=\\sum_{i=1}^n r_i^2.\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#linear-regression-in-python",
    "href": "materials/slides/regression.html#linear-regression-in-python",
    "title": "Regression",
    "section": "Linear regression in Python",
    "text": "Linear regression in Python\nThe scikit-learn pattern still applies:\n\nCreate a training and test set\nInstantiate a model\nFit the model on training\nUse model on testing set",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#linear-regression-train-test-split",
    "href": "materials/slides/regression.html#linear-regression-train-test-split",
    "title": "Regression",
    "section": "Linear regression: Train test split",
    "text": "Linear regression: Train test split",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#linear-regression-fit-the-model",
    "href": "materials/slides/regression.html#linear-regression-fit-the-model",
    "title": "Regression",
    "section": "Linear regression: Fit the model",
    "text": "Linear regression: Fit the model\n\n\n\n\n\n\n\n\\(\\text{house sale price} =\\) 137.29 \\(+\\) 15642.31 \\(\\cdot (\\text{house size}).\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#linear-regression-predictions",
    "href": "materials/slides/regression.html#linear-regression-predictions",
    "title": "Regression",
    "section": "Linear regression: Predictions",
    "text": "Linear regression: Predictions",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#linear-regression-plot",
    "href": "materials/slides/regression.html#linear-regression-plot",
    "title": "Regression",
    "section": "Linear regression: Plot",
    "text": "Linear regression: Plot",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#standarization",
    "href": "materials/slides/regression.html#standarization",
    "title": "Regression",
    "section": "Standarization",
    "text": "Standarization\n\nWe did not need to standarize like we did for KNN.\nIn linear regression, if we standarize, we convert all the units to unit-less standard deviations\nStandarization in linear regression does not change the fit of the model\n\nIt will, however, change the coefficients!",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#multiple-linear-regression",
    "href": "materials/slides/regression.html#multiple-linear-regression",
    "title": "Regression",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nMore predictor variables! (More does not always mean better‚Ä¶)\nWhen \\(p\\), the number of variables, is greater than 1, we have multiple linear regression.\n\n\nNote: We will talk about categorical predictors later in the workshop",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#sacramento-real-estate-2-predictors",
    "href": "materials/slides/regression.html#sacramento-real-estate-2-predictors",
    "title": "Regression",
    "section": "Sacramento real estate: 2 predictors",
    "text": "Sacramento real estate: 2 predictors",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#sacramento-real-estate-coefficients",
    "href": "materials/slides/regression.html#sacramento-real-estate-coefficients",
    "title": "Regression",
    "section": "Sacramento real estate: Coefficients",
    "text": "Sacramento real estate: Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\text{house sale price} = 53180.27 + 154.59\\cdot(\\text{house size}) -20333.43\\cdot(\\text{bedrooms}),\\] where:\n\nIntercept (\\(\\hat{\\beta}_0=53180.27\\)): Predicted sale price when house size = 0 and bedrooms = 0 (not meaningful in reality, but needed for the equation).\nHouse size (\\(\\hat{\\beta}_1=154.59\\)): Holding bedrooms constant, each extra unit of size (e.g., square foot) increases the price by $154.59 on average.\nNumber of bedrooms (\\(\\hat{\\beta}_2=-20333.43\\)): Holding size constant, each additional bedroom reduces price by about $20,333.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#more-variables-make-it-harder-to-visualize",
    "href": "materials/slides/regression.html#more-variables-make-it-harder-to-visualize",
    "title": "Regression",
    "section": "More variables make it harder to visualize",
    "text": "More variables make it harder to visualize",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#sacramento-real-estate-mlm-rmspe",
    "href": "materials/slides/regression.html#sacramento-real-estate-mlm-rmspe",
    "title": "Regression",
    "section": "Sacramento real estate: mlm rmspe",
    "text": "Sacramento real estate: mlm rmspe",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#outliers-and-multicollinearity",
    "href": "materials/slides/regression.html#outliers-and-multicollinearity",
    "title": "Regression",
    "section": "Outliers and Multicollinearity",
    "text": "Outliers and Multicollinearity\n\nOutliers: extreme values that can move the best fit line\nMulticollinearity: variables that are highly correlated to one another",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#outliers",
    "href": "materials/slides/regression.html#outliers",
    "title": "Regression",
    "section": "Outliers",
    "text": "Outliers\n\n\nSubset\n\n\n\n\n\n\n\nFull data",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#multicollinearity",
    "href": "materials/slides/regression.html#multicollinearity",
    "title": "Regression",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity means that some (or all) of the explanatory variables are linearly related!\nWhen this happens, the coefficient estimates are very ‚Äúunstable‚Äù and the contribution of one variable gets mixed with that of another variable correlated with it.\nEssentially, the plane of best fit has regression coefficients that are very sensitive to the exact values in the data.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#feature-selection",
    "href": "materials/slides/regression.html#feature-selection",
    "title": "Regression",
    "section": "Feature selection",
    "text": "Feature selection\n\nIn modern datasets, we often have many features (sometimes more than the number of observations).\nNot all features are informative or relevant.\nIncluding irrelevant predictors can lead to:\n\nOverfitting\nMulticollinearity\nPoor generalization to new data\n\nWe need methods that can automatically select features while fitting the model‚Ä¶",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#l1-regularization",
    "href": "materials/slides/regression.html#l1-regularization",
    "title": "Regression",
    "section": "L1-regularization",
    "text": "L1-regularization\n\nL1-regularization, often referred to as LASSO (Least Absolute Shrinkage and Selection Operator), adds an L1 penalty to the least squares loss:\n\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\arg\\min_{\\boldsymbol{\\beta}} \\left\\{\n\\sum_{i=1}^n (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^p |\\beta_j|\n\\right\\}\n\\]\n\n\\(\\lambda \\geq 0\\) is a tuning parameter that controls the strength of the penalty\nWith the L1 penalty, many coefficients get set equal to zero.\nThus, LASSO performs variable selection and regularization.\n\n\n\n\n\n\n\nNote\n\n\nStandardization is necessary in penalized regression because penalty terms are scale-sensitive.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#l1-regularization-in-scikit-learn",
    "href": "materials/slides/regression.html#l1-regularization-in-scikit-learn",
    "title": "Regression",
    "section": "L1-regularization in scikit-learn",
    "text": "L1-regularization in scikit-learn",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#choosing-the-shrinkage-parameter",
    "href": "materials/slides/regression.html#choosing-the-shrinkage-parameter",
    "title": "Regression",
    "section": "Choosing the shrinkage parameter",
    "text": "Choosing the shrinkage parameter\n\nThe \\(\\alpha\\) parameter controls the strength of the L1 penalty:\n\nLarger \\(\\alpha\\) ‚Üí stronger regularization ‚Üí more coefficients shrunk to zero\nSmaller \\(\\alpha\\) ‚Üí less regularization ‚Üí more complex model\n\nProper selection of \\(\\alpha\\) is critical for balancing bias and variance.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#tuning-alpha-with-cross-validation",
    "href": "materials/slides/regression.html#tuning-alpha-with-cross-validation",
    "title": "Regression",
    "section": "Tuning \\(\\alpha\\) with cross-validation",
    "text": "Tuning \\(\\alpha\\) with cross-validation\n\nJust like tuning \\(k\\) in KNN, we can select \\(\\alpha\\) by evaluating performance on validation sets.\nExample workflow with GridSearchCV:",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#clicker-questions",
    "href": "materials/slides/regression.html#clicker-questions",
    "title": "Regression",
    "section": "Clicker questions",
    "text": "Clicker questions\n\nhttps://www.menti.com/algmf2xzfs8g9\nAlternatively, go to menti.com and use code 1668 5101",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#additional-resources",
    "href": "materials/slides/regression.html#additional-resources",
    "title": "Regression",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe Regression I: K-nearest neighbors and Regression II: linear regression chapters of Data Science: A First Introduction (Python Edition) by Tiffany Timbers, Trevor Campbell, Melissa Lee, Joel Ostblom, Lindsey Heagy contains all the content presented here with a detailed narrative.\nThe scikit-learn website is an excellent reference for more details on, and advanced usage of, the functions and packages in this lesson. Aside from that, it also offers many useful tutorials to get you started.\nAn Introduction to Statistical Learning by Gareth James Daniela Witten Trevor Hastie, and Robert Tibshirani provides a great next stop in the process of learning about classification. Chapter 3 discusses lienar regression in more depth. As well as how it comares to K-nearest neighbors.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#references",
    "href": "materials/slides/regression.html#references",
    "title": "Regression",
    "section": "References",
    "text": "References\nThomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21‚Äì27, 1967.\nEvelyn Fix and Joseph Hodges. Discriminatory analysis. nonparametric discrimination: consistency properties. Technical Report, USAF School of Aviation Medicine, Randolph Field, Texas, 1951.",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#code-for-cv-in-k-nn-regression",
    "href": "materials/slides/regression.html#code-for-cv-in-k-nn-regression",
    "title": "Regression",
    "section": "Code for CV in K-NN regression",
    "text": "Code for CV in K-NN regression",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#multivariable-k-nn-regression-preprocessor",
    "href": "materials/slides/regression.html#multivariable-k-nn-regression-preprocessor",
    "title": "Regression",
    "section": "Multivariable K-NN regression: Preprocessor",
    "text": "Multivariable K-NN regression: Preprocessor",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#multivariable-k-nn-regression-cv",
    "href": "materials/slides/regression.html#multivariable-k-nn-regression-cv",
    "title": "Regression",
    "section": "Multivariable K-NN regression: CV",
    "text": "Multivariable K-NN regression: CV",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#multivariable-k-nn-regression-best-k",
    "href": "materials/slides/regression.html#multivariable-k-nn-regression-best-k",
    "title": "Regression",
    "section": "Multivariable K-NN regression: Best K",
    "text": "Multivariable K-NN regression: Best K",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#multivariable-k-nn-regression-best-model",
    "href": "materials/slides/regression.html#multivariable-k-nn-regression-best-model",
    "title": "Regression",
    "section": "Multivariable K-NN regression: Best model",
    "text": "Multivariable K-NN regression: Best model\n\n\n\n\n\n\nBest K\n\n\n\n\n\n\nBest RMSPE",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#multivariable-k-nn-regression-test-data",
    "href": "materials/slides/regression.html#multivariable-k-nn-regression-test-data",
    "title": "Regression",
    "section": "Multivariable K-NN regression: Test data",
    "text": "Multivariable K-NN regression: Test data",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/regression.html#multivariable-k-nn-regression-visualize",
    "href": "materials/slides/regression.html#multivariable-k-nn-regression-visualize",
    "title": "Regression",
    "section": "Multivariable K-NN regression: Visualize",
    "text": "Multivariable K-NN regression: Visualize",
    "crumbs": [
      "Home",
      "Slides",
      "Regression"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#tree-based-methods",
    "href": "materials/slides/ensembles.html#tree-based-methods",
    "title": "Tree-based and ensemble models",
    "section": "Tree-based methods",
    "text": "Tree-based methods\n\n\n\n\n\n\n\nAlgorithms that stratifying or segmenting the predictor space into a number of simple regions.\nWe call these algorithms decision-tree methods because the decisions used to segment the predictor space can be summarized in a tree.\nDecision trees on their own, are very explainable and intuitive, but not very powerful at predicting.\nHowever, there are extensions of decision trees, such as random forest and boosted trees, which are very powerful at predicting. We will demonstrate two of these in this session.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#decision-trees",
    "href": "materials/slides/ensembles.html#decision-trees",
    "title": "Tree-based and ensemble models",
    "section": "Decision trees",
    "text": "Decision trees\n\nDecision Trees by Jared Wilber & Luc√≠a Santamar√≠a",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#classification-decision-trees",
    "href": "materials/slides/ensembles.html#classification-decision-trees",
    "title": "Tree-based and ensemble models",
    "section": "Classification Decision trees",
    "text": "Classification Decision trees\n\nUse recursive binary splitting to grow a classification tree (splitting of the predictor space into \\(J\\) distinct, non-overlapping regions).\nFor every observation that falls into the region \\(R_j\\) , we make the same prediction, which is the majority vote for the training observations in \\(R_j\\).\nWhere to split the predictor space is done in a top-down and greedy manner, and in practice for classification, the best split at any point in the algorithm is one that minimizes the Gini index (a measure of node purity).\nDecision trees are useful because they are very interpretable.\nA limitation of decision trees is that theyn tend to overfit, so in practice we use cross-validation to tune a hyperparameter, \\(\\alpha\\), to find the optimal, pruned tree.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#example-the-heart-data-set",
    "href": "materials/slides/ensembles.html#example-the-heart-data-set",
    "title": "Tree-based and ensemble models",
    "section": "Example: the heart data set",
    "text": "Example: the heart data set\n\nLet‚Äôs consider a situation where we‚Äôd like to be able to predict the presence of heart disease (AHD) in patients, based off 13 measured characteristics.\nThe heart data set contains a binary outcome for heart disease for patients who presented with chest pain.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#example-the-heart-data-set-contd",
    "href": "materials/slides/ensembles.html#example-the-heart-data-set-contd",
    "title": "Tree-based and ensemble models",
    "section": "Example: the heart data set (cont‚Äôd)",
    "text": "Example: the heart data set (cont‚Äôd)\nAn angiographic test was performed and a label for AHD of Yes was labelled to indicate the presence of heart disease, otherwise the label was No.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#do-we-have-a-class-imbalance",
    "href": "materials/slides/ensembles.html#do-we-have-a-class-imbalance",
    "title": "Tree-based and ensemble models",
    "section": "Do we have a class imbalance?",
    "text": "Do we have a class imbalance?\nIt‚Äôs always important to check this, as it may impact your splitting and/or modeling decisions.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#data-splitting",
    "href": "materials/slides/ensembles.html#data-splitting",
    "title": "Tree-based and ensemble models",
    "section": "Data splitting",
    "text": "Data splitting\nLet‚Äôs split the data into training and test sets:",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#categorical-variables",
    "href": "materials/slides/ensembles.html#categorical-variables",
    "title": "Tree-based and ensemble models",
    "section": "Categorical variables",
    "text": "Categorical variables\n\n\n\nThis is our first case of seeing categorical predictor variables, can we treat them the same as numerical ones? No!\nIn scikit-learn we must perform one-hot encoding\n\n\n\nSource: https://scales.arabpsychology.com/stats/how-can-i-perform-one-hot-encoding-in-r/",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#look-at-the-data-again",
    "href": "materials/slides/ensembles.html#look-at-the-data-again",
    "title": "Tree-based and ensemble models",
    "section": "Look at the data again",
    "text": "Look at the data again\nWhich columns do we need to standardize?\nWhich do we need to one-hot encode?",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#one-hot-encoding-pre-processing",
    "href": "materials/slides/ensembles.html#one-hot-encoding-pre-processing",
    "title": "Tree-based and ensemble models",
    "section": "One hot encoding & pre-processing",
    "text": "One hot encoding & pre-processing\n\n\n\n\n\n\n\nhandle_unknown = \"ignore\" handles the case where categories exist in the test data, which were missing in the training set. Specifically, it sets the value for those to 0 for all cases of the category.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#fitting-a-dummy-classifier",
    "href": "materials/slides/ensembles.html#fitting-a-dummy-classifier",
    "title": "Tree-based and ensemble models",
    "section": "Fitting a dummy classifier",
    "text": "Fitting a dummy classifier",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#put-the-mean-cross-validated-error-in-a-data-frame",
    "href": "materials/slides/ensembles.html#put-the-mean-cross-validated-error-in-a-data-frame",
    "title": "Tree-based and ensemble models",
    "section": "Put the mean cross-validated error in a data frame",
    "text": "Put the mean cross-validated error in a data frame",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#fitting-a-decision-tree",
    "href": "materials/slides/ensembles.html#fitting-a-decision-tree",
    "title": "Tree-based and ensemble models",
    "section": "Fitting a decision tree",
    "text": "Fitting a decision tree",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#add-the-mean-cross-validated-error-to-our-results-data-frame",
    "href": "materials/slides/ensembles.html#add-the-mean-cross-validated-error-to-our-results-data-frame",
    "title": "Tree-based and ensemble models",
    "section": "Add the mean cross-validated error to our results data frame",
    "text": "Add the mean cross-validated error to our results data frame",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#can-we-do-better",
    "href": "materials/slides/ensembles.html#can-we-do-better",
    "title": "Tree-based and ensemble models",
    "section": "Can we do better?",
    "text": "Can we do better?\n\nWe could tune some decision tree parameters (e.g., alpha, maximum tree depth, etc)‚Ä¶\nWe could also try a different tree-based method!\nThe Random Forest Algorithm by Jenny Yeon & Jared Wilber",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#the-random-forest-algorithm",
    "href": "materials/slides/ensembles.html#the-random-forest-algorithm",
    "title": "Tree-based and ensemble models",
    "section": "The Random Forest Algorithm",
    "text": "The Random Forest Algorithm\n\nBuild a number of decision trees on bootstrapped training samples.\nWhen building the trees from the bootstrapped samples, at each stage of splitting, the best splitting is computed using a randomly selected subset of the features.\nTake the majority votes across all the trees for the final prediction.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#random-forest-in-scikit-learn",
    "href": "materials/slides/ensembles.html#random-forest-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Random forest in scikit-learn",
    "text": "Random forest in scikit-learn",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#add-the-mean-cross-validated-error-to-our-results-data-frame-1",
    "href": "materials/slides/ensembles.html#add-the-mean-cross-validated-error-to-our-results-data-frame-1",
    "title": "Tree-based and ensemble models",
    "section": "Add the mean cross-validated error to our results data frame",
    "text": "Add the mean cross-validated error to our results data frame",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#can-we-do-better-1",
    "href": "materials/slides/ensembles.html#can-we-do-better-1",
    "title": "Tree-based and ensemble models",
    "section": "Can we do better?",
    "text": "Can we do better?\n\nRandom forest can be tuned a several important parameters, including:\n\nn_estimators: number of decision trees (higher = more complexity)\nmax_depth: max depth of each decision tree (higher = more complexity)\nmax_features: the number of features you get to look at each split (higher = more complexity)\n\nWe can use GridSearchCV to search for the optimal parameters for these, as we did for \\(K\\) in \\(K\\)-nearest neighbors.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#tuning-random-forest-in-scikit-learn",
    "href": "materials/slides/ensembles.html#tuning-random-forest-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Tuning random forest in scikit-learn",
    "text": "Tuning random forest in scikit-learn",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#comparing-to-our-other-models",
    "href": "materials/slides/ensembles.html#comparing-to-our-other-models",
    "title": "Tree-based and ensemble models",
    "section": "Comparing to our other models",
    "text": "Comparing to our other models\nHow did the tuned Random Forest compare against the other models we tried?",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#boosting",
    "href": "materials/slides/ensembles.html#boosting",
    "title": "Tree-based and ensemble models",
    "section": "Boosting",
    "text": "Boosting\n\nNo randomization.\nThe key idea is combining many simple models called weak learners, to create a strong learner.\nThey combine multiple shallow (depth 1 to 5) decision trees.\nThey build trees in a serial manner, where each tree tries to correct the mistakes of the previous one.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#tuning-boosted-classifiers-with-scikit-learn",
    "href": "materials/slides/ensembles.html#tuning-boosted-classifiers-with-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Tuning Boosted Classifiers with scikit-learn",
    "text": "Tuning Boosted Classifiers with scikit-learn\n\nHistGradientBoostingClassifier can be tuned a several important parameters, including:\n\nmax_iter: number of decision trees (higher = more complexity)\nmax_depth: max depth of each decision tree (higher = more complexity)\nlearning_rate: the shrinkage parameter which controls the rate at which boosting learns. Values between 0.01 or 0.001 are typical.\n\nWe can use GridSearchCV to search for the optimal parameters for these, as we did for the parameters in Random Forest.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#tuning-boosted-classifiers-with-scikit-learn-contd",
    "href": "materials/slides/ensembles.html#tuning-boosted-classifiers-with-scikit-learn-contd",
    "title": "Tree-based and ensemble models",
    "section": "Tuning Boosted Classifiers with scikit-learn (cont‚Äôd)",
    "text": "Tuning Boosted Classifiers with scikit-learn (cont‚Äôd)",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#histgradientboostingclassifier-results",
    "href": "materials/slides/ensembles.html#histgradientboostingclassifier-results",
    "title": "Tree-based and ensemble models",
    "section": "HistGradientBoostingClassifier results",
    "text": "HistGradientBoostingClassifier results\nHow did the HistGradientBoostingClassifier compare against the other models we tried?",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#how-do-we-choose-the-final-model",
    "href": "materials/slides/ensembles.html#how-do-we-choose-the-final-model",
    "title": "Tree-based and ensemble models",
    "section": "How do we choose the final model?",
    "text": "How do we choose the final model?\n\nRemember, what is your question or application?\nA good rule when models are not very different, what is the simplest model that does well?\nLook at other metrics that are important to you (not just the metric you used for tuning your model), remember precision & recall, for example.\nRemember - no peaking at the test set until you choose! And then, you should only look at the test set for one model!",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#precision-and-recall-on-the-tuned-random-forest-model",
    "href": "materials/slides/ensembles.html#precision-and-recall-on-the-tuned-random-forest-model",
    "title": "Tree-based and ensemble models",
    "section": "Precision and recall on the tuned random forest model",
    "text": "Precision and recall on the tuned random forest model",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#precision-and-recall-contd",
    "href": "materials/slides/ensembles.html#precision-and-recall-contd",
    "title": "Tree-based and ensemble models",
    "section": "Precision and recall cont‚Äôd",
    "text": "Precision and recall cont‚Äôd\n\nWhat do we think? Is this model ready for production in a diagnostic setting?\nHow could we improve it further?",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#feature-importances-key-points",
    "href": "materials/slides/ensembles.html#feature-importances-key-points",
    "title": "Tree-based and ensemble models",
    "section": "Feature importances: key points",
    "text": "Feature importances: key points\n\nDecision trees are very interpretable (decision rules!), however in ensemble models (e.g., Random Forest and Boosting) there are many trees - individual decision rules are not as meaningful‚Ä¶\nInstead, we can calculate feature importances as the total decrease in impurity for all splits involving that feature, weighted by the number of samples involved in those splits, normalized and averaged over all the trees.\nThese are calculated on the training set, as that is the set the model is trained on.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#feature-importances-notes-of-caution",
    "href": "materials/slides/ensembles.html#feature-importances-notes-of-caution",
    "title": "Tree-based and ensemble models",
    "section": "Feature importances: Notes of caution!",
    "text": "Feature importances: Notes of caution!\n\nFeature importances can be unreliable with both highly cardinal, and multicollinear features.\nUnlike the linear model coefficients, feature importances do not have a sign! They tell us about importance, but not an ‚Äúup or down‚Äù.\nIncreasing a feature may cause the prediction to first go up, and then go down.\nAlternatives to feature importance to understanding models exist, such as post-hoc explanations (sometimes called ‚Äúexplainable AI‚Äù, see Interpretable Machine Learning by Christoph Molnar for an introduction)",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#feature-importances-in-scikit-learn",
    "href": "materials/slides/ensembles.html#feature-importances-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Feature importances in scikit-learn",
    "text": "Feature importances in scikit-learn",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#feature-importances-in-scikit-learn-contd",
    "href": "materials/slides/ensembles.html#feature-importances-in-scikit-learn-contd",
    "title": "Tree-based and ensemble models",
    "section": "Feature importances in scikit-learn (cont‚Äôd)",
    "text": "Feature importances in scikit-learn (cont‚Äôd)",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#visualizing-the-results",
    "href": "materials/slides/ensembles.html#visualizing-the-results",
    "title": "Tree-based and ensemble models",
    "section": "Visualizing the results",
    "text": "Visualizing the results",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#visualizing-the-results-1",
    "href": "materials/slides/ensembles.html#visualizing-the-results-1",
    "title": "Tree-based and ensemble models",
    "section": "Visualizing the results",
    "text": "Visualizing the results",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#evaluating-on-the-test-set",
    "href": "materials/slides/ensembles.html#evaluating-on-the-test-set",
    "title": "Tree-based and ensemble models",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\nPredict on the test set:",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#evaluating-on-the-test-set-1",
    "href": "materials/slides/ensembles.html#evaluating-on-the-test-set-1",
    "title": "Tree-based and ensemble models",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\nExamine accuracy, precision and recall:",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#evaluating-on-the-test-set-2",
    "href": "materials/slides/ensembles.html#evaluating-on-the-test-set-2",
    "title": "Tree-based and ensemble models",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#keep-learning",
    "href": "materials/slides/ensembles.html#keep-learning",
    "title": "Tree-based and ensemble models",
    "section": "Keep learning!",
    "text": "Keep learning!\n\n\n https://python.datasciencebook.ca/\n\n https://www.statlearning.com/",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#local-installation",
    "href": "materials/slides/ensembles.html#local-installation",
    "title": "Tree-based and ensemble models",
    "section": "Local installation",
    "text": "Local installation\n\nUsing Docker: Data Science: A First Introduction (Python Edition) Installation Instructions\nUsing conda: UBC MDS Installation Instructions",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#additional-resources",
    "href": "materials/slides/ensembles.html#additional-resources",
    "title": "Tree-based and ensemble models",
    "section": "Additional resources",
    "text": "Additional resources\n\nThe UBC DSCI 573 (Feature and Model Selection notes) chapter of Data Science: A First Introduction (Python Edition) by Varada Kolhatkar and Joel Ostblom. These notes cover classification and regression metrics, advanced variable selection and more on ensembles.\nThe scikit-learn website is an excellent reference for more details on, and advanced usage of, the functions and packages in the past two chapters. Aside from that, it also offers many useful tutorials to get you started.\nAn Introduction to Statistical Learning {cite:p}james2013introduction provides a great next stop in the process of learning about classification. Chapter 4 discusses additional basic techniques for classification that we do not cover, such as logistic regression, linear discriminant analysis, and naive Bayes.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/ensembles.html#references",
    "href": "materials/slides/ensembles.html#references",
    "title": "Tree-based and ensemble models",
    "section": "References",
    "text": "References\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani and Jonathan Taylor. An Introduction to Statistical Learning with Applications in Python. Springer, 1st edition, 2023. URL: https://www.statlearning.com/.\nKolhatkar, V., and Ostblom, J. (2024). UBC DSCI 573: Feature and Model Selection course notes. URL: https://ubc-mds.github.io/DSCI_573_feat-model-select\nPedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825‚Äì2830.",
    "crumbs": [
      "Home",
      "Slides",
      "Tree-based and ensemble models"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#introductions",
    "href": "materials/slides/classification1.html#introductions",
    "title": "Classification I: training & predicting",
    "section": "Introductions",
    "text": "Introductions\n\n\n\n\nTiffany Timbers\nAssociate Professor of Teaching,  Dept. of Statistics, UBC\n\n\n\nKatie Burak\nAssistant Professor of Teaching,  Dept. of Statistics, UBC",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#helpful-resources",
    "href": "materials/slides/classification1.html#helpful-resources",
    "title": "Classification I: training & predicting",
    "section": "Helpful resources",
    "text": "Helpful resources\n\nWorkshop material: https://posit-conf-2025.github.io/scikit-learn/\nhttps://python.datasciencebook.ca/\nscikit-learn docs\nKCDS ML course",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#session-learning-objectives",
    "href": "materials/slides/classification1.html#session-learning-objectives",
    "title": "Classification I: training & predicting",
    "section": "Session learning objectives",
    "text": "Session learning objectives\nBy the end of the session, learners will be able to do the following:\n\nRecognize situations where a simple classifier would be appropriate for making predictions.\nExplain the \\(K\\)-nearest neighbor classification algorithm.\nInterpret the output of a classifier.\nDescribe what a training data set is and how it is used in classification.\nGiven a dataset with two explanatory variables/predictors, use \\(K\\)-nearest neighbor classification in Python using the scikit-learn framework to predict the class of a single new observation.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#the-classification-problem",
    "href": "materials/slides/classification1.html#the-classification-problem",
    "title": "Classification I: training & predicting",
    "section": "The classification problem",
    "text": "The classification problem\n\npredicting a categorical class (sometimes called a label) for an observation given its other variables (sometimes called features)\n\n\nDiagnose a patient as healthy or sick\nTag an email as ‚Äúspam‚Äù or ‚Äúnot spam‚Äù\nPredict whether a purchase is fraudulent",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#training-set",
    "href": "materials/slides/classification1.html#training-set",
    "title": "Classification I: training & predicting",
    "section": "Training set",
    "text": "Training set\n\nObservations with known classes that we use as a basis for prediction\n\n\nAssign an observation without a known class (e.g., a new patient)\nTo a class (e.g., diseased or healthy)\n\nHow?\n\nBy similar it is to other observations for which we do know the class\n\n(e.g., previous patients with known diseases and symptoms)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#k-nearest-neighbors",
    "href": "materials/slides/classification1.html#k-nearest-neighbors",
    "title": "Classification I: training & predicting",
    "section": "K-nearest neighbors",
    "text": "K-nearest neighbors\n\nOne of many possible classification methods\n\nKNN, decision trees, support vector machines (SVMs), logistic regression, neural networks, and more;\n\n\n\nPredict observations based on other observations ‚Äúclose‚Äù to it",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#exploring-a-data-set",
    "href": "materials/slides/classification1.html#exploring-a-data-set",
    "title": "Classification I: training & predicting",
    "section": "Exploring a data set",
    "text": "Exploring a data set\nData:\n\ndigitized breast cancer image features, created by Dr.¬†William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian\nEach row:\n\ndiagnosis (benign or malignant)\nseveral other measurements (nucleus texture, perimeter, area, and more)\n\nDiagnosis for each image was conducted by physicians.\n\nFormulate a predictive question:\n\nCan we use the tumor image measurements available to us to predict whether a future tumor image (with unknown diagnosis) shows a benign or malignant tumor?",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#loading-the-cancer-data",
    "href": "materials/slides/classification1.html#loading-the-cancer-data",
    "title": "Classification I: training & predicting",
    "section": "Loading the cancer data",
    "text": "Loading the cancer data\n\n\n\n\n\n\n\nthese values have been standardized (centered and scaled)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#describing-the-variables-in-the-cancer-data-set",
    "href": "materials/slides/classification1.html#describing-the-variables-in-the-cancer-data-set",
    "title": "Classification I: training & predicting",
    "section": "Describing the variables in the cancer data set",
    "text": "Describing the variables in the cancer data set\n\nID: identification number\nClass: the diagnosis (M = malignant or B = benign)\nRadius: the mean of distances from center to points on the perimeter\nTexture: the standard deviation of gray-scale values\nPerimeter: the length of the surrounding contour\nArea: the area inside the contour\nSmoothness: the local variation in radius lengths\nCompactness: the ratio of squared perimeter and area\nConcavity: severity of concave portions of the contour\nConcave Points: the number of concave portions of the contour\nSymmetry: how similar the nucleus is when mirrored\nFractal Dimension: a measurement of how ‚Äúrough‚Äù the perimeter is",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#dataframe-info",
    "href": "materials/slides/classification1.html#dataframe-info",
    "title": "Classification I: training & predicting",
    "section": "DataFrame; info",
    "text": "DataFrame; info",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#series-unique",
    "href": "materials/slides/classification1.html#series-unique",
    "title": "Classification I: training & predicting",
    "section": "Series; unique",
    "text": "Series; unique",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#series-replace",
    "href": "materials/slides/classification1.html#series-replace",
    "title": "Classification I: training & predicting",
    "section": "Series; replace",
    "text": "Series; replace",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#exploring-the-cancer-data",
    "href": "materials/slides/classification1.html#exploring-the-cancer-data",
    "title": "Classification I: training & predicting",
    "section": "Exploring the cancer data",
    "text": "Exploring the cancer data",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#visualization-scatter",
    "href": "materials/slides/classification1.html#visualization-scatter",
    "title": "Classification I: training & predicting",
    "section": "Visualization; scatter",
    "text": "Visualization; scatter\n\n\n\n\n\n\n\nMalignant: upper right-hand corner\nBenign: lower left-hand corner",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#classification-with-k-nearest-neighbors",
    "href": "materials/slides/classification1.html#classification-with-k-nearest-neighbors",
    "title": "Classification I: training & predicting",
    "section": "Classification with K-nearest neighbors",
    "text": "Classification with K-nearest neighbors",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#distances",
    "href": "materials/slides/classification1.html#distances",
    "title": "Classification I: training & predicting",
    "section": "Distances",
    "text": "Distances\n-Compute the distance matrix between each pair from a vector array X and Y using euclidean_distances",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#k-nearest-neighbors-classification",
    "href": "materials/slides/classification1.html#k-nearest-neighbors-classification",
    "title": "Classification I: training & predicting",
    "section": "K-nearest neighbors; classification",
    "text": "K-nearest neighbors; classification\n\nfind the \\(K\\) ‚Äúnearest‚Äù or ‚Äúmost similar‚Äù observations in our training set\npredict new observation based on closest points",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#knn-example-new-point",
    "href": "materials/slides/classification1.html#knn-example-new-point",
    "title": "Classification I: training & predicting",
    "section": "KNN Example: new point",
    "text": "KNN Example: new point",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#knn-example-closest-point",
    "href": "materials/slides/classification1.html#knn-example-closest-point",
    "title": "Classification I: training & predicting",
    "section": "KNN example: closest point",
    "text": "KNN example: closest point\n\nif a point is close to another in the scatter plot, then the perimeter and concavity values are similar, and so we may expect that they would have the same diagnosis",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#knn-example-another-new-point",
    "href": "materials/slides/classification1.html#knn-example-another-new-point",
    "title": "Classification I: training & predicting",
    "section": "KNN Example: another new point",
    "text": "KNN Example: another new point",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#knn-improve-the-prediction-with-k",
    "href": "materials/slides/classification1.html#knn-improve-the-prediction-with-k",
    "title": "Classification I: training & predicting",
    "section": "KNN: improve the prediction with k",
    "text": "KNN: improve the prediction with k\nwe can consider several neighboring points, k=3",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#distance-between-points",
    "href": "materials/slides/classification1.html#distance-between-points",
    "title": "Classification I: training & predicting",
    "section": "Distance between points",
    "text": "Distance between points\n\\[\\mathrm{Distance} = \\sqrt{(a_x -b_x)^2 + (a_y - b_y)^2}\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#distance-between-points-k5",
    "href": "materials/slides/classification1.html#distance-between-points-k5",
    "title": "Classification I: training & predicting",
    "section": "Distance between points: k=5",
    "text": "Distance between points: k=5\n\n3 of the 5 nearest neighbors to our new observation are malignant",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#more-than-two-explanatory-variables-distance-formula",
    "href": "materials/slides/classification1.html#more-than-two-explanatory-variables-distance-formula",
    "title": "Classification I: training & predicting",
    "section": "More than two explanatory variables: distance formula",
    "text": "More than two explanatory variables: distance formula\nThe distance formula becomes\n\\[\\mathrm{Distance} = \\sqrt{(a_{1} -b_{1})^2 + (a_{2} - b_{2})^2 + \\dots + (a_{m} - b_{m})^2}.\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#more-than-two-explanatory-variables-visualize",
    "href": "materials/slides/classification1.html#more-than-two-explanatory-variables-visualize",
    "title": "Classification I: training & predicting",
    "section": "More than two explanatory variables: visualize",
    "text": "More than two explanatory variables: visualize",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#summary-of-k-nearest-neighbors-algorithm",
    "href": "materials/slides/classification1.html#summary-of-k-nearest-neighbors-algorithm",
    "title": "Classification I: training & predicting",
    "section": "Summary of K-nearest neighbors algorithm",
    "text": "Summary of K-nearest neighbors algorithm\nThe K-nearest neighbors algorithm works as follows:\n\nCompute the distance between the new observation and each observation in the training set\nFind the \\(K\\) rows corresponding to the \\(K\\) smallest distances\nClassify the new observation based on a majority vote of the neighbor classes",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#k-nearest-neighbors-with-scikit-learn",
    "href": "materials/slides/classification1.html#k-nearest-neighbors-with-scikit-learn",
    "title": "Classification I: training & predicting",
    "section": "K-nearest neighbors with scikit-learn",
    "text": "K-nearest neighbors with scikit-learn\n\nK-nearest neighbors algorithm is implemented in scikit-learn\n\n\n\n\n\n\n\nNow we can get started with sklearn and KNeighborsClassifier()",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#review-cancer-data",
    "href": "materials/slides/classification1.html#review-cancer-data",
    "title": "Classification I: training & predicting",
    "section": "Review cancer data",
    "text": "Review cancer data",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#scikit-learn-create-model-object",
    "href": "materials/slides/classification1.html#scikit-learn-create-model-object",
    "title": "Classification I: training & predicting",
    "section": "scikit-learn: Create Model Object",
    "text": "scikit-learn: Create Model Object",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#scikit-learn-fit-the-model",
    "href": "materials/slides/classification1.html#scikit-learn-fit-the-model",
    "title": "Classification I: training & predicting",
    "section": "scikit-learn: Fit the model",
    "text": "scikit-learn: Fit the model\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe do not re-assign the variable\nThe arguments are X and y (note the capitialization). This comes from matrix notation.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#scikit-learn-predict",
    "href": "materials/slides/classification1.html#scikit-learn-predict",
    "title": "Classification I: training & predicting",
    "section": "scikit-learn: Predict",
    "text": "scikit-learn: Predict",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#data-preprocessing-scaling",
    "href": "materials/slides/classification1.html#data-preprocessing-scaling",
    "title": "Classification I: training & predicting",
    "section": "Data preprocessing: Scaling",
    "text": "Data preprocessing: Scaling\nFor KNN:\n\nthe scale of each variable (i.e., its size and range of values) matters\ndistance based algorithm\n\nCompare these 2 scenarios:\n\nPerson A (200 lbs, 6ft tall) vs Person B (202 lbs, 6ft tall)\nPerson A (200 lbs, 6ft tall) vs Person B (200 lbs, 8ft tall)\n\nAll have a distance of 2",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#data-preprocessing-centering",
    "href": "materials/slides/classification1.html#data-preprocessing-centering",
    "title": "Classification I: training & predicting",
    "section": "Data preprocessing: Centering",
    "text": "Data preprocessing: Centering\nMany other models:\n\ncenter of each variable (e.g., its mean) matters as well\nDoes not matter as much in KNN:\nPerson A (200 lbs, 6ft tall) vs Person B (202 lbs, 6ft tall)\nPerson A (200 lbs, 6ft tall) vs Person B (200 lbs, 8ft tall)\n\nDifference in weight is in the 10s, difference in height is fractions of a foot.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#data-preprocessing-standardization",
    "href": "materials/slides/classification1.html#data-preprocessing-standardization",
    "title": "Classification I: training & predicting",
    "section": "Data preprocessing: Standardization",
    "text": "Data preprocessing: Standardization\n\nThe mean is used to center, the standard deviation is used to scale\nStandardization: transform the data such that the mean is 0, and a standard deviation is 1",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#scikit-learn-columntransformer",
    "href": "materials/slides/classification1.html#scikit-learn-columntransformer",
    "title": "Classification I: training & predicting",
    "section": "scikit-learn: ColumnTransformer",
    "text": "scikit-learn: ColumnTransformer\n\nscikit-learn has a preprocessing module\n\nStandardScaler(): scale our data\n\nmake_column_transformer: creates a ColumnTransformer to select columns",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#scikit-learn-select-numeric-columns",
    "href": "materials/slides/classification1.html#scikit-learn-select-numeric-columns",
    "title": "Classification I: training & predicting",
    "section": "scikit-learn: Select numeric columns",
    "text": "scikit-learn: Select numeric columns",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#scikit-learn-transform",
    "href": "materials/slides/classification1.html#scikit-learn-transform",
    "title": "Classification I: training & predicting",
    "section": "scikit-learn: transform",
    "text": "scikit-learn: transform\nScale the data\n\n\n\n\n\n\nCompare unscaled vs scaled",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#visualize-unstandarized-vs-standarized-data",
    "href": "materials/slides/classification1.html#visualize-unstandarized-vs-standarized-data",
    "title": "Classification I: training & predicting",
    "section": "Visualize unstandarized vs standarized data",
    "text": "Visualize unstandarized vs standarized data",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#why-scikit-learn-pipelines",
    "href": "materials/slides/classification1.html#why-scikit-learn-pipelines",
    "title": "Classification I: training & predicting",
    "section": "Why scikit-learn pipelines?",
    "text": "Why scikit-learn pipelines?\n\nManually standarizing is error prone\nDoes not automatically account for new data\nPrevent data leakage by processing on training data to use on test data (later)\nNeed same mean and standarization from training to use on test / new data",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#balancing-class-imbalance",
    "href": "materials/slides/classification1.html#balancing-class-imbalance",
    "title": "Classification I: training & predicting",
    "section": "Balancing + class imbalance",
    "text": "Balancing + class imbalance\nWhat if we have class imbalance? i.e., if the response variable has a big difference in frequency counts between classes?",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#visualizing-class-imbalance",
    "href": "materials/slides/classification1.html#visualizing-class-imbalance",
    "title": "Classification I: training & predicting",
    "section": "Visualizing class imbalance",
    "text": "Visualizing class imbalance",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#predicting-with-class-imbalance",
    "href": "materials/slides/classification1.html#predicting-with-class-imbalance",
    "title": "Classification I: training & predicting",
    "section": "Predicting with class imbalance",
    "text": "Predicting with class imbalance",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#upsampling",
    "href": "materials/slides/classification1.html#upsampling",
    "title": "Classification I: training & predicting",
    "section": "Upsampling",
    "text": "Upsampling\nRebalance the data by oversampling the rare class\n\nSeparate the classes out into their own data frames by filtering\nUse the .sample() method on the rare class data frame\n\nSample with replacement so the classes are the same size\n\nUse the .value_counts() method to see that our classes are now balanced",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#upsampling-code",
    "href": "materials/slides/classification1.html#upsampling-code",
    "title": "Classification I: training & predicting",
    "section": "Upsampling: code",
    "text": "Upsampling: code\nSet seed\n\n\n\n\n\n\nUpsample the rare class",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#upsampling-re-train-knn-k7",
    "href": "materials/slides/classification1.html#upsampling-re-train-knn-k7",
    "title": "Classification I: training & predicting",
    "section": "Upsampling: Re-train KNN k=7",
    "text": "Upsampling: Re-train KNN k=7",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#missing-data",
    "href": "materials/slides/classification1.html#missing-data",
    "title": "Classification I: training & predicting",
    "section": "Missing data",
    "text": "Missing data\nAssume we are only looking at ‚Äúrandomly missing‚Äù data",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#missing-data-.dropna",
    "href": "materials/slides/classification1.html#missing-data-.dropna",
    "title": "Classification I: training & predicting",
    "section": "Missing data: .dropna()",
    "text": "Missing data: .dropna()\nKNN computes distances across all the features, it needs complete observations",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#missing-data-simpleimputer",
    "href": "materials/slides/classification1.html#missing-data-simpleimputer",
    "title": "Classification I: training & predicting",
    "section": "Missing data: SimpleImputer()",
    "text": "Missing data: SimpleImputer()\nWe can impute missing data (with the mean) if there‚Äôs too many missing values",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#imputed-data",
    "href": "materials/slides/classification1.html#imputed-data",
    "title": "Classification I: training & predicting",
    "section": "Imputed data",
    "text": "Imputed data",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#put-it-all-together-preprocessor",
    "href": "materials/slides/classification1.html#put-it-all-together-preprocessor",
    "title": "Classification I: training & predicting",
    "section": "Put it all together: Preprocessor",
    "text": "Put it all together: Preprocessor",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#put-it-all-together-pipeline",
    "href": "materials/slides/classification1.html#put-it-all-together-pipeline",
    "title": "Classification I: training & predicting",
    "section": "Put it all together: Pipeline",
    "text": "Put it all together: Pipeline",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#put-it-all-together-predict",
    "href": "materials/slides/classification1.html#put-it-all-together-predict",
    "title": "Classification I: training & predicting",
    "section": "Put it all together: Predict",
    "text": "Put it all together: Predict",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#prediction-area",
    "href": "materials/slides/classification1.html#prediction-area",
    "title": "Classification I: training & predicting",
    "section": "Prediction Area",
    "text": "Prediction Area\nModel prediction area.\n\n\nPoints are on original unscaled data\nArea is using the pipeline model",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#reference-code",
    "href": "materials/slides/classification1.html#reference-code",
    "title": "Classification I: training & predicting",
    "section": "Reference Code",
    "text": "Reference Code",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#clicker-questions",
    "href": "materials/slides/classification1.html#clicker-questions",
    "title": "Classification I: training & predicting",
    "section": "Clicker questions",
    "text": "Clicker questions\n\nhttps://www.menti.com/alpi9oanfsf9\nAlternatively, go to menti.com and use code 8481 0955",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#additional-resources",
    "href": "materials/slides/classification1.html#additional-resources",
    "title": "Classification I: training & predicting",
    "section": "Additional resources",
    "text": "Additional resources\n\nThe Classification I: training & predicting chapter of Data Science: A First Introduction (Python Edition) by Tiffany Timbers, Trevor Campbell, Melissa Lee, Joel Ostblom, Lindsey Heagy contains all the content presented here with a detailed narrative.\nThe scikit-learn website is an excellent reference for more details on, and advanced usage of, the functions and packages in this lesson. Aside from that, it also offers many useful tutorials to get you started.\nAn Introduction to Statistical Learning by Gareth James Daniela Witten Trevor Hastie, and Robert Tibshirani provides a great next stop in the process of learning about classification. Chapter 4 discusses additional basic techniques for classification that we do not cover, such as logistic regression, linear discriminant analysis, and naive Bayes.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/slides/classification1.html#references",
    "href": "materials/slides/classification1.html#references",
    "title": "Classification I: training & predicting",
    "section": "References",
    "text": "References\nLars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Ga√´l Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, 108‚Äì122. 2013.\nThomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21‚Äì27, 1967.\nEvelyn Fix and Joseph Hodges. Discriminatory analysis. nonparametric discrimination: consistency properties. Technical Report, USAF School of Aviation Medicine, Randolph Field, Texas, 1951.\nWilliam Nick Street, William Wolberg, and Olvi Mangasarian. Nuclear feature extraction for breast tumor diagnosis. In International Symposium on Electronic Imaging: Science and Technology. 1993.\nStanford Health Care. What is cancer? 2021. URL: https://stanfordhealthcare.org/medical-conditions/cancer/cancer.html.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification I: training & predicting"
    ]
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html",
    "title": "Worksheet - Classification (Part II)",
    "section": "",
    "text": "After completing this workshop session, you will be able to:\n\nDescribe what a test data set is and how it is used in classification.\nUnderstand several ways of representing classifier performance: accuracy, precision, and recall, and the confusion matrix.\nUsing Python, evaluate classifier performance using a test data set and appropriate metrics.\nUsing Python, execute cross-validation in Python to choose the number of neighbours.\nIdentify when it is necessary to scale variables before classification and do this using Python\nIn a dataset with &gt; 2 attributes, perform k-nearest neighbour classification in Python using the scikit-learn package to predict the class of a test dataset.\nDescribe advantages and disadvantages of the k-nearest neighbour classification algorithm.\n\nThis worksheet covers parts of Chapter 6 of the online textbook. You can refer to and read this chapter to help you answer the worksheet. Any place you see ___, you must fill in the function, variable, or data to complete the code before running the cell.\n\n### Run this cell before continuing.\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import set_config\nfrom sklearn.compose import make_column_transformer, ColumnTransformer\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#learning-goals",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#learning-goals",
    "title": "Worksheet - Classification (Part II)",
    "section": "",
    "text": "After completing this workshop session, you will be able to:\n\nDescribe what a test data set is and how it is used in classification.\nUnderstand several ways of representing classifier performance: accuracy, precision, and recall, and the confusion matrix.\nUsing Python, evaluate classifier performance using a test data set and appropriate metrics.\nUsing Python, execute cross-validation in Python to choose the number of neighbours.\nIdentify when it is necessary to scale variables before classification and do this using Python\nIn a dataset with &gt; 2 attributes, perform k-nearest neighbour classification in Python using the scikit-learn package to predict the class of a test dataset.\nDescribe advantages and disadvantages of the k-nearest neighbour classification algorithm.\n\nThis worksheet covers parts of Chapter 6 of the online textbook. You can refer to and read this chapter to help you answer the worksheet. Any place you see ___, you must fill in the function, variable, or data to complete the code before running the cell.\n\n### Run this cell before continuing.\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import set_config\nfrom sklearn.compose import make_column_transformer, ColumnTransformer\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#fruit-data-example",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#fruit-data-example",
    "title": "Worksheet - Classification (Part II)",
    "section": "1. Fruit Data Example",
    "text": "1. Fruit Data Example\nIn the agricultural industry, cleaning, sorting, grading, and packaging food products are all necessary tasks in the post-harvest process. Products are classified based on appearance, size and shape, attributes which helps determine the quality of the food. Sorting can be done by humans, but it is tedious and time consuming. Automatic sorting could help save time and money. Images of the food products are captured and analysed to determine visual characteristics.\nThe dataset contains observations of fruit described with four features: (1) mass (in g), (2) width (in cm), (3) height (in cm), and (4) color score (on a scale from 0 - 1).\nTo get started building a classifier that can classfiy a fruit based on its appearance, use pd.read_csv to load the file fruit_data.csv (found in the data folder) from the previous tutorial into your notebook.\nAssign your data to an object called fruit_data.\n\n# Run this cell to read the data\nfruit_data = pd.read_csv(\"data/fruit_data.csv\")\n\nLet‚Äôs take a look at the first few observations in the fruit dataset. Run the cell below.\n\n# Run this cell to preview the data\nfruit_data.head()\n\nNow let‚Äôs investigate the class counts for each kind of fruit:\n\nfruit_data['fruit_name'].value_counts(normalize=False)\n\nWe can change to using normalize=True to get the class proportions:\n\nfruit_data['fruit_name'].value_counts(normalize=True)"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#randomness-and-setting-seeds",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#randomness-and-setting-seeds",
    "title": "Worksheet - Classification (Part II)",
    "section": "Randomness and Setting Seeds",
    "text": "Randomness and Setting Seeds\nThis worksheet uses functions from the scikit-learn library, which not only allows us to perform K-nearest neighbour classification, but also allows us to evaluate how well our classification worked. In order to ensure that the steps in the worksheet are reproducible, we need to set a random_state or random seed, i.e., a numerical ‚Äústarting value,‚Äù which determines the sequence of random numbers Python will generate.\nBelow in many cells we have included an argument to set the random_state or np.random.seed. They are necessary to make sure the autotesting code functions properly. In your own analysis however, it is a better practice to set the random_state or random seed just once at the beginning of your script."
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#splitting-the-data-into-a-training-and-test-set",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#splitting-the-data-into-a-training-and-test-set",
    "title": "Worksheet - Classification (Part II)",
    "section": "2. Splitting the data into a training and test set",
    "text": "2. Splitting the data into a training and test set\nIn this exercise, we will be partitioning fruit_data into a training (75%) and testing (25%) set using the scikit-learn package. After creating the test set, we will put the test set away in a lock box and not touch it again until we have found the best k-nn classifier we can make using the training set. We will use the variable fruit_name as our class label.\n\nQuestion 1\nTo create the training and test set, we would use the train_test_split function from scikit-learn package. Save the trained dataset and test dataset as fruit_train and fruit_test, respectively. To help you out, we have put a scaffold of the code in the cell below. Your job is to fill in the blanks with the correct values.\n\nNote: by default scikit-learn will not stratify the split by the label, so if we forget to set stratify to fruit_data['fruit_name'] it could be possible that we may not end up with an observation in the test set with the label ‚Äúmandarin‚Äù (because there are only 5 mandarin observations in the entire data set), and as a consequence, we would not be able to evaluate predictions on it‚Ä¶\n\n\n# Split data into a training and test set\n\n# fruit_train, fruit_test = train_test_split(____, \n#                                            test_size=____,\n#                                            stratify=____,\n#                                            random_state=____) # set the random state to be 2020\n\nfruit_train.head()\n\n\nfruit_test.head()\n\n\n# Run these tests to check your answer\n\n# Check training data \nassert isinstance(fruit_train, pd.DataFrame), \"`fruit_train` is not a pandas DataFrame\"\nassert fruit_train.shape == (44, 7), f\"Expected shape (44, 7), but got {fruit_train.shape}\"\nassert (fruit_train['fruit_name'] == 'mandarin').any(), \"`mandarin` not found in `fruit_name`\"\nnp.testing.assert_array_equal(np.array([14, 14, 12,  4]), fruit_train['fruit_name'].value_counts().values,\n                             err_msg='Some unexpected observations are in your training set.')\nassert fruit_train['fruit_name'].value_counts().index.tolist() == ['orange', 'apple', 'lemon', 'mandarin'], 'Some unexpected observations are in your training set.'\n\n# Check test data\nassert isinstance(fruit_test, pd.DataFrame), \"`fruit_test` is not a pandas DataFrame\"\nassert fruit_test.shape == (15, 7), f\"Expected shape (15, 7), but got {fruit_train.shape}\"\nassert (fruit_test['fruit_name'] == 'mandarin').any(), \"`mandarin` not found in `fruit_name`\"\nnp.testing.assert_array_equal(np.array([5, 5, 4, 1]), fruit_test['fruit_name'].value_counts().values,\n                             err_msg='Some unexpected observations are in your test set.')\nassert fruit_test['fruit_name'].value_counts().index.tolist() == ['apple', 'orange', 'lemon', 'mandarin'], 'Some unexpected observations are in your test set.'\n\nprint('Success!')\n\n\n\nQuestion 2\nK-nearest neighbors is sensitive to the scale of the predictors so we should do some preprocessing to standardize them. Remember that standardization is part of your training procedure, so you can‚Äôt use your test data to compute the centered / scaled values for each variable. Once we have created the standardization preprocessor, we can then later on apply it separately to both the training and test data sets.\nFor this worksheet, let‚Äôs see if mass, width, height and color_score can predict fruit_name.\nTo scale and center the data, first, pass the predictors to the make_column_transformer function to make the preprocessor. To help you out, we have put a scaffold of the code in the cell below. Your job is to fill in the blanks with the correct values.\nAssign your answer to an object called fruit_preprocessor.\n\n# Create a preprocessor using the training data\n\n# fruit_preprocessor = make_column_transformer(\n#     (____, ____,\n#     verbose_feature_names_out=____\n# )\n\nfruit_preprocessor\n\n\n# Run these tests to check your answer\n\nassert isinstance(fruit_preprocessor, ColumnTransformer), \"fruit_preprocessor is not a ColumnTransformer\"\nexpected_columns = {'mass', 'width', 'height', 'color_score'}\nactual_columns = set(fruit_preprocessor.transformers[0][2])\nassert actual_columns == expected_columns, f\"Expected columns {expected_columns}, but got {actual_columns}\"\nassert isinstance(fruit_preprocessor.transformers[0][1], StandardScaler), \"Transformer is not an instance of StandardScaler\"\n\nprint('Success!')\n\nNow that we have split the data, we can do things like exploratory data analysis and model fitting!!!\nA common way to explore the data when peforming classification is to create pairwise plots of the predictor variables, and color the points by the class label. Remember, that our model is sensitive to scale, and so we should also scale the data when visualizing it for modeling insights.\n\n# Run this cell to scale the data and create a pairwise scatter plot\n\n# Create a scaled version of the data\nfruit_preprocessor.fit(fruit_train)\nscaled_fruit_train = fruit_preprocessor.transform(fruit_train)\n\n# Create the scatterplot\nfruit_pairwise = alt.Chart(fruit_train).mark_point(filled=True, stroke='black', strokeWidth=0.5, size=75, opacity=0.9).encode(\n    x=alt.X(alt.repeat(\"column\"), type='quantitative'),\n    y=alt.Y(alt.repeat(\"row\"), type='quantitative'),\n    color=alt.Color(\"fruit_name:N\", scale=alt.Scale(range=['green', 'yellow', '#f26a02', '#ffd1a6'])),\n    shape=alt.Shape(\"fruit_name:N\", scale=alt.Scale(range=['circle', 'diamond', 'circle', 'square'])),\n).properties(\n    width=175,\n    height=175\n).repeat(\n    row=['mass', 'width', 'height', 'color_score'],\n    column=['color_score', 'height','width', 'mass']\n).interactive()\n\nfruit_pairwise"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#cross-validation-and-parameter-value-selection",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#cross-validation-and-parameter-value-selection",
    "title": "Worksheet - Classification (Part II)",
    "section": "3. Cross-validation and parameter value selection",
    "text": "3. Cross-validation and parameter value selection\n\nQuestion 3\nWe want to pick the number of neighbours to maximize the performance of our classifier on data it hasn‚Äôt seen yet. One way to do this is to cross-validation. To do this in Python we should do the following:\n\nSetup a pipeline object that specifies the data and preprocessor, as well as the type of model we want to use,\nCreate a dictionary with the range of values we want to search through for the parameter(s)\nPass the pipline object, and parameter grid dictionary object to a GridSearchCV.\nRun fit on the GridSearchCV object to carryout the tuning process.\nUse pd.DataFrame to convert the output to a dataframe for convenience.\n\nPerform the above using to run 5-fold cross validation to choose \\(K\\) for K-nearest neighbors classification. Use the fruit_preprocessor object you created in question 2, and search across the values from 1 to 15 (incrementing by 1).\nAssign your final data frame to an object called fruit_vfold_score so you can check your work.\n\nnp.random.seed(2020)  # DO NOT REMOVE\n\n# # 1. Setup pipeline object\n# fruit_tune_pipeline = make_pipeline(____, \n#                                     ____)\n# \n# # 2. Define parameter grid\n# parameter_grid = {\n#     ____: range(____, ____, ____),\n# }\n# \n# # 3. Create `GridSearchCV` object\n# fruit_tune_grid = GridSearchCV(\n#     estimator=____,\n#     param_grid=____,\n#     cv=____\n# )\n# \n# # 4. Fit `GridSearchCV` object\n# fruit_tune_grid.fit(\n#     ____,\n#     ____\n# )\n# \n# # 5. Convert results to data frame\n# fruit_vfold_score = pd.DataFrame(____)\n# \nfruit_vfold_score.head()\n\n\nfruit_vfold_score['rank_test_score'].value_counts()\n\n\n# Run these tests to check your answer\n\nassert isinstance(fruit_vfold_score, pd.DataFrame), \"`fruit_test` is not a pandas DataFrame\"\nassert fruit_vfold_score.shape == (14, 13), f\"Expected shape (14, 13), but got {fruit_vfold_score.shape}\"\nexpected_columns = {'mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n       'param_kneighborsclassifier__n_neighbors', 'params',\n       'split0_test_score', 'split1_test_score', 'split2_test_score',\n       'split3_test_score', 'mean_test_score', 'std_test_score',\n       'rank_test_score'}\nactual_columns = set(fruit_vfold_score.columns)\nassert actual_columns == expected_columns, f\"Expected columns {expected_columns}, but got {actual_columns}\"\n\nnp.testing.assert_array_equal(np.array([3, 2, 2, 1, 1, 1, 1, 1, 1, 1]), fruit_vfold_score['rank_test_score'].value_counts().values,\n                             err_msg='Something unexpected happened with the tuning of your classifier. Check your model and cross-validation specs.')\nassert fruit_vfold_score['rank_test_score'].value_counts().index.tolist() == [4, 1, 9, 3, 7, 8, 12, 11, 14, 13], 'Something unexpected happened with the tuning of your classifier. Check your model and cross-validation specs.'\n\nprint('Success!')\n\nTo make the results a bit easier to interpret, let‚Äôs select just the columns we are interested in:\n\nparam_kneighborsclassifier__n_neighbors\nmean_test_score\n\n\n# Run this cell to see a table of just the parameter values and\n# the mean cross-validation accuracy\n\nfruit_vfold_score[['param_kneighborsclassifier__n_neighbors',\n                  'mean_test_score']]\n\n\n\nVisualizing a parameter‚Äôs effect on model performance\nVisually inspecting the grid search results can help us find, and choose the best value for the number of neighbors parameter.\nBelow, we use altair (a Python visualization library) to create a line plot using the accuracies_grid dataframe with param_kneighborsclassifier__n_neighbors on the x-axis and the mean_test_score on the y-axis. Use point=True to include a point for each value of \\(K\\).\n\n# Run this cell to visualize the parameters vs mean cross-validation accuracy \n# as a line chart\n\naccuracy_versus_k_grid = alt.Chart(fruit_vfold_score).mark_line(point=True).encode(\n    x=alt.X(\"param_kneighborsclassifier__n_neighbors\")\n        .title(\"Neighbors\")\n        .scale(zero=False),\n    y=alt.Y(\"mean_test_score\")\n        .title(\"Mean Test Score\")\n        .scale(zero=False)\n)\n\naccuracy_versus_k_grid"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#understanding-prediction-errors",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#understanding-prediction-errors",
    "title": "Worksheet - Classification (Part II)",
    "section": "6. Understanding prediction errors",
    "text": "6. Understanding prediction errors\nNow, let‚Äôs look at the confusion matrix for the classifier. This will show us a table comparing the predicted labels with the true labels.\nAlthough we can create a confusion matrix by using the crosstab function from pandas, with many observations, it can be difficult to interpret the confusion matrix when it is presented as a table like above. In these cases, we could instead use the ConfusionMatrixDisplay function of the scikit-learn package to visualize the confusion matrix as a heatmap. Please run the cell below to see the fruit confusion matrix as a heatmap.\n\nConfusionMatrixDisplay.from_estimator(\n    estimator=fruit_tune_grid,  # We are directly passing the pipeline and let sklearn do the predictions for us\n    X=fruit_train.drop(columns=['fruit_name']),\n    y=fruit_train['fruit_name']\n)\n\nWhat mistakes is it making? Revisit the pairwise scatter plot you created after question 2, can you guess why its making these mistakes? Do you have any ideas of what you could do to improve the model (hint, think about the data)?"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#model-generalizability",
    "href": "materials/worksheets/py_worksheet_classification2/py_worksheet_classification2.html#model-generalizability",
    "title": "Worksheet - Classification (Part II)",
    "section": "Model generalizability",
    "text": "Model generalizability\nOnce you have finished selecting your single, final model (and only then!) you may wish to estimate how well your model will generalize to unseen data. You have kept your test set for just that purpose, and that purpose alone.\n\nQuestion 4\nThe first step in assessing model generalizability is to use your model to predict labels for the test set observations. To do this with Python, use predict on the fruit_tune_grid object (that contains your trained model) to add a column named predicted to the fruit_test data frame. By default, scikit-learn will use the ‚Äúbest estimator‚Äù, here \\(K\\) with the highest accuracy, to do this. To help you out, we have put a scaffold of the code in the cell below. Your job is to fill in the blanks with the correct values.\nAssign your answer to an object called fruit_test_w_predictions.\n\n# Make a deep copy of `fruit_test` and name it `fruit_test_w_predictions`\nfruit_test_w_predictions = fruit_test.copy()\n\n# Use trained classifier to predict labels for test data\n\n# fruit_test_w_predictions['predicted'] = fruit_tune_grid.predict(\n#     ____)\n# )\n\nfruit_test_w_predictions.head()\n\n\n# Run these tests to check your answer\n\nassert isinstance(fruit_test_w_predictions, pd.DataFrame), '`fruit_test_w_predictions` is not a pandas DataFrame'\nassert fruit_test_w_predictions.shape == (15, 8), f\"Expected shape (15, 8), but got {fruit_test_w_predictions.shape}\"\nexpected_columns = {'fruit_label', 'fruit_name', 'fruit_subtype', 'mass', 'width', 'height', 'color_score', 'predicted'}\nactual_columns = set(fruit_test_w_predictions.columns)\nassert actual_columns == expected_columns, f\"Expected columns {expected_columns}, but got {actual_columns}\"\nnp.testing.assert_array_equal(np.array([5, 5, 4, 1]), fruit_test_w_predictions['predicted'].value_counts().values,\n                             err_msg='Some unexpected predictions were made from your classifier.')\nassert fruit_test_w_predictions['predicted'].value_counts().index.tolist() == ['apple', 'orange', 'lemon', 'mandarin'], 'Some unexpected predictions were made from your classifier.'\n\nprint('Success!')\n\n\n\nQuestion 5\nNext, we can calculate our prediction metrics on the test set, as well a compute and visualize a confusion matrix. We typically expect some drop in performance from what we observed in our training set (because we used that set to train the model), but ideally the model performs similarly.\nUse Python to calculate accuracy, weighted precision and weighted recall for the test set. To help you out, we have put a scaffold of the code in the cell below. Your job is to fill in the blanks with the correct values. Name your objects for accuracy, weighted precision and weighted recall accuracy, weighted_precision and weighted_recall, respectively.\n\nNote: given we have many categories, we want to know the weighted average precision and recall instead of calculating it for just one class.\n\n\n# Calculate accuracy for the test set\n\n# accuracy = fruit_tune_grid.score(\n#     X=____,\n#     y=____\n# )\n\naccuracy\ntype(accuracy)\n\n\n# Calculate weighted precision for the test set\n\n# weighted_precision = precision_score(\n#     y_true=____,\n#     y_pred=____,\n#     average='weighted'\n# )\n\nweighted_precision\n\n\n# Calculate weighted recall for the test set\n\n# weighted_recall = recall_score(\n#     y_true=____,\n#     y_pred=____,\n#     average='weighted'\n# )\n\nweighted_recall\n\n\n# Run these tests to check your answer\n\n# Check accuracy\nassert type(accuracy) == np.float64, '`accuracy` should be a single value, of type `numpy.float64`.'\nassert accuracy == 1, '`accuracy` value is incorrect'\n\n# Check weighted precision\nassert type(weighted_precision) == np.float64, '`weighted_precision` should be a single value, of type `numpy.float64`.'\nassert weighted_precision == 1, '`weighted_precision` value is incorrect'\n\n# Check weighted recall\nassert type(weighted_recall) == np.float64, '`weighted_recall` should be a single value, of type `numpy.float64`.'\nassert weighted_recall == 1, '`weighted_recall` value is incorrect'\n\nprint('Success!')\n\n\n# Visualize prediction performance as a confusion matrix\n\n# ConfusionMatrixDisplay.from_estimator(\n#     estimator=____\n#     X=____,\n#     y=____\n# )\n\nHmmm‚Ä¶ no mistakes on the test set‚Ä¶ Why might that be?"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html",
    "title": "Tree-based and ensemble models",
    "section": "",
    "text": "Code from the slides in an executable notebook."
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#example-the-heart-data-set",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#example-the-heart-data-set",
    "title": "Tree-based and ensemble models",
    "section": "Example: the heart data set",
    "text": "Example: the heart data set\n\nimport pandas as pd\nheart = pd.read_csv(\"data/Heart.csv\", index_col=0)\nheart.info()\n\n\nheart.head()"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#do-we-have-a-class-imbalance",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#do-we-have-a-class-imbalance",
    "title": "Tree-based and ensemble models",
    "section": "Do we have a class imbalance?",
    "text": "Do we have a class imbalance?\n\nheart['AHD'].value_counts(normalize=True)"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#data-splitting",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#data-splitting",
    "title": "Tree-based and ensemble models",
    "section": "Data splitting",
    "text": "Data splitting\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(2024)\n\nheart_train, heart_test = train_test_split(\n    heart, train_size=0.8, stratify=heart[\"AHD\"]\n)\n\nX_train = heart_train.drop(columns=['AHD'])\ny_train = heart_train['AHD']\nX_test = heart_test.drop(columns=['AHD'])\ny_test = heart_test['AHD']"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#one-hot-encoding-pre-processing",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#one-hot-encoding-pre-processing",
    "title": "Tree-based and ensemble models",
    "section": "One hot encoding & pre-processing",
    "text": "One hot encoding & pre-processing\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\n\nnumeric_feats = ['Age', 'RestBP', 'Chol', 'RestECG', 'MaxHR', 'Oldpeak','Slope', 'Ca']\npassthrough_feats = ['Sex', 'Fbs', 'ExAng']\ncategorical_feats = ['ChestPain', 'Thal']\n\nheart_preprocessor = make_column_transformer(\n    (StandardScaler(), numeric_feats), \n    (\"passthrough\", passthrough_feats),     \n    (OneHotEncoder(handle_unknown = \"ignore\"), categorical_feats),     \n)"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#fitting-a-dummy-classifier",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#fitting-a-dummy-classifier",
    "title": "Tree-based and ensemble models",
    "section": "Fitting a dummy classifier",
    "text": "Fitting a dummy classifier\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate\n\ndummy = DummyClassifier()\ndummy_pipeline = make_pipeline(heart_preprocessor, dummy)\ncv_10_dummy = pd.DataFrame(\n    cross_validate(\n        estimator=dummy_pipeline,\n        cv=10,\n        X=X_train,\n        y=y_train\n    )\n)\ncv_10_dummy_metrics = cv_10_dummy.agg([\"mean\"])\nresults = pd.DataFrame({'mean' : [cv_10_dummy_metrics.test_score.iloc[0]]},\n  index = ['Dummy classifier']\n)\nresults"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#fitting-a-decision-tree",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#fitting-a-decision-tree",
    "title": "Tree-based and ensemble models",
    "section": "Fitting a decision tree",
    "text": "Fitting a decision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(random_state=2025)\n\ndt_pipeline = make_pipeline(heart_preprocessor, decision_tree)\ncv_10_dt = pd.DataFrame(\n    cross_validate(\n        estimator=dt_pipeline,\n        cv=10,\n        X=X_train,\n        y=y_train\n    )\n)\ncv_10_dt_metrics = cv_10_dt.agg([\"mean\"])\nresults_dt = pd.DataFrame({'mean' : [cv_10_dt_metrics.test_score.iloc[0]]},\n  index = ['Decision tree']\n)\nresults = pd.concat([results, results_dt])\nresults"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#random-forest-in-scikit-learn",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#random-forest-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Random forest in scikit-learn",
    "text": "Random forest in scikit-learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(random_state=2025)\nrf_pipeline = make_pipeline(heart_preprocessor, random_forest)\ncv_10_rf = pd.DataFrame(\n    cross_validate(\n        estimator=rf_pipeline,\n        cv=10,\n        X=X_train,\n        y=y_train\n    )\n)\n\ncv_10_rf_metrics = cv_10_rf.agg([\"mean\"])\nresults_rf = pd.DataFrame({'mean' : [cv_10_rf_metrics.test_score.iloc[0]]},\n  index = ['Random forest']\n)\nresults = pd.concat([results, results_rf])\nresults"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#tuning-random-forest-in-scikit-learn",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#tuning-random-forest-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Tuning random forest in scikit-learn",
    "text": "Tuning random forest in scikit-learn\n\nfrom sklearn.model_selection import GridSearchCV\n\nrf_param_grid = {'randomforestclassifier__n_estimators': [200],\n              'randomforestclassifier__max_depth': [1, 3, 5, 7, 9],\n              'randomforestclassifier__max_features': [1, 2, 3, 4, 5, 6, 7]}\n\nrf_tune_grid = GridSearchCV(\n    estimator=rf_pipeline,\n    param_grid=rf_param_grid,\n    cv=10,\n    n_jobs=-1 # tells computer to use all available CPUs\n)\nrf_tune_grid.fit(\n    X_train,\n    y_train\n)\n\ncv_10_rf_tuned_metrics = pd.DataFrame(rf_tune_grid.cv_results_)\nresults_rf_tuned = pd.DataFrame({'mean' : rf_tune_grid.best_score_},\n  index = ['Random forest tuned']\n)\nresults = pd.concat([results, results_rf_tuned])\n\n\nresults"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#tuning-boosted-classifiers-with-scikit-learn",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#tuning-boosted-classifiers-with-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Tuning Boosted Classifiers with scikit-learn",
    "text": "Tuning Boosted Classifiers with scikit-learn\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\ngradient_boosted_classifier = HistGradientBoostingClassifier(random_state=2025)\ngb_pipeline = make_pipeline(heart_preprocessor, gradient_boosted_classifier)\ngb_param_grid = {'histgradientboostingclassifier__max_iter': [200],\n              'histgradientboostingclassifier__max_depth': [1, 3, 5, 7, 9],\n              'histgradientboostingclassifier__learning_rate': [0.001, 0.005, 0.01]}\ngb_tune_grid = GridSearchCV(\n    estimator=gb_pipeline,\n    param_grid=gb_param_grid,\n    cv=10,\n    n_jobs=-1 # tells computer to use all available CPUs\n)\ngb_tune_grid.fit(\n    X_train,\n    y_train\n)\n\ncv_10_gb_tuned_metrics = pd.DataFrame(gb_tune_grid.cv_results_)\nresults_gb_tuned = pd.DataFrame({'mean' : gb_tune_grid.best_score_},\n  index = ['Gradient boosted classifier tuned']\n)\nresults = pd.concat([results, results_gb_tuned])\n\n\nresults"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#precision-and-recall-on-the-tuned-random-forest-model",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#precision-and-recall-on-the-tuned-random-forest-model",
    "title": "Tree-based and ensemble models",
    "section": "Precision and recall on the tuned random forest model",
    "text": "Precision and recall on the tuned random forest model\n\nfrom sklearn.metrics import make_scorer, precision_score, recall_score\n\nscoring = {\n    'accuracy': 'accuracy',\n    'precision': make_scorer(precision_score, pos_label='Yes'),\n    'recall': make_scorer(recall_score, pos_label='Yes')\n}\n\nrf_tune_grid = GridSearchCV(\n    estimator=rf_pipeline,\n    param_grid=rf_param_grid,\n    cv=10,\n    n_jobs=-1,\n    scoring=scoring,\n    refit='accuracy'\n)\n\nrf_tune_grid.fit(X_train, y_train)\n\n\ncv_results = pd.DataFrame(rf_tune_grid.cv_results_)\n\nmean_precision = cv_results['mean_test_precision'].iloc[rf_tune_grid.best_index_]\nmean_recall = cv_results['mean_test_recall'].iloc[rf_tune_grid.best_index_]\n\nresults_rf_tuned = pd.DataFrame({\n    'mean': [rf_tune_grid.best_score_, mean_precision, mean_recall]},\n                                index=['accuracy', 'precision', 'recall'])\n\nresults_rf_tuned"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#feature-importances-in-scikit-learn",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#feature-importances-in-scikit-learn",
    "title": "Tree-based and ensemble models",
    "section": "Feature importances in scikit-learn",
    "text": "Feature importances in scikit-learn\n\n# Access the best pipeline\nbest_pipeline = rf_tune_grid.best_estimator_\n\n# Extract the trained RandomForestClassifier from the pipeline\nbest_rf = best_pipeline.named_steps['randomforestclassifier']\n\n# Extract feature names after preprocessing\n# Get the names of features from each transformer in the pipeline\nnumeric_features = numeric_feats\ncategorical_feature_names = best_pipeline.named_steps['columntransformer'].transformers_[2][1].get_feature_names_out(categorical_feats)\npassthrough_features = passthrough_feats\n\n# Combine all feature names into a single list\nfeature_names = np.concatenate([numeric_features, passthrough_features, categorical_feature_names])\n\n# Calculate feature importances\nfeature_importances = best_rf.feature_importances_\n\n# Create a DataFrame to display feature importances\nimportances_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importances\n})\n\n# Sort by importance (descending order)\nimportances_df = importances_df.sort_values(by='Importance', ascending=False)"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#visualizing-the-results",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#visualizing-the-results",
    "title": "Tree-based and ensemble models",
    "section": "Visualizing the results",
    "text": "Visualizing the results\n\nimport altair as alt\n\nbar_chart = alt.Chart(importances_df).mark_bar().encode(\n    x=alt.X('Importance:Q', title='Feature Importance'),\n    y=alt.Y('Feature:N', sort='-x', title='Feature'),\n    tooltip=['Feature', 'Importance']\n).properties(\n    title='Feature Importances from Random Forest Model',\n    width=600,\n    height=400\n)\nbar_chart"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#evaluating-on-the-test-set",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#evaluating-on-the-test-set",
    "title": "Tree-based and ensemble models",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\n\nX_test = heart_test.drop(columns=['AHD'])\ny_test = heart_test['AHD']\n\nheart_test[\"predicted\"] = rf_tune_grid.predict(\n    X_test\n)\n\nAccuracy\n\nrf_tune_grid.score(\n    X_test,\n    y_test\n)\n\nPrecision\n\nprecision_score(\n    y_true=heart_test[\"AHD\"],\n    y_pred=heart_test[\"predicted\"],\n    pos_label='Yes'\n)\n\nRecall\n\nrecall_score(\n    y_true=heart_test[\"AHD\"],\n    y_pred=heart_test[\"predicted\"],\n    pos_label='Yes'\n)\n\nConfusion matrix\n\nconf_matrix = pd.crosstab(\n    heart_test[\"AHD\"],\n    heart_test[\"predicted\"]\n)\nprint(conf_matrix)"
  },
  {
    "objectID": "materials/worksheets/py_ensembles/ensembles-code.html#referencesgareth-james-daniela-witten-trevor-hastie-robert-tibshirani-and-jonathan-taylor.-an-introduction-to-statistical-learning-with-applications-in-python.-springer-1st-edition-2023.-url-httpswww.statlearning.com.kolhatkar-v.-and-ostblom-j.-2024.-ubc-dsci-573-feature-and-model-selection-course-notes.-url-httpsubc-mds.github.iodsci_573_feat-model-selectpedregosa-f.-et-al.-2011.-scikit-learn-machine-learning-in-python.-journal-of-machine-learning-research-12oct-pp.28252830.",
    "href": "materials/worksheets/py_ensembles/ensembles-code.html#referencesgareth-james-daniela-witten-trevor-hastie-robert-tibshirani-and-jonathan-taylor.-an-introduction-to-statistical-learning-with-applications-in-python.-springer-1st-edition-2023.-url-httpswww.statlearning.com.kolhatkar-v.-and-ostblom-j.-2024.-ubc-dsci-573-feature-and-model-selection-course-notes.-url-httpsubc-mds.github.iodsci_573_feat-model-selectpedregosa-f.-et-al.-2011.-scikit-learn-machine-learning-in-python.-journal-of-machine-learning-research-12oct-pp.28252830.",
    "title": "Tree-based and ensemble models",
    "section": "ReferencesGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani and Jonathan Taylor. An Introduction to Statistical Learning with Applications in Python. Springer, 1st edition, 2023. URL: https://www.statlearning.com/.Kolhatkar, V., and Ostblom, J. (2024). UBC DSCI 573: Feature and Model Selection course notes. URL: https://ubc-mds.github.io/DSCI_573_feat-model-selectPedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825‚Äì2830.",
    "text": "ReferencesGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani and Jonathan Taylor. An Introduction to Statistical Learning with Applications in Python. Springer, 1st edition, 2023. URL: https://www.statlearning.com/.Kolhatkar, V., and Ostblom, J. (2024). UBC DSCI 573: Feature and Model Selection course notes. URL: https://ubc-mds.github.io/DSCI_573_feat-model-selectPedregosa, F. et al., 2011. Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), pp.2825‚Äì2830."
  },
  {
    "objectID": "materials/worksheets/py_worksheet_regression/py_worksheet_regression.html",
    "href": "materials/worksheets/py_worksheet_regression/py_worksheet_regression.html",
    "title": "Worksheet - Linear Regression",
    "section": "",
    "text": "Recognize situations where a simple regression analysis would be appropriate for making predictions.\nPerform ordinary least squares regression in Python using scikit-learn to predict the values for a test dataset.\nUse Python to fit simple and multivariable linear regression models on training data.\nEvaluate the linear regression model on test data.\n\nThis worksheet covers parts of Chapter 8 of the online textbook. You should read this chapter to gain a better understanding of this assignment. Any place you see ___, you must fill in the function, variable, or data to complete the code. Substitute the raise NotImplementedError with your completed code and answers then proceed to run the cell.\n\n### Run this cell before continuing.\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import set_config\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_regression/py_worksheet_regression.html#learning-objectives",
    "href": "materials/worksheets/py_worksheet_regression/py_worksheet_regression.html#learning-objectives",
    "title": "Worksheet - Linear Regression",
    "section": "",
    "text": "Recognize situations where a simple regression analysis would be appropriate for making predictions.\nPerform ordinary least squares regression in Python using scikit-learn to predict the values for a test dataset.\nUse Python to fit simple and multivariable linear regression models on training data.\nEvaluate the linear regression model on test data.\n\nThis worksheet covers parts of Chapter 8 of the online textbook. You should read this chapter to gain a better understanding of this assignment. Any place you see ___, you must fill in the function, variable, or data to complete the code. Substitute the raise NotImplementedError with your completed code and answers then proceed to run the cell.\n\n### Run this cell before continuing.\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import set_config\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_regression/py_worksheet_regression.html#marathon-training-with-linear-regression",
    "href": "materials/worksheets/py_worksheet_regression/py_worksheet_regression.html#marathon-training-with-linear-regression",
    "title": "Worksheet - Linear Regression",
    "section": "Marathon Training with Linear Regression!",
    "text": "Marathon Training with Linear Regression!\n\nSource: https://media.giphy.com/media/BDagLpxFIm3SM/giphy.gif\nQuestion: what features predict whether athletes will perform better than others? Specifically, we are interested in marathon runners, and looking at how the maximum distance ran per week during training predicts the time it takes a runner to end the race?\nThis time around, however, we will analyze the data using simple linear regression rather than \\(k\\)-nn regression. In the end, we will compare our results to what we found with \\(k\\)-nn regression.\nQuestion 1.0  {points: 1}\nLoad the marathon data from the data/ folder and assign it to an object called marathon.\n\n# your code here\nraise NotImplementedError\nmarathon\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon is None)).encode(\"utf-8\")+b\"739e2\").hexdigest() == \"6e92f9a4e4c3ee38226eb47ccf55024c5f32aa31\", \"type of marathon is None is not bool. marathon is None should be a bool\"\nassert sha1(str(marathon is None).encode(\"utf-8\")+b\"739e2\").hexdigest() == \"59296bffcfb7bcd365f6c79ebfd8e5a40df62b2f\", \"boolean value of marathon is None is not correct\"\n\nassert sha1(str(type(marathon)).encode(\"utf-8\")+b\"739e3\").hexdigest() == \"bff0380c6a360860e46c818fdc334b2bcfeb5adb\", \"type of type(marathon) is not correct\"\n\nassert sha1(str(type(marathon.shape)).encode(\"utf-8\")+b\"739e4\").hexdigest() == \"1eada99c236028fcff78fd6303354632c2cb5e62\", \"type of marathon.shape is not tuple. marathon.shape should be a tuple\"\nassert sha1(str(len(marathon.shape)).encode(\"utf-8\")+b\"739e4\").hexdigest() == \"2e34911a50836d3cf47d13ee702a774b05b66acf\", \"length of marathon.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon.shape))).encode(\"utf-8\")+b\"739e4\").hexdigest() == \"07d7c57045b521db23e59bcd29a30b64f750e408\", \"values of marathon.shape are not correct\"\nassert sha1(str(marathon.shape).encode(\"utf-8\")+b\"739e4\").hexdigest() == \"c1b0dafb0e4d543a159d3cef1509b02336267aef\", \"order of elements of marathon.shape is not correct\"\n\nassert sha1(str(type(\"time_hrs\" in marathon.columns)).encode(\"utf-8\")+b\"739e5\").hexdigest() == \"b804c528bed5bf422960fd5c803d8d8689b2cdd6\", \"type of \\\"time_hrs\\\" in marathon.columns is not bool. \\\"time_hrs\\\" in marathon.columns should be a bool\"\nassert sha1(str(\"time_hrs\" in marathon.columns).encode(\"utf-8\")+b\"739e5\").hexdigest() == \"65ee06ccfc8ecf0d0497cf6b3322c74797040486\", \"boolean value of \\\"time_hrs\\\" in marathon.columns is not correct\"\n\nassert sha1(str(type(\"max\" in marathon.columns)).encode(\"utf-8\")+b\"739e6\").hexdigest() == \"73f7cbaa58ac0ae4fcf13fdb39b6a596d0a9e637\", \"type of \\\"max\\\" in marathon.columns is not bool. \\\"max\\\" in marathon.columns should be a bool\"\nassert sha1(str(\"max\" in marathon.columns).encode(\"utf-8\")+b\"739e6\").hexdigest() == \"11b4588727f7e1cd86d42c8e61e257ddf72cb9cf\", \"boolean value of \\\"max\\\" in marathon.columns is not correct\"\n\nassert sha1(str(type(round(sum(marathon['max']), 0))).encode(\"utf-8\")+b\"739e7\").hexdigest() == \"cb0dcf331ef1975fca37d4d6dc1e453b08bf2a33\", \"type of round(sum(marathon['max']), 0) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(round(sum(marathon['max']), 0), 2)).encode(\"utf-8\")+b\"739e7\").hexdigest() == \"0c743d369241ee5ae9d99436f698cf6105d89029\", \"value of round(sum(marathon['max']), 0) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(round(sum(marathon['time_hrs']), 0))).encode(\"utf-8\")+b\"739e8\").hexdigest() == \"bc74cad374cdd1f333d8f0dbdd3e8f40872b3827\", \"type of round(sum(marathon['time_hrs']), 0) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(round(sum(marathon['time_hrs']), 0), 2)).encode(\"utf-8\")+b\"739e8\").hexdigest() == \"014c3d62ed4ff997c449ec3c8c8a4ee9a2353073\", \"value of round(sum(marathon['time_hrs']), 0) is not correct (rounded to 2 decimal places)\"\n\nprint('Success!')\n\nQuestion 1.1  {points: 1}\nSimilar to what we have been doing, we will first split the dataset into the training and testing datasets, using 75% of the original data as the training data. Remember, we will be putting the test dataset away in a ‚Äòlock box‚Äô that we will comeback to later after we choose our final model. Assign your training dataset to an object named marathon_training and your testing dataset to an object named marathon_testing.\nNext, set the time_hrs as the target (y) and max as the feature (X). Store the features as X_train and X_test and targets as y_train and y_test respectively for the marathon_training and marathon_testing.\nAssign the objects to marathon_training, marathon_testing, X_train, y_train, X_test and y_test respectively.\n\n# ___, ___ = train_test_split(\n#     ___,\n#     test_size=___,\n#     random_state=2000,  # Do not change the random_state\n# )\n\n# X_train = ___[___]  # A single column data frame\n# y_train = ___[___]  # A series\n\n# X_test = ___[___]  # A single column data frame\n# y_test = ___[___]  # A series\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_training is None)).encode(\"utf-8\")+b\"4f6ad\").hexdigest() == \"f674e38d6fdcb566897037d6bd60a47404feeb21\", \"type of marathon_training is None is not bool. marathon_training is None should be a bool\"\nassert sha1(str(marathon_training is None).encode(\"utf-8\")+b\"4f6ad\").hexdigest() == \"6fa797429392a5d615e12d87d6c270c224b62e87\", \"boolean value of marathon_training is None is not correct\"\n\nassert sha1(str(type(marathon_training.shape)).encode(\"utf-8\")+b\"4f6ae\").hexdigest() == \"c65c0f879cf7d4bd9e072e641450bd00c675daa9\", \"type of marathon_training.shape is not tuple. marathon_training.shape should be a tuple\"\nassert sha1(str(len(marathon_training.shape)).encode(\"utf-8\")+b\"4f6ae\").hexdigest() == \"33b155dbfcff513325406a6efb32adad20db7a02\", \"length of marathon_training.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_training.shape))).encode(\"utf-8\")+b\"4f6ae\").hexdigest() == \"30d227d9ac1ed57ba85872a2a07b6117d273c251\", \"values of marathon_training.shape are not correct\"\nassert sha1(str(marathon_training.shape).encode(\"utf-8\")+b\"4f6ae\").hexdigest() == \"bb16852ef07255cdb14cab3907de9fdd370231a4\", \"order of elements of marathon_training.shape is not correct\"\n\nassert sha1(str(type(sum(marathon_training.age))).encode(\"utf-8\")+b\"4f6af\").hexdigest() == \"ba50b4a4b9e30582e162d71f7e94533f0cd2670e\", \"type of sum(marathon_training.age) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(marathon_training.age)).encode(\"utf-8\")+b\"4f6af\").hexdigest() == \"97e4849e109b18815b375aabbc4a675bf8b572f8\", \"value of sum(marathon_training.age) is not correct\"\n\nassert sha1(str(type(marathon_testing is None)).encode(\"utf-8\")+b\"4f6b0\").hexdigest() == \"874282999c909b82564d7ce122d4cc20808ae07d\", \"type of marathon_testing is None is not bool. marathon_testing is None should be a bool\"\nassert sha1(str(marathon_testing is None).encode(\"utf-8\")+b\"4f6b0\").hexdigest() == \"ca991e955d46513ca9dc43cb7faad1510405e6e2\", \"boolean value of marathon_testing is None is not correct\"\n\nassert sha1(str(type(marathon_testing.shape)).encode(\"utf-8\")+b\"4f6b1\").hexdigest() == \"3827f444c705a95782a04332415cd5d97f324216\", \"type of marathon_testing.shape is not tuple. marathon_testing.shape should be a tuple\"\nassert sha1(str(len(marathon_testing.shape)).encode(\"utf-8\")+b\"4f6b1\").hexdigest() == \"906078a9f2abf4de01fcf0daf6461d5788643f77\", \"length of marathon_testing.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_testing.shape))).encode(\"utf-8\")+b\"4f6b1\").hexdigest() == \"2cc1bdf059f9d6117e78f7d6f6e5821ec9fc1bdb\", \"values of marathon_testing.shape are not correct\"\nassert sha1(str(marathon_testing.shape).encode(\"utf-8\")+b\"4f6b1\").hexdigest() == \"5a1c8ea5fc9a728dee1c0b720c690e0a130064e2\", \"order of elements of marathon_testing.shape is not correct\"\n\nassert sha1(str(type(sum(marathon_testing.age))).encode(\"utf-8\")+b\"4f6b2\").hexdigest() == \"e7a8984d43f4d32c814e54d4358442ad0a96003c\", \"type of sum(marathon_testing.age) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(sum(marathon_testing.age)).encode(\"utf-8\")+b\"4f6b2\").hexdigest() == \"34fa963b05468045a3dfe6fa84cff7cdcc47d488\", \"value of sum(marathon_testing.age) is not correct\"\n\nassert sha1(str(type(X_train.columns.values)).encode(\"utf-8\")+b\"4f6b3\").hexdigest() == \"58cab9eda29ff2b2d6749caed7393fbdc6843f76\", \"type of X_train.columns.values is not correct\"\nassert sha1(str(X_train.columns.values).encode(\"utf-8\")+b\"4f6b3\").hexdigest() == \"569ca1bdf2aee914b88e0cd7bf39a6fb4701d65a\", \"value of X_train.columns.values is not correct\"\n\nassert sha1(str(type(X_train.shape)).encode(\"utf-8\")+b\"4f6b4\").hexdigest() == \"c6004691c652e546af1b6b67844b4e846344c1d6\", \"type of X_train.shape is not tuple. X_train.shape should be a tuple\"\nassert sha1(str(len(X_train.shape)).encode(\"utf-8\")+b\"4f6b4\").hexdigest() == \"b5e0ccffd5b8cbe7ba6731bff5c23dab8c0f4590\", \"length of X_train.shape is not correct\"\nassert sha1(str(sorted(map(str, X_train.shape))).encode(\"utf-8\")+b\"4f6b4\").hexdigest() == \"1e3e3ec67d9ab5e7ee523bab8f560ee2549bc591\", \"values of X_train.shape are not correct\"\nassert sha1(str(X_train.shape).encode(\"utf-8\")+b\"4f6b4\").hexdigest() == \"010ca15c9c455f5f34e84a6483db2b38abbf6bc0\", \"order of elements of X_train.shape is not correct\"\n\nassert sha1(str(type(y_train.name)).encode(\"utf-8\")+b\"4f6b5\").hexdigest() == \"dffb93a65b9c15090a1dac65485392d02dd3b05e\", \"type of y_train.name is not str. y_train.name should be an str\"\nassert sha1(str(len(y_train.name)).encode(\"utf-8\")+b\"4f6b5\").hexdigest() == \"0def0bf84e8da55beab10cd1fbfd9f7fbd31a1cc\", \"length of y_train.name is not correct\"\nassert sha1(str(y_train.name.lower()).encode(\"utf-8\")+b\"4f6b5\").hexdigest() == \"9ede03ef4c98e99b336e0465df700ab86f738d01\", \"value of y_train.name is not correct\"\nassert sha1(str(y_train.name).encode(\"utf-8\")+b\"4f6b5\").hexdigest() == \"9ede03ef4c98e99b336e0465df700ab86f738d01\", \"correct string value of y_train.name but incorrect case of letters\"\n\nassert sha1(str(type(y_train.shape)).encode(\"utf-8\")+b\"4f6b6\").hexdigest() == \"14ef9f53aed59146615896c647d72fdee84a030d\", \"type of y_train.shape is not tuple. y_train.shape should be a tuple\"\nassert sha1(str(len(y_train.shape)).encode(\"utf-8\")+b\"4f6b6\").hexdigest() == \"bcfdddcab49f87ca8545d798e2d70e638926a45e\", \"length of y_train.shape is not correct\"\nassert sha1(str(sorted(map(str, y_train.shape))).encode(\"utf-8\")+b\"4f6b6\").hexdigest() == \"9149d98edddfb638a4de90d61ed99ae9884bb8a7\", \"values of y_train.shape are not correct\"\nassert sha1(str(y_train.shape).encode(\"utf-8\")+b\"4f6b6\").hexdigest() == \"b7502ed93b7fb6823b51c2982622b915a4fb1041\", \"order of elements of y_train.shape is not correct\"\n\nassert sha1(str(type(X_test.columns.values)).encode(\"utf-8\")+b\"4f6b7\").hexdigest() == \"4474a8a8bfdfc8b012d956db8931980b76744722\", \"type of X_test.columns.values is not correct\"\nassert sha1(str(X_test.columns.values).encode(\"utf-8\")+b\"4f6b7\").hexdigest() == \"b2549900463f19a702b41da70c671104c0e8b682\", \"value of X_test.columns.values is not correct\"\n\nassert sha1(str(type(X_test.shape)).encode(\"utf-8\")+b\"4f6b8\").hexdigest() == \"4c2da79bd29e66ef3fbb291d5188459fc5c4706f\", \"type of X_test.shape is not tuple. X_test.shape should be a tuple\"\nassert sha1(str(len(X_test.shape)).encode(\"utf-8\")+b\"4f6b8\").hexdigest() == \"ecd604671069f62e45124763ec88fc80d7b82498\", \"length of X_test.shape is not correct\"\nassert sha1(str(sorted(map(str, X_test.shape))).encode(\"utf-8\")+b\"4f6b8\").hexdigest() == \"3bf5695fc56458e4405a6f88a4f5506f5d901cb8\", \"values of X_test.shape are not correct\"\nassert sha1(str(X_test.shape).encode(\"utf-8\")+b\"4f6b8\").hexdigest() == \"352788e35c8d809802def81e35c175258a93bd69\", \"order of elements of X_test.shape is not correct\"\n\nassert sha1(str(type(y_test.name)).encode(\"utf-8\")+b\"4f6b9\").hexdigest() == \"4b81d94ef69cd62c1396789593bbee6e8a5fa1a5\", \"type of y_test.name is not str. y_test.name should be an str\"\nassert sha1(str(len(y_test.name)).encode(\"utf-8\")+b\"4f6b9\").hexdigest() == \"f9f25f4c81c55229317166657fe150f016b21d38\", \"length of y_test.name is not correct\"\nassert sha1(str(y_test.name.lower()).encode(\"utf-8\")+b\"4f6b9\").hexdigest() == \"b708b2f0439fe252df818bf470eee84eb19da428\", \"value of y_test.name is not correct\"\nassert sha1(str(y_test.name).encode(\"utf-8\")+b\"4f6b9\").hexdigest() == \"b708b2f0439fe252df818bf470eee84eb19da428\", \"correct string value of y_test.name but incorrect case of letters\"\n\nassert sha1(str(type(y_test.shape)).encode(\"utf-8\")+b\"4f6ba\").hexdigest() == \"f18361d9de35bec90f44b45ee43725ea9777275c\", \"type of y_test.shape is not tuple. y_test.shape should be a tuple\"\nassert sha1(str(len(y_test.shape)).encode(\"utf-8\")+b\"4f6ba\").hexdigest() == \"e2309e5b908c5857ae791835ba17a8025f1c4312\", \"length of y_test.shape is not correct\"\nassert sha1(str(sorted(map(str, y_test.shape))).encode(\"utf-8\")+b\"4f6ba\").hexdigest() == \"89c5cd5a88c20980c89973a438767b9f8a88ce3e\", \"values of y_test.shape are not correct\"\nassert sha1(str(y_test.shape).encode(\"utf-8\")+b\"4f6ba\").hexdigest() == \"78ec5b8e1118ac59272fb5d5fdde68b526f3e166\", \"order of elements of y_test.shape is not correct\"\n\nprint('Success!')\n\nQuestion 1.2  {points: 1}\nUsing only the observations in the training dataset, create a scatterplot to assess the relationship between race time (time_hrs) and maximum distance ran per week during training (max). Put time_hrs on the y-axis and max on the x-axis. Use mark_point and remember to do whatever is necessary to make this an effective visualization, including addressing overplotting in a suitable manner.\nAssign this plot to an object called marathon_scatter.\n\n# your code here\nmarathon_scatter = alt.Chart(marathon_training).mark_point(opacity=0.4).encode(\n    x=alt.X(\"max\").title(\"Max Distance Ran per Week During Training (miles)\"),\n    y=alt.Y(\"time_hrs\")\n        .title(\"Race Time (hours)\")\n        .scale(zero=False)\n)\n\nmarathon_scatter\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_scatter is None)).encode(\"utf-8\")+b\"6986b\").hexdigest() == \"be72f6a83cd9482319f43f4b049fdd940e1f4c6d\", \"type of marathon_scatter is None is not bool. marathon_scatter is None should be a bool\"\nassert sha1(str(marathon_scatter is None).encode(\"utf-8\")+b\"6986b\").hexdigest() == \"79682770c2f1c151e70550446c11cc299f292dd9\", \"boolean value of marathon_scatter is None is not correct\"\n\nassert sha1(str(type(marathon_scatter.encoding.x['shorthand'])).encode(\"utf-8\")+b\"6986c\").hexdigest() == \"6d3c30ce678b314024b4d80b82e5ed9697c02ce0\", \"type of marathon_scatter.encoding.x['shorthand'] is not str. marathon_scatter.encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(marathon_scatter.encoding.x['shorthand'])).encode(\"utf-8\")+b\"6986c\").hexdigest() == \"0d12f86dda5f48bc1e5d3675345950ca7a48c4e7\", \"length of marathon_scatter.encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_scatter.encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"6986c\").hexdigest() == \"8630293695025e3d22cab897d7c35cb472b47b6a\", \"value of marathon_scatter.encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_scatter.encoding.x['shorthand']).encode(\"utf-8\")+b\"6986c\").hexdigest() == \"8630293695025e3d22cab897d7c35cb472b47b6a\", \"correct string value of marathon_scatter.encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_scatter.encoding.y['shorthand'])).encode(\"utf-8\")+b\"6986d\").hexdigest() == \"2d41e0469437832c726023d3cf4e14159bd09b4c\", \"type of marathon_scatter.encoding.y['shorthand'] is not str. marathon_scatter.encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_scatter.encoding.y['shorthand'])).encode(\"utf-8\")+b\"6986d\").hexdigest() == \"d931044a5378994c5f2df4ddb4637dbabf5d23f6\", \"length of marathon_scatter.encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_scatter.encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"6986d\").hexdigest() == \"c3e46f0aece8b220b81d7e8ea490b10e5265e671\", \"value of marathon_scatter.encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_scatter.encoding.y['shorthand']).encode(\"utf-8\")+b\"6986d\").hexdigest() == \"c3e46f0aece8b220b81d7e8ea490b10e5265e671\", \"correct string value of marathon_scatter.encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_scatter.mark.type)).encode(\"utf-8\")+b\"6986e\").hexdigest() == \"25bf70dd77143087753edd66a74bdbdab609b641\", \"type of marathon_scatter.mark.type is not str. marathon_scatter.mark.type should be an str\"\nassert sha1(str(len(marathon_scatter.mark.type)).encode(\"utf-8\")+b\"6986e\").hexdigest() == \"64843ac47bf38de071408c16ebd821f565913dc5\", \"length of marathon_scatter.mark.type is not correct\"\nassert sha1(str(marathon_scatter.mark.type.lower()).encode(\"utf-8\")+b\"6986e\").hexdigest() == \"0c5b2e8e01caf2686729e9e986b2e067d2f5f42c\", \"value of marathon_scatter.mark.type is not correct\"\nassert sha1(str(marathon_scatter.mark.type).encode(\"utf-8\")+b\"6986e\").hexdigest() == \"0c5b2e8e01caf2686729e9e986b2e067d2f5f42c\", \"correct string value of marathon_scatter.mark.type but incorrect case of letters\"\n\nassert sha1(str(type(marathon_scatter.data.shape[0])).encode(\"utf-8\")+b\"6986f\").hexdigest() == \"fb072f8df94e039c392391cbcbafbdcbb5f6b693\", \"type of marathon_scatter.data.shape[0] is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(marathon_scatter.data.shape[0]).encode(\"utf-8\")+b\"6986f\").hexdigest() == \"b7c9c137f9bca9116ce605daa01dcd1c0edf9ec4\", \"value of marathon_scatter.data.shape[0] is not correct\"\n\nassert sha1(str(type('opacity' in marathon_scatter.mark.to_dict())).encode(\"utf-8\")+b\"69870\").hexdigest() == \"dd9e9a03b10d76cac8120e773139b76b99491ff9\", \"type of 'opacity' in marathon_scatter.mark.to_dict() is not bool. 'opacity' in marathon_scatter.mark.to_dict() should be a bool\"\nassert sha1(str('opacity' in marathon_scatter.mark.to_dict()).encode(\"utf-8\")+b\"69870\").hexdigest() == \"3c374d1e3da1cbd8cdef7d942941642f04285685\", \"boolean value of 'opacity' in marathon_scatter.mark.to_dict() is not correct\"\n\nassert sha1(str(type(isinstance(marathon_scatter.encoding.x['title'], str))).encode(\"utf-8\")+b\"69871\").hexdigest() == \"e02db54bd87fbf96616612a56f8acf8cf441d515\", \"type of isinstance(marathon_scatter.encoding.x['title'], str) is not bool. isinstance(marathon_scatter.encoding.x['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_scatter.encoding.x['title'], str)).encode(\"utf-8\")+b\"69871\").hexdigest() == \"acd2e62da16b691eb93e82f53cbf50df9822ab62\", \"boolean value of isinstance(marathon_scatter.encoding.x['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(marathon_scatter.encoding.y['title'], str))).encode(\"utf-8\")+b\"69872\").hexdigest() == \"8295031153ea95799755569e0f06ae54b3f3b979\", \"type of isinstance(marathon_scatter.encoding.y['title'], str) is not bool. isinstance(marathon_scatter.encoding.y['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_scatter.encoding.y['title'], str)).encode(\"utf-8\")+b\"69872\").hexdigest() == \"db51f97f3427306d46b9eb73b81474c0e722baa9\", \"boolean value of isinstance(marathon_scatter.encoding.y['title'], str) is not correct\"\n\nprint('Success!')\n\nQuestion 1.3  {points: 1}\nNow that we have looked at our training data, the next step is to build a linear regression model.\nInstead of using the KNeighborsRegressor function, we will be using the LinearRegression function to let scikit-learn know we want to perform a linear regression.\nAssign your answer to an object named lm.\n\n# lm = _____()\n\n# your code here\nraise NotImplementedError\nlm\n\n\nfrom hashlib import sha1\nassert sha1(str(type(lm is None)).encode(\"utf-8\")+b\"72c66\").hexdigest() == \"e97b0813818ca802348d6ece9741f60284f4097b\", \"type of lm is None is not bool. lm is None should be a bool\"\nassert sha1(str(lm is None).encode(\"utf-8\")+b\"72c66\").hexdigest() == \"3ce1f42f1c88457c878430a27cbeb6f909b0785c\", \"boolean value of lm is None is not correct\"\n\nassert sha1(str(type(type(lm))).encode(\"utf-8\")+b\"72c67\").hexdigest() == \"1d32ce0e00e2e0a8dd67995b0fad71c00083aa05\", \"type of type(lm) is not correct\"\nassert sha1(str(type(lm)).encode(\"utf-8\")+b\"72c67\").hexdigest() == \"c91e4bde80d303487c49aae5c34b1f381a58f9c1\", \"value of type(lm) is not correct\"\n\nprint('Success!')\n\nQuestion 1.3.1 {points: 1}\nAfter we have created our linear regression model, the next step is to fit the training dataset.\nAssign your answer to an object named lm_fit.\n\n# ___ = ___.fit(___, ___)\n\n# your code here\nraise NotImplementedError\nlm_fit\n\n\nfrom hashlib import sha1\nassert sha1(str(type(lm_fit is None)).encode(\"utf-8\")+b\"ad3bc\").hexdigest() == \"1def08aec5fd67069ff460655ea29adfcd0cb743\", \"type of lm_fit is None is not bool. lm_fit is None should be a bool\"\nassert sha1(str(lm_fit is None).encode(\"utf-8\")+b\"ad3bc\").hexdigest() == \"fa8f0226d403f32ba484b227d8cc4455061bc019\", \"boolean value of lm_fit is None is not correct\"\n\nassert sha1(str(type(type(lm_fit))).encode(\"utf-8\")+b\"ad3bd\").hexdigest() == \"4366e1fb6059f520d285ba77171d1b1744923f34\", \"type of type(lm_fit) is not correct\"\nassert sha1(str(type(lm_fit)).encode(\"utf-8\")+b\"ad3bd\").hexdigest() == \"86f605051db98874d8cf0eb9c0db6fd94496091f\", \"value of type(lm_fit) is not correct\"\n\nassert sha1(str(type(lm_fit.coef_)).encode(\"utf-8\")+b\"ad3be\").hexdigest() == \"228ee7178527d0f2e5cff2853156700111ed8d69\", \"type of lm_fit.coef_ is not correct\"\nassert sha1(str(lm_fit.coef_).encode(\"utf-8\")+b\"ad3be\").hexdigest() == \"dfde7db1fb226af0937f0a18a269f587a6be23e6\", \"value of lm_fit.coef_ is not correct\"\n\nprint('Success!')\n\nQuestion 1.4  {points: 1}\nNow, let‚Äôs visualize the model predictions as a straight line overlaid on the training data. Use the predict function of lm to create predictions for the marathon_training data. Then, add the column of predictions to the marathon_training data frame using the assign function. Name the resulting data frame marathon_preds and the new column predictions.\nNext, create a scatterplot with the marathon time (y-axis) against the maximum distance run per week (x-axis) from marathon_preds. Use mark_circle with an opacity of 0.4 to avoid overplotting. Assign your plot to a variable called marathon_plot. Plot the predictions as a black line over the data points. Remember the fundamentals of effective visualizations such as having a human-readable axes titles.\nName your plot marathon_plot.\n\n# marathon_preds = ____.assign(\n#     predictions= _____.predict(____)\n# )\n\n# scatterplot = alt.Chart(marathon_preds).mark_circle(opacity=0.4).encode(\n#     x=alt.X(\"max\").title(\"Max Distance Ran per Week During Training (miles)\"),\n#     y=alt.Y(\"time_hrs\")\n#         .title(\"Race Time (hours)\")\n#         .scale(zero=False)\n# )\n\n# marathon_plot = scatterplot + scatterplot.mark_line(color='black').encode(\n#     y=\"predictions\"\n# )\n\n# your code here\nraise NotImplementedError\nmarathon_plot\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_preds is None)).encode(\"utf-8\")+b\"e4358\").hexdigest() == \"bc2dd580b40ad4a5e8f6594b34d9ed807fc56664\", \"type of marathon_preds is None is not bool. marathon_preds is None should be a bool\"\nassert sha1(str(marathon_preds is None).encode(\"utf-8\")+b\"e4358\").hexdigest() == \"02df6a309e4c0c99b29a5ebbbf4dc40df215e117\", \"boolean value of marathon_preds is None is not correct\"\n\nassert sha1(str(type(marathon_preds)).encode(\"utf-8\")+b\"e4359\").hexdigest() == \"65a340844bf360cea4cd1679e420c55d09e7829e\", \"type of type(marathon_preds) is not correct\"\n\nassert sha1(str(type(marathon_preds.shape)).encode(\"utf-8\")+b\"e435a\").hexdigest() == \"94b8396ef806348aa6e6d9b48d968db839e0de43\", \"type of marathon_preds.shape is not tuple. marathon_preds.shape should be a tuple\"\nassert sha1(str(len(marathon_preds.shape)).encode(\"utf-8\")+b\"e435a\").hexdigest() == \"80b67edab105b410e9f659d46edd8aafd252ea78\", \"length of marathon_preds.shape is not correct\"\nassert sha1(str(sorted(map(str, marathon_preds.shape))).encode(\"utf-8\")+b\"e435a\").hexdigest() == \"c199087afcc62232805f8ca370d7d5302e4062f1\", \"values of marathon_preds.shape are not correct\"\nassert sha1(str(marathon_preds.shape).encode(\"utf-8\")+b\"e435a\").hexdigest() == \"75a932b02b8b81bcd148774ce58e3466adaa9909\", \"order of elements of marathon_preds.shape is not correct\"\n\nassert sha1(str(type(\"predictions\" in marathon_preds.columns)).encode(\"utf-8\")+b\"e435b\").hexdigest() == \"7b0e6ae601f0ebf430c4e607354a84912f34ebc0\", \"type of \\\"predictions\\\" in marathon_preds.columns is not bool. \\\"predictions\\\" in marathon_preds.columns should be a bool\"\nassert sha1(str(\"predictions\" in marathon_preds.columns).encode(\"utf-8\")+b\"e435b\").hexdigest() == \"d146815eae7ededa977915471ab4ffbe2dd98791\", \"boolean value of \\\"predictions\\\" in marathon_preds.columns is not correct\"\n\nassert sha1(str(type(sum(marathon_preds.predictions))).encode(\"utf-8\")+b\"e435c\").hexdigest() == \"3531d0a530b71a74d474021791451f589648ab04\", \"type of sum(marathon_preds.predictions) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(marathon_preds.predictions), 2)).encode(\"utf-8\")+b\"e435c\").hexdigest() == \"6c69e3cb1469494214b9c3c30df43792dcd4283e\", \"value of sum(marathon_preds.predictions) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(marathon_preds.time_hrs))).encode(\"utf-8\")+b\"e435d\").hexdigest() == \"f6d32fcdfb7d338ca64c10d130c219848d79c187\", \"type of sum(marathon_preds.time_hrs) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(marathon_preds.time_hrs), 2)).encode(\"utf-8\")+b\"e435d\").hexdigest() == \"2f02cb1ab8ae3a31440a49ce11b661ff8fe9147e\", \"value of sum(marathon_preds.time_hrs) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(marathon_plot is None)).encode(\"utf-8\")+b\"e435e\").hexdigest() == \"6895d3cddb99e4147ba45a6c81bc4ddc364e23c2\", \"type of marathon_plot is None is not bool. marathon_plot is None should be a bool\"\nassert sha1(str(marathon_plot is None).encode(\"utf-8\")+b\"e435e\").hexdigest() == \"5390df15c8c3f55e9daedb39794d57638e99efee\", \"boolean value of marathon_plot is None is not correct\"\n\nassert sha1(str(type(len(marathon_plot.layer))).encode(\"utf-8\")+b\"e435f\").hexdigest() == \"e97a9a1e7ad3c233095b4ba0008121bc6a74261d\", \"type of len(marathon_plot.layer) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(len(marathon_plot.layer)).encode(\"utf-8\")+b\"e435f\").hexdigest() == \"2b20945255b0f2a7695411e709eafe5b6b888d2f\", \"value of len(marathon_plot.layer) is not correct\"\n\nassert sha1(str(type(marathon_plot.layer[0].mark)).encode(\"utf-8\")+b\"e4360\").hexdigest() == \"94d874e6d384f6f20dee36b57d02ee0e9d835e2d\", \"type of marathon_plot.layer[0].mark is not correct\"\nassert sha1(str(marathon_plot.layer[0].mark).encode(\"utf-8\")+b\"e4360\").hexdigest() == \"a944ff98b1e615df7fa19be2866a054377367251\", \"value of marathon_plot.layer[0].mark is not correct\"\n\nassert sha1(str(type(marathon_plot.layer[1].mark)).encode(\"utf-8\")+b\"e4361\").hexdigest() == \"71a33d1d974bc8658efed76d2ca77308b690d2fd\", \"type of marathon_plot.layer[1].mark is not correct\"\nassert sha1(str(marathon_plot.layer[1].mark).encode(\"utf-8\")+b\"e4361\").hexdigest() == \"a277cc73edf5fe1ef0dbef797094351e1114f778\", \"value of marathon_plot.layer[1].mark is not correct\"\n\nassert sha1(str(type(marathon_plot.layer[0].encoding.x['shorthand'])).encode(\"utf-8\")+b\"e4362\").hexdigest() == \"d8ab0ac01648e3309ae4d80e94515a5364bb5499\", \"type of marathon_plot.layer[0].encoding.x['shorthand'] is not str. marathon_plot.layer[0].encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot.layer[0].encoding.x['shorthand'])).encode(\"utf-8\")+b\"e4362\").hexdigest() == \"839d138b1eeed4328365ef5af63e42c0910bb888\", \"length of marathon_plot.layer[0].encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"e4362\").hexdigest() == \"c6293c84c9e09e66386aeb65a91fcd8df6601e09\", \"value of marathon_plot.layer[0].encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.x['shorthand']).encode(\"utf-8\")+b\"e4362\").hexdigest() == \"c6293c84c9e09e66386aeb65a91fcd8df6601e09\", \"correct string value of marathon_plot.layer[0].encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_plot.layer[0].encoding.y['shorthand'])).encode(\"utf-8\")+b\"e4363\").hexdigest() == \"3fd3774342c69f8858f9acea94a03c9890a18a93\", \"type of marathon_plot.layer[0].encoding.y['shorthand'] is not str. marathon_plot.layer[0].encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot.layer[0].encoding.y['shorthand'])).encode(\"utf-8\")+b\"e4363\").hexdigest() == \"c5b64aeb2742f503eb3854087c4f836b06acede1\", \"length of marathon_plot.layer[0].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"e4363\").hexdigest() == \"06f9ff5b96ee6e75cd8cf966d5552fe7f717f260\", \"value of marathon_plot.layer[0].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[0].encoding.y['shorthand']).encode(\"utf-8\")+b\"e4363\").hexdigest() == \"06f9ff5b96ee6e75cd8cf966d5552fe7f717f260\", \"correct string value of marathon_plot.layer[0].encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_plot.layer[1].encoding.y['shorthand'])).encode(\"utf-8\")+b\"e4364\").hexdigest() == \"14b5ec8e3ca9e9418bdbbaa77a9b3aa179999fe9\", \"type of marathon_plot.layer[1].encoding.y['shorthand'] is not str. marathon_plot.layer[1].encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot.layer[1].encoding.y['shorthand'])).encode(\"utf-8\")+b\"e4364\").hexdigest() == \"6f6fc84ebe98dc81fb989889721610b969b2e700\", \"length of marathon_plot.layer[1].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[1].encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"e4364\").hexdigest() == \"6ae5bc9df9fd334f0eac6345e13a01b9dd019844\", \"value of marathon_plot.layer[1].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot.layer[1].encoding.y['shorthand']).encode(\"utf-8\")+b\"e4364\").hexdigest() == \"6ae5bc9df9fd334f0eac6345e13a01b9dd019844\", \"correct string value of marathon_plot.layer[1].encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(isinstance(marathon_plot.layer[0].encoding.x['title'], str))).encode(\"utf-8\")+b\"e4365\").hexdigest() == \"6d1dfcd98beb4541c755ed9ee5b65dbd667070f0\", \"type of isinstance(marathon_plot.layer[0].encoding.x['title'], str) is not bool. isinstance(marathon_plot.layer[0].encoding.x['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_plot.layer[0].encoding.x['title'], str)).encode(\"utf-8\")+b\"e4365\").hexdigest() == \"7b15eabe489e4ffa5a44043c97d017234a7e0a0a\", \"boolean value of isinstance(marathon_plot.layer[0].encoding.x['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(marathon_plot.layer[0].encoding.y['title'], str))).encode(\"utf-8\")+b\"e4366\").hexdigest() == \"f0651e9d8742d2b0750aea688fcf11ea2e3383f8\", \"type of isinstance(marathon_plot.layer[0].encoding.y['title'], str) is not bool. isinstance(marathon_plot.layer[0].encoding.y['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_plot.layer[0].encoding.y['title'], str)).encode(\"utf-8\")+b\"e4366\").hexdigest() == \"0e91055a5f12d74e8593f359e143862b6a8fc193\", \"boolean value of isinstance(marathon_plot.layer[0].encoding.y['title'], str) is not correct\"\n\nprint('Success!')\n\nQuestion 1.5  {points: 1}\nGreat! We can now see the line of best fit on the graph. Now let‚Äôs calculate the RMSPE using the test data. To get to this point, first, use the lm object to make predictions on the test data. Then, add the column of predictions to the marathon_testing data frame using the assign function. Name the resulting data frame test_preds and the new column predictions.\nAfterwards, calculate the RMSPE using the mean_squared_error function.\nAssign the RMSPE score to an object called lm_rmspe.\n\n# ___ = ___.assign(\n#     predictions=___.predict(___)\n# )\n\n# ___ = ___(___, ___)**(1/2)\n\n# your code here\nraise NotImplementedError\nlm_rmspe\n\n\nfrom hashlib import sha1\nassert sha1(str(type(test_preds is None)).encode(\"utf-8\")+b\"3ebdc\").hexdigest() == \"29ebee7ab2061555cdd2621507b3792dbb886297\", \"type of test_preds is None is not bool. test_preds is None should be a bool\"\nassert sha1(str(test_preds is None).encode(\"utf-8\")+b\"3ebdc\").hexdigest() == \"6a7c108f74c9bf5ddfe1618a204e71dd5069e8c5\", \"boolean value of test_preds is None is not correct\"\n\nassert sha1(str(type(test_preds)).encode(\"utf-8\")+b\"3ebdd\").hexdigest() == \"763191835b51c258b842454c271700f9cdfea3bf\", \"type of type(test_preds) is not correct\"\n\nassert sha1(str(type(test_preds.shape)).encode(\"utf-8\")+b\"3ebde\").hexdigest() == \"89d3f1293bc4be750513c8cdb8c60056333d8f96\", \"type of test_preds.shape is not tuple. test_preds.shape should be a tuple\"\nassert sha1(str(len(test_preds.shape)).encode(\"utf-8\")+b\"3ebde\").hexdigest() == \"bc721a860c2268ce910d4702796d17031d8ea8a8\", \"length of test_preds.shape is not correct\"\nassert sha1(str(sorted(map(str, test_preds.shape))).encode(\"utf-8\")+b\"3ebde\").hexdigest() == \"1a83b659bfa801824e9221c8426149a4b5fa861b\", \"values of test_preds.shape are not correct\"\nassert sha1(str(test_preds.shape).encode(\"utf-8\")+b\"3ebde\").hexdigest() == \"bd984a5c94d60104a3e575235235426ccfbc98c7\", \"order of elements of test_preds.shape is not correct\"\n\nassert sha1(str(type(sum(test_preds.predictions))).encode(\"utf-8\")+b\"3ebdf\").hexdigest() == \"a072e028d9a3b80e8f68ddb270a40eb499596870\", \"type of sum(test_preds.predictions) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(test_preds.predictions), 2)).encode(\"utf-8\")+b\"3ebdf\").hexdigest() == \"71c296aa4d1218d1231b125f36f9a6c915f67cf1\", \"value of sum(test_preds.predictions) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(round(lm_rmspe, 1))).encode(\"utf-8\")+b\"3ebe2\").hexdigest() == 'e566caf8a3b27dc42c29f121d257bc3db3506298', \"type of round(lm_rmspe, 1) is not correct\"\nassert sha1(str(round(lm_rmspe, 1)).encode(\"utf-8\")+b\"3ebe2\").hexdigest() == \"a6d4449e0d5e3251aa0d7523dcbda18946bf141d\", \"value of round(lm_rmspe, 1) is not correct\"\n\nprint('Success!')\n\nQuestion 1.5.1  {points: 1}\nNow, let‚Äôs visualize the model predictions as a straight line overlaid on the test data. First, create a scatterplot to assess the relationship between race time (time_hrs) and maximum distance ran per week during training (max) on the testing data. Use mark_circle with an opacity of 0.4 to avoid overplotting. Then add a line to the plot corresponding to the predictions (predictions) from the fit linear regression model. Remember to do whatever is necessary to make this an effective visualization.\nAssign the plot to an object called marathon_plot_test.\n\n# marathon_plot = ___\n\n# your code here\nraise NotImplementedError\nmarathon_plot_test\n\n\nfrom hashlib import sha1\nassert sha1(str(type(marathon_plot_test is None)).encode(\"utf-8\")+b\"61e8f\").hexdigest() == \"ee748f5ffeeccfa151a95c1c8e40fb2e51b1ce9f\", \"type of marathon_plot_test is None is not bool. marathon_plot_test is None should be a bool\"\nassert sha1(str(marathon_plot_test is None).encode(\"utf-8\")+b\"61e8f\").hexdigest() == \"f71b2edba5243e8d1a90ce0d7e7b37557a0f0052\", \"boolean value of marathon_plot_test is None is not correct\"\n\nassert sha1(str(type(len(marathon_plot_test.layer))).encode(\"utf-8\")+b\"61e90\").hexdigest() == \"6d39d35690606462cf06c5a978d8b727d3090082\", \"type of len(marathon_plot_test.layer) is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(len(marathon_plot_test.layer)).encode(\"utf-8\")+b\"61e90\").hexdigest() == \"421355b186989ec09a7f6e161cda4600cd1021bb\", \"value of len(marathon_plot_test.layer) is not correct\"\n\nassert sha1(str(type(marathon_plot_test.layer[0].mark)).encode(\"utf-8\")+b\"61e91\").hexdigest() == \"3246755d74b7833a9ea578910cdf8696d3fce524\", \"type of marathon_plot_test.layer[0].mark is not correct\"\nassert sha1(str(marathon_plot_test.layer[0].mark).encode(\"utf-8\")+b\"61e91\").hexdigest() == \"bf51585cca690c54f425c1a947e05ea8faa39710\", \"value of marathon_plot_test.layer[0].mark is not correct\"\n\nassert sha1(str(type(marathon_plot_test.layer[1].mark)).encode(\"utf-8\")+b\"61e92\").hexdigest() == \"fed14b1460cbd668b78fe2a24b76c721a7b6ef35\", \"type of marathon_plot_test.layer[1].mark is not correct\"\nassert sha1(str(marathon_plot_test.layer[1].mark).encode(\"utf-8\")+b\"61e92\").hexdigest() == \"7c3f41c9edb9129aef9b6afbb5afbd53b5a21bc3\", \"value of marathon_plot_test.layer[1].mark is not correct\"\n\nassert sha1(str(type(marathon_plot_test.layer[0].encoding.x['shorthand'])).encode(\"utf-8\")+b\"61e93\").hexdigest() == \"b36d5e994a7b12c3f08e6d34610ef7745b04a68b\", \"type of marathon_plot_test.layer[0].encoding.x['shorthand'] is not str. marathon_plot_test.layer[0].encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot_test.layer[0].encoding.x['shorthand'])).encode(\"utf-8\")+b\"61e93\").hexdigest() == \"6ab52032148d76cb4c992137ba98a5642ccd1c07\", \"length of marathon_plot_test.layer[0].encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[0].encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"61e93\").hexdigest() == \"f5d25df889e84991486b73bd90133cc33805a2c7\", \"value of marathon_plot_test.layer[0].encoding.x['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[0].encoding.x['shorthand']).encode(\"utf-8\")+b\"61e93\").hexdigest() == \"f5d25df889e84991486b73bd90133cc33805a2c7\", \"correct string value of marathon_plot_test.layer[0].encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_plot_test.layer[0].encoding.y['shorthand'])).encode(\"utf-8\")+b\"61e94\").hexdigest() == \"ddef04a4b677f36a2461ae87bd64c145cbe414e7\", \"type of marathon_plot_test.layer[0].encoding.y['shorthand'] is not str. marathon_plot_test.layer[0].encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot_test.layer[0].encoding.y['shorthand'])).encode(\"utf-8\")+b\"61e94\").hexdigest() == \"d12e59210898a0cbdd59886ed5b66fe72ef419a5\", \"length of marathon_plot_test.layer[0].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[0].encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"61e94\").hexdigest() == \"0528e9c8beb5841dd4f6ea3e120aa2ea6851f6d3\", \"value of marathon_plot_test.layer[0].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[0].encoding.y['shorthand']).encode(\"utf-8\")+b\"61e94\").hexdigest() == \"0528e9c8beb5841dd4f6ea3e120aa2ea6851f6d3\", \"correct string value of marathon_plot_test.layer[0].encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(marathon_plot_test.layer[1].encoding.y['shorthand'])).encode(\"utf-8\")+b\"61e95\").hexdigest() == \"393f83ca4126abac28e3721499e74cbc8eec49f5\", \"type of marathon_plot_test.layer[1].encoding.y['shorthand'] is not str. marathon_plot_test.layer[1].encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(marathon_plot_test.layer[1].encoding.y['shorthand'])).encode(\"utf-8\")+b\"61e95\").hexdigest() == \"cf6648e5023cea85b35e732f848817cbef50a2ea\", \"length of marathon_plot_test.layer[1].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[1].encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"61e95\").hexdigest() == \"482407e92cdb21525edc25dc9759ecd8f28ac4b0\", \"value of marathon_plot_test.layer[1].encoding.y['shorthand'] is not correct\"\nassert sha1(str(marathon_plot_test.layer[1].encoding.y['shorthand']).encode(\"utf-8\")+b\"61e95\").hexdigest() == \"482407e92cdb21525edc25dc9759ecd8f28ac4b0\", \"correct string value of marathon_plot_test.layer[1].encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(isinstance(marathon_plot_test.layer[0].encoding.x['title'], str))).encode(\"utf-8\")+b\"61e96\").hexdigest() == \"4e9a552197c0d9f091c287c51186481faa373b84\", \"type of isinstance(marathon_plot_test.layer[0].encoding.x['title'], str) is not bool. isinstance(marathon_plot_test.layer[0].encoding.x['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_plot_test.layer[0].encoding.x['title'], str)).encode(\"utf-8\")+b\"61e96\").hexdigest() == \"b5a959a717c4d36b796c0351445d67ceba59555b\", \"boolean value of isinstance(marathon_plot_test.layer[0].encoding.x['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(marathon_plot_test.layer[0].encoding.y['title'], str))).encode(\"utf-8\")+b\"61e97\").hexdigest() == \"69d55630d9b08b127c64a06a01079ec5fd719e2c\", \"type of isinstance(marathon_plot_test.layer[0].encoding.y['title'], str) is not bool. isinstance(marathon_plot_test.layer[0].encoding.y['title'], str) should be a bool\"\nassert sha1(str(isinstance(marathon_plot_test.layer[0].encoding.y['title'], str)).encode(\"utf-8\")+b\"61e97\").hexdigest() == \"9669df5114cc0166cef2d5b5f35d9333a937d24f\", \"boolean value of isinstance(marathon_plot_test.layer[0].encoding.y['title'], str) is not correct\"\n\nprint('Success!')\n\nQuestion 1.6  {points: 1}\nCompare the RMSPE of k-nn regression (0.616 from last worksheet) to that of simple linear regression. Which is greater?\nA. Simple linear regression has a greater RMSPE\nB. \\(k\\)-nn regression has a greater RMSPE\nC. Neither, they are identical\nSave the letter of your answer to a variable named answer1_6. Make sure you put quotations around the letter and pay attention to case.\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer1_6)).encode(\"utf-8\")+b\"7d5bf\").hexdigest() == \"3cebb18cc78a764b9b957667aa3b539e088a53b6\", \"type of answer1_6 is not str. answer3_6 should be an str\"\nassert sha1(str(len(answer1_6)).encode(\"utf-8\")+b\"7d5bf\").hexdigest() == \"e2ea38993eafe6e4258dd8f2732bc8f10bf05850\", \"length of answer1_6 is not correct\"\nassert sha1(str(answer1_6.lower()).encode(\"utf-8\")+b\"7d5bf\").hexdigest() == \"d8fc4778f9ae37aeb0d7eebc142c30f17a58afcf\", \"value of answer1_6 is not correct\"\nassert sha1(str(answer1_6).encode(\"utf-8\")+b\"7d5bf\").hexdigest() == \"3469bf05b66975e09572d8f6cebca168c20a4543\", \"correct string value of answer1_6 but incorrect case of letters\"\n\nprint('Success!')\n\nQuestion 1.7  {points: 1}\nWhich model does a better job of predicting on the test dataset?\nA. Simple linear regression\nB. \\(k\\)-nn regression\nC. Neither, they are identical\nSave the letter of your answer to a variable named answer1_7. Make sure you put quotations around the letter and pay attention to case.\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer1_7)).encode(\"utf-8\")+b\"c4b77\").hexdigest() == \"cbde8d789af13720b9730aa64643b32c579e43c8\", \"type of answer1_7 is not str. answer1_7 should be an str\"\nassert sha1(str(len(answer1_7)).encode(\"utf-8\")+b\"c4b77\").hexdigest() == \"1f1de87eb3b5d616cdc5148f63387a030af2dcdd\", \"length of answer1_7 is not correct\"\nassert sha1(str(answer1_7.lower()).encode(\"utf-8\")+b\"c4b77\").hexdigest() == \"9500e6ffe1848679b170175d4752de72899e0f9e\", \"value of answer1_7 is not correct\"\nassert sha1(str(answer1_7).encode(\"utf-8\")+b\"c4b77\").hexdigest() == \"e7041b0f0cc00781dddf503ef1693bd99029eedf\", \"correct string value of answer1_7 but incorrect case of letters\"\n\nprint('Success!')\n\nGiven that the linear regression model is a straight line, we can write our model as a mathematical equation. We can get the two numbers we need for this from the coef_ and intercept_ attributes from lm_fit.\n\n# run this cell\nprint(f\"The coefficient for the linear regression is {lm_fit.coef_[0]:0.3f}.\")\nprint(f\"The intercept for the linear regression is {lm_fit.intercept_:0.3f}.\")\n\nQuestion 1.8.1  {points: 1}\nWhich of the following mathematical equations represents the model based on the numbers output in the cell above?\nA. \\(Predicted \\ race \\ time \\ (in \\ hours) = 4.851 - 0.022  \\times max \\ (in \\ miles)\\)\nB. \\(Predicted \\ race \\ time \\ (in \\ hours) = -0.022 + 4.851 \\times max \\ (in \\ miles)\\)\nC. \\(Predicted \\ max \\ (in \\ miles) = 4.851 - 0.022 \\times  \\ race \\ time \\ (in \\ hours)\\)\nD. \\(Predicted \\ max \\ (in \\ miles) = -0.022 + 4.851 \\times  \\ race \\ time \\ (in \\ hours)\\)\nSave the letter of your answer to a variable named answer1_8_1. Make sure you put quotations around the letter and pay attention to case.\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer1_8_1)).encode(\"utf-8\")+b\"5b6e9\").hexdigest() == \"c1e898538331c6feb698ad544c8e708eb101f335\", \"type of answer3_8_1 is not str. answer3_8_1 should be an str\"\nassert sha1(str(len(answer1_8_1)).encode(\"utf-8\")+b\"5b6e9\").hexdigest() == \"4f4756a152ba2c274ee3b194dbe295ac57e849ac\", \"length of answer3_8_1 is not correct\"\nassert sha1(str(answer1_8_1.lower()).encode(\"utf-8\")+b\"5b6e9\").hexdigest() == \"11099d2bab2f6a29a949f290697d5e5906cece6b\", \"value of answer3_8_1 is not correct\"\nassert sha1(str(answer1_8_1).encode(\"utf-8\")+b\"5b6e9\").hexdigest() == \"8a5fd0b1b4b71ea33d160ad2d610f6af5c67f963\", \"correct string value of answer3_8_1 but incorrect case of letters\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html",
    "href": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html",
    "title": "Worksheet Classification (Part I)",
    "section": "",
    "text": "After completing this workshop session, you will be able to:\n\nRecognize situations where a simple classifier would be appropriate for making predictions.\nExplain the \\(K\\)-nearest neighbour classification algorithm.\nInterpret the output of a classifier.\nCompute, by hand, the distance between points when there are two explanatory variables/predictors.\nDescribe what a training data set is and how it is used in classification.\nGiven a dataset with two explanatory variables/predictors, use \\(K\\)-nearest neighbour classification in Python using the scikit-learn framework to predict the class of a single new observation.\n\nThis worksheet covers parts of Chapter 5 of the online textbook. You should read this chapter to gain a better understanding of the assignment. Any place you see ___, you must fill in the function, variable, or data to complete the code. Substitute the raise NotImplementedError with your completed code and answers then proceed to run the cell.\n\n### Run this cell before continuing\nimport random\n\nimport altair as alt\nimport pandas as pd\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn import set_config\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#learning-goals",
    "href": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#learning-goals",
    "title": "Worksheet Classification (Part I)",
    "section": "",
    "text": "After completing this workshop session, you will be able to:\n\nRecognize situations where a simple classifier would be appropriate for making predictions.\nExplain the \\(K\\)-nearest neighbour classification algorithm.\nInterpret the output of a classifier.\nCompute, by hand, the distance between points when there are two explanatory variables/predictors.\nDescribe what a training data set is and how it is used in classification.\nGiven a dataset with two explanatory variables/predictors, use \\(K\\)-nearest neighbour classification in Python using the scikit-learn framework to predict the class of a single new observation.\n\nThis worksheet covers parts of Chapter 5 of the online textbook. You should read this chapter to gain a better understanding of the assignment. Any place you see ___, you must fill in the function, variable, or data to complete the code. Substitute the raise NotImplementedError with your completed code and answers then proceed to run the cell.\n\n### Run this cell before continuing\nimport random\n\nimport altair as alt\nimport pandas as pd\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn import set_config\n\n# Simplify working with large datasets in Altair\nalt.data_transformers.disable_max_rows()\n\n# Output dataframes instead of arrays\nset_config(transform_output=\"pandas\")"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#breast-cancer-data-set",
    "href": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#breast-cancer-data-set",
    "title": "Worksheet Classification (Part I)",
    "section": "1. Breast Cancer Data Set",
    "text": "1. Breast Cancer Data Set\nWe will work with the breast cancer data from this from the accompanying textbook chapter.\n\nNote that the breast cancer data in this worksheet have been standardized (centred and scaled) for you already. We will implement these steps in future worksheet later, but for now, know the data has been standardized. Therefore the variables are unitless and hence why we have zero and negative values for variables like Radius.\n\nQuestion 1.0  {points: 1}\nRead the clean-wdbc-data.csv file (found in the data directory) using the pd.read_csv function into the notebook and store it as a data frame. Name it cancer.\n\n# your code here\nraise NotImplementedError\ncancer\n\n\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\nCell In[2], line 2\n      1 # your code here\n----&gt; 2 raise NotImplementedError\n      3 cancer\n\nNotImplementedError: \n\n\n\n\nfrom hashlib import sha1\nassert sha1(str(type(cancer is None)).encode(\"utf-8\")+b\"1edfa\").hexdigest() == \"f89107c5738f5567ac4ce7e619af326d8dc7e7e4\", \"type of cancer is None is not bool. cancer is None should be a bool\"\nassert sha1(str(cancer is None).encode(\"utf-8\")+b\"1edfa\").hexdigest() == \"71bbe216c3f112174b74da6122dd837cb4abaafa\", \"boolean value of cancer is None is not correct\"\n\nassert sha1(str(type(cancer)).encode(\"utf-8\")+b\"1edfb\").hexdigest() == \"5f4717efa0f9568127506afdab187398929a3f76\", \"type of type(cancer) is not correct\"\n\nassert sha1(str(type(cancer.shape)).encode(\"utf-8\")+b\"1edfc\").hexdigest() == \"6790c026fc62f7f025cc5dfb65b5589e70c94f24\", \"type of cancer.shape is not tuple. cancer.shape should be a tuple\"\nassert sha1(str(len(cancer.shape)).encode(\"utf-8\")+b\"1edfc\").hexdigest() == \"e400bacf4406589a930f6cb04146f55ae09adbe1\", \"length of cancer.shape is not correct\"\nassert sha1(str(sorted(map(str, cancer.shape))).encode(\"utf-8\")+b\"1edfc\").hexdigest() == \"bc58412175f96aad1b5f00e35cbf014630e9667a\", \"values of cancer.shape are not correct\"\nassert sha1(str(cancer.shape).encode(\"utf-8\")+b\"1edfc\").hexdigest() == \"03f45e6b2934a9d89a82761d8cd384e41a96882e\", \"order of elements of cancer.shape is not correct\"\n\nassert sha1(str(type(sum(cancer.Area))).encode(\"utf-8\")+b\"1edfd\").hexdigest() == \"8459034dfcce3dc398256d3235e7ea1c0a6eab66\", \"type of sum(cancer.Area) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(cancer.Area), 2)).encode(\"utf-8\")+b\"1edfd\").hexdigest() == \"578eeb3cce10656ea3740570a63c462e74b5bee3\", \"value of sum(cancer.Area) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(cancer.columns.values)).encode(\"utf-8\")+b\"1edfe\").hexdigest() == \"ec1b028616e5bf668b318569d326edbe4fa5792d\", \"type of cancer.columns.values is not correct\"\nassert sha1(str(cancer.columns.values).encode(\"utf-8\")+b\"1edfe\").hexdigest() == \"46e462ccc418670863709af1c7c6c89e4aa501be\", \"value of cancer.columns.values is not correct\"\n\nassert sha1(str(type(cancer['Class'].dtype)).encode(\"utf-8\")+b\"1edff\").hexdigest() == \"0c82f629a8e37cf9fc16959e34213e0fa89a6a68\", \"type of cancer['Class'].dtype is not correct\"\nassert sha1(str(cancer['Class'].dtype).encode(\"utf-8\")+b\"1edff\").hexdigest() == \"852a6171a68036a67ec486822760a69c8cd61e0d\", \"value of cancer['Class'].dtype is not correct\"\n\nprint('Success!')\n\nSuccess!\n\n\nQuestion 1.1 True or False:  {points: 1}\nAfter looking at the first six rows of the cancer data fame, suppose we asked you to predict the variable ‚Äúarea‚Äù for a new observation. Is this a classification problem?\nAssign your answer to an object called answer1_1. Make sure the correct answer is a boolean. i.e.¬†True or False.\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer1_1)).encode(\"utf-8\")+b\"3524b\").hexdigest() == \"96e967f4261014f1a2a76ba5f230d7b7e85abe83\", \"type of answer1_1 is not bool. answer1_1 should be a bool\"\nassert sha1(str(answer1_1).encode(\"utf-8\")+b\"3524b\").hexdigest() == \"88e4fbb75d430084961a28b324179d14f4999b12\", \"boolean value of answer1_1 is not correct\"\n\nprint('Success!')\n\nQuestion 1.2  {points: 1}\nCreate a scatterplot of the data with Symmetry on the x-axis and Radius on the y-axis. Modify your aesthetics by colouring for Class. As you create this plot, ensure you follow the guidelines for creating effective visualizations. In particular, note in the chart axis titles whether the data is standardized or not and add a suitable opacity level to the graphical mark. You should also replace the values in the dataframe‚Äôs Class column from 'M' to 'Malignant' and from 'B' to 'Benign'.\nAssign your plot to an object called cancer_plot.\n\ncancer[\"Class\"] = cancer[\"Class\"].replace({\n    'M' : 'Malignant',\n    'B' : 'Benign'\n})\ncancer_plot = alt.Chart(cancer).mark_point(opacity=0.5).encode(\n    x=alt.X(\"Symmetry\").title(\"Standardized symmetry\"),\n    y=alt.Y(\"Radius\").title(\"Standardized radius\"),\n    color=alt.Color(\"Class\").title(\"Diagnosis\")\n)\ncancer_plot\n\n\nfrom hashlib import sha1\nassert sha1(str(type(cancer['Class'].unique())).encode(\"utf-8\")+b\"7038e\").hexdigest() == \"2c870d7a3657b742557d66961de4a4891ee76aa2\", \"type of cancer['Class'].unique() is not correct\"\nassert sha1(str(cancer['Class'].unique()).encode(\"utf-8\")+b\"7038e\").hexdigest() == \"7be82206c225ab0f0a4ffad7c1488a64143481f7\", \"value of cancer['Class'].unique() is not correct\"\n\nassert sha1(str(type(cancer_plot is None)).encode(\"utf-8\")+b\"7038f\").hexdigest() == \"5f05f0c0e171e12d0d2b2a22d2789f0d9a8e342e\", \"type of cancer_plot is None is not bool. cancer_plot is None should be a bool\"\nassert sha1(str(cancer_plot is None).encode(\"utf-8\")+b\"7038f\").hexdigest() == \"515f1ad42fa781dff7c9975c8ded7f317ce95332\", \"boolean value of cancer_plot is None is not correct\"\n\nassert sha1(str(type(cancer_plot.encoding.x['shorthand'])).encode(\"utf-8\")+b\"70390\").hexdigest() == \"1b054e65bf0aac12e848da1d2afa2b12a5226b61\", \"type of cancer_plot.encoding.x['shorthand'] is not str. cancer_plot.encoding.x['shorthand'] should be an str\"\nassert sha1(str(len(cancer_plot.encoding.x['shorthand'])).encode(\"utf-8\")+b\"70390\").hexdigest() == \"dd930b7fa9aa4d16b4d1a89c7a39089300827489\", \"length of cancer_plot.encoding.x['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.x['shorthand'].lower()).encode(\"utf-8\")+b\"70390\").hexdigest() == \"dd17739f27d755296153ed0ec742286fd3b39cb2\", \"value of cancer_plot.encoding.x['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.x['shorthand']).encode(\"utf-8\")+b\"70390\").hexdigest() == \"d4212b1d5679f4601309609b6ea38ffb2d4ed07c\", \"correct string value of cancer_plot.encoding.x['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(cancer_plot.encoding.y['shorthand'])).encode(\"utf-8\")+b\"70391\").hexdigest() == \"6fac44d279065f2a3f185c5a884fd9df6dd81cfb\", \"type of cancer_plot.encoding.y['shorthand'] is not str. cancer_plot.encoding.y['shorthand'] should be an str\"\nassert sha1(str(len(cancer_plot.encoding.y['shorthand'])).encode(\"utf-8\")+b\"70391\").hexdigest() == \"df067cfef2a7c5291dd0f9acc53fe92d50ccf1fe\", \"length of cancer_plot.encoding.y['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.y['shorthand'].lower()).encode(\"utf-8\")+b\"70391\").hexdigest() == \"edf8bc4ede29cf991a36bb6d8b38713806753b47\", \"value of cancer_plot.encoding.y['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.y['shorthand']).encode(\"utf-8\")+b\"70391\").hexdigest() == \"e3f5fd9a0d9f3c695791e536282f29ee7d2c6b4a\", \"correct string value of cancer_plot.encoding.y['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(cancer_plot.encoding.color['shorthand'])).encode(\"utf-8\")+b\"70392\").hexdigest() == \"a1514e0d638e7fa47039d17e310bba44e6a8ff2c\", \"type of cancer_plot.encoding.color['shorthand'] is not str. cancer_plot.encoding.color['shorthand'] should be an str\"\nassert sha1(str(len(cancer_plot.encoding.color['shorthand'])).encode(\"utf-8\")+b\"70392\").hexdigest() == \"a43d370b386a7d21c0ac8e73bdc942b85ee93cae\", \"length of cancer_plot.encoding.color['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.color['shorthand'].lower()).encode(\"utf-8\")+b\"70392\").hexdigest() == \"a2e54ddb2a451b8647df9f5b9aae970b19620f66\", \"value of cancer_plot.encoding.color['shorthand'] is not correct\"\nassert sha1(str(cancer_plot.encoding.color['shorthand']).encode(\"utf-8\")+b\"70392\").hexdigest() == \"dfff632fd3104b75a651bf07567aa5d572835f70\", \"correct string value of cancer_plot.encoding.color['shorthand'] but incorrect case of letters\"\n\nassert sha1(str(type(cancer_plot.mark)).encode(\"utf-8\")+b\"70393\").hexdigest() == \"e736aec18d8c2224b69bfe505db7d3968a4a2c7e\", \"type of cancer_plot.mark is not correct\"\nassert sha1(str(cancer_plot.mark).encode(\"utf-8\")+b\"70393\").hexdigest() == \"1746955c8bc53ef47b4cf0267c98ce50ce3b184a\", \"value of cancer_plot.mark is not correct\"\n\nassert sha1(str(type(isinstance(cancer_plot.encoding.color['title'], str))).encode(\"utf-8\")+b\"70394\").hexdigest() == \"40775fb7d185dcb2ad7acf54925bb5075bd50aca\", \"type of isinstance(cancer_plot.encoding.color['title'], str) is not bool. isinstance(cancer_plot.encoding.color['title'], str) should be a bool\"\nassert sha1(str(isinstance(cancer_plot.encoding.color['title'], str)).encode(\"utf-8\")+b\"70394\").hexdigest() == \"e42ecc8f1d08c9a9cbb062b31d6180e6271e740b\", \"boolean value of isinstance(cancer_plot.encoding.color['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(cancer_plot.encoding.x['title'], str))).encode(\"utf-8\")+b\"70395\").hexdigest() == \"711e00550c10a33b1b90544076b3b8d4a89d23bf\", \"type of isinstance(cancer_plot.encoding.x['title'], str) is not bool. isinstance(cancer_plot.encoding.x['title'], str) should be a bool\"\nassert sha1(str(isinstance(cancer_plot.encoding.x['title'], str)).encode(\"utf-8\")+b\"70395\").hexdigest() == \"b697a6b22f874c7f85b3e081a24d1b5cdd084659\", \"boolean value of isinstance(cancer_plot.encoding.x['title'], str) is not correct\"\n\nassert sha1(str(type(isinstance(cancer_plot.encoding.y['title'], str))).encode(\"utf-8\")+b\"70396\").hexdigest() == \"37f686ca3136e2e102714b66fd51cf3ef4703fc6\", \"type of isinstance(cancer_plot.encoding.y['title'], str) is not bool. isinstance(cancer_plot.encoding.y['title'], str) should be a bool\"\nassert sha1(str(isinstance(cancer_plot.encoding.y['title'], str)).encode(\"utf-8\")+b\"70396\").hexdigest() == \"d0eb5c4439424db54df14662a17296d7dc74ff3d\", \"boolean value of isinstance(cancer_plot.encoding.y['title'], str) is not correct\"\n\nprint('Success!')\n\nQuestion 1.3  {points: 1}\nJust by looking at the scatterplot above, how would you classify an observation with Symmetry = 1 and Radius = 1 (Benign or Malignant)?\nAssign your answer to an object called answer1_3. Make sure the correct answer is written fully. Remember to surround your answer with quotation marks (e.g.¬†‚ÄúBenign‚Äù / ‚ÄúMalignant‚Äù).\n\n# your code here\nraise NotImplementedError\n\n\nfrom hashlib import sha1\nassert sha1(str(type(answer1_3)).encode(\"utf-8\")+b\"192dd\").hexdigest() == \"c22b7e131d7091f92de118d328cf99ebfc373e56\", \"type of answer1_3 is not str. answer1_3 should be an str\"\nassert sha1(str(len(answer1_3)).encode(\"utf-8\")+b\"192dd\").hexdigest() == \"426e383fda7dfde77b6cbef2a70be0047a593331\", \"length of answer1_3 is not correct\"\nassert sha1(str(answer1_3.lower()).encode(\"utf-8\")+b\"192dd\").hexdigest() == \"04f0ffa0d0870e9e4d9a8bf7bb1ed255f9165a52\", \"value of answer1_3 is not correct\"\nassert sha1(str(answer1_3).encode(\"utf-8\")+b\"192dd\").hexdigest() == \"11394ec2da0fa66e3dd679f582c3d6024557dc1a\", \"correct string value of answer1_3 but incorrect case of letters\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#using-scikit-learn-to-perform-k-nearest-neighbours",
    "href": "materials/worksheets/py_worksheet_classification1/py_worksheet_classification1.html#using-scikit-learn-to-perform-k-nearest-neighbours",
    "title": "Worksheet Classification (Part I)",
    "section": "2. Using scikit-learn to perform k-nearest neighbours",
    "text": "2. Using scikit-learn to perform k-nearest neighbours\nNow that we understand how K-nearest neighbours (k-nn) classification works, let‚Äôs get familar with the scikit-learn Python package. The benefit of using scikit-learn is that it will keep our code simple, readable and accurate. Coding less and in a tidier format means that there is less chance for errors to occur.\nWe‚Äôll again focus on Radius and Symmetry as the two predictors. This time, we would like to predict the class of a new observation with Symmetry = 1 and Radius = 0. This one is a bit tricky to do visually from the plot below, and so is a motivating example for us to compute the prediction using k-nn with the scikit-learn package. Let‚Äôs use K = 7.\n\n# Run this to remind yourself what the data looks like\ncancer_plot\n\nQuestion 2.1  {points: 1}\nCreate a model for K-nearest neighbours classification by using the KNeighborsClassifier function. Specify that we want to set n_neighbors = 7.\nName your model specification knn_spec.\n\n# ___ = KNeighborsClassifier(n_neighbors=___)\n\n# your code here\nraise NotImplementedError\nknn_spec\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_spec is None)).encode(\"utf-8\")+b\"2245\").hexdigest() == \"78a85e8a7790e4e7a7d20d518a6e9e23249aeb08\", \"type of knn_spec is None is not bool. knn_spec is None should be a bool\"\nassert sha1(str(knn_spec is None).encode(\"utf-8\")+b\"2245\").hexdigest() == \"b44afb32e99c75fdbfb62a424921d1a2fa6f12ee\", \"boolean value of knn_spec is None is not correct\"\n\nassert sha1(str(type(knn_spec.n_neighbors)).encode(\"utf-8\")+b\"2246\").hexdigest() == \"1458700d2adfd82356dfd982b7941979cbd45589\", \"type of knn_spec.n_neighbors is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(knn_spec.n_neighbors).encode(\"utf-8\")+b\"2246\").hexdigest() == \"22e82178737bce10c71283d1cf37544dd1dcbbb8\", \"value of knn_spec.n_neighbors is not correct\"\n\nassert sha1(str(type(knn_spec.algorithm)).encode(\"utf-8\")+b\"2247\").hexdigest() == \"c71db7d462b7abd56ce64492cec1d48ea9040224\", \"type of knn_spec.algorithm is not str. knn_spec.algorithm should be an str\"\nassert sha1(str(len(knn_spec.algorithm)).encode(\"utf-8\")+b\"2247\").hexdigest() == \"fc15a33f2e81ff2a1ebe2cdd52ca95582dc1df46\", \"length of knn_spec.algorithm is not correct\"\nassert sha1(str(knn_spec.algorithm.lower()).encode(\"utf-8\")+b\"2247\").hexdigest() == \"6fb8474d83ec0dc215e52cdc08b990751b7d94a9\", \"value of knn_spec.algorithm is not correct\"\nassert sha1(str(knn_spec.algorithm).encode(\"utf-8\")+b\"2247\").hexdigest() == \"6fb8474d83ec0dc215e52cdc08b990751b7d94a9\", \"correct string value of knn_spec.algorithm but incorrect case of letters\"\n\nprint('Success!')\n\nQuestion 2.2  {points: 1}\nTo train the model on the breast cancer dataset, pass knn_spec and the cancer dataset to the .fit function. Specify Class as your target variable and the Symmetry and Radius variables as your predictors. Name your fitted model as knn_fit.\n\n# X = ___[[\"Symmetry\", ___]]\n# y = ___[___]\n# ___ = ___.fit(___, ___)\n\n# your code here\nraise NotImplementedError\nknn_fit\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_fit is None)).encode(\"utf-8\")+b\"2bff6\").hexdigest() == \"56a79c088e5ec5a42b96d6ea0d14cd8712fac03e\", \"type of knn_fit is None is not bool. knn_fit is None should be a bool\"\nassert sha1(str(knn_fit is None).encode(\"utf-8\")+b\"2bff6\").hexdigest() == \"268e8a79ae1936524d056ca5169d2c71a72e70a5\", \"boolean value of knn_fit is None is not correct\"\n\nassert sha1(str(type(type(knn_fit))).encode(\"utf-8\")+b\"2bff7\").hexdigest() == \"097a69bbecb57e7e121a5edde688e93d2fbf9d17\", \"type of type(knn_fit) is not correct\"\nassert sha1(str(type(knn_fit)).encode(\"utf-8\")+b\"2bff7\").hexdigest() == \"40872fc0b9fdb9d1467bfb606dfb31df50cfe4ea\", \"value of type(knn_fit) is not correct\"\n\nassert sha1(str(type(knn_fit.classes_)).encode(\"utf-8\")+b\"2bff8\").hexdigest() == \"cb4bb5ca40cc7954a382bfa33bd1532a3df2e583\", \"type of knn_fit.classes_ is not correct\"\nassert sha1(str(knn_fit.classes_).encode(\"utf-8\")+b\"2bff8\").hexdigest() == \"c422ff7ed5336560783f0ada3740882c912a4d97\", \"value of knn_fit.classes_ is not correct\"\n\nassert sha1(str(type(knn_fit.effective_metric_)).encode(\"utf-8\")+b\"2bff9\").hexdigest() == \"aa9d11a76326cf64bec451997ff4ae28ec4e2a9a\", \"type of knn_fit.effective_metric_ is not str. knn_fit.effective_metric_ should be an str\"\nassert sha1(str(len(knn_fit.effective_metric_)).encode(\"utf-8\")+b\"2bff9\").hexdigest() == \"6e6efca67ebf0603a534109614346757c0ef146d\", \"length of knn_fit.effective_metric_ is not correct\"\nassert sha1(str(knn_fit.effective_metric_.lower()).encode(\"utf-8\")+b\"2bff9\").hexdigest() == \"49892ddeb7ca1f1c4bf772ff4a585dd10249b813\", \"value of knn_fit.effective_metric_ is not correct\"\nassert sha1(str(knn_fit.effective_metric_).encode(\"utf-8\")+b\"2bff9\").hexdigest() == \"49892ddeb7ca1f1c4bf772ff4a585dd10249b813\", \"correct string value of knn_fit.effective_metric_ but incorrect case of letters\"\n\nassert sha1(str(type(knn_fit.n_features_in_)).encode(\"utf-8\")+b\"2bffa\").hexdigest() == \"08548a765b78a776a8b132bc9a51a75e1d960554\", \"type of knn_fit.n_features_in_ is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(knn_fit.n_features_in_).encode(\"utf-8\")+b\"2bffa\").hexdigest() == \"114d53a02f71bf2cea7abe3280555955b16647b8\", \"value of knn_fit.n_features_in_ is not correct\"\n\nassert sha1(str(type(X.columns.values)).encode(\"utf-8\")+b\"2bffb\").hexdigest() == \"bd7b6259f8053a315fca30986b168530209f266d\", \"type of X.columns.values is not correct\"\nassert sha1(str(X.columns.values).encode(\"utf-8\")+b\"2bffb\").hexdigest() == \"b7dfb165b13d0e67f16b09e1dc18733df42a65a0\", \"value of X.columns.values is not correct\"\n\nassert sha1(str(type(y.name)).encode(\"utf-8\")+b\"2bffc\").hexdigest() == \"e8b9e2753401141ac2885e2394f32cc9cffa348a\", \"type of y.name is not str. y.name should be an str\"\nassert sha1(str(len(y.name)).encode(\"utf-8\")+b\"2bffc\").hexdigest() == \"793a9c94797d9b5899e9865188980d72d080ac2d\", \"length of y.name is not correct\"\nassert sha1(str(y.name.lower()).encode(\"utf-8\")+b\"2bffc\").hexdigest() == \"bcffa9a693072729c80524aafde25fd6b259f4c3\", \"value of y.name is not correct\"\nassert sha1(str(y.name).encode(\"utf-8\")+b\"2bffc\").hexdigest() == \"788a47e28ae9d6cd3cbf4823a5d93017aa6addd9\", \"correct string value of y.name but incorrect case of letters\"\n\nassert sha1(str(type(sum(X.Symmetry))).encode(\"utf-8\")+b\"2bffd\").hexdigest() == \"903f7c4e29e57fe29f567ef056e34f51335aec78\", \"type of sum(X.Symmetry) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(X.Symmetry), 2)).encode(\"utf-8\")+b\"2bffd\").hexdigest() == \"9397d4f7e4087a12b928268b533b6f46554e7efa\", \"value of sum(X.Symmetry) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(X.Radius))).encode(\"utf-8\")+b\"2bffe\").hexdigest() == \"d9df2ec2ea3462b25d7d591366163062bdb4021e\", \"type of sum(X.Radius) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(X.Radius), 2)).encode(\"utf-8\")+b\"2bffe\").hexdigest() == \"5bd565738755b0a3859dbce84dffc4a36fe6656e\", \"value of sum(X.Radius) is not correct (rounded to 2 decimal places)\"\n\nprint('Success!')\n\nQuestion 2.3 {points: 1}\nNow we will make our prediction on the Class of a new observation with a Symmetry of 1 and a Radius of 0. First, create a dataframe with these variables and values and call it new_obs. Next, use the .predict function to obtain our prediction by passing knn_fit and new_obs to it. Name your predicted class as class_prediction.\n\n# ___ = pd.DataFrame([[1, 0]], columns=[___, ___])\n# ___ = ___.predict(___)\n\n# your code here\nraise NotImplementedError\nclass_prediction\n\n\nfrom hashlib import sha1\nassert sha1(str(type(new_obs is None)).encode(\"utf-8\")+b\"cc461\").hexdigest() == \"ab0357cbb8bfe0c9949b6d69e4c245d2af0635c6\", \"type of new_obs is None is not bool. new_obs is None should be a bool\"\nassert sha1(str(new_obs is None).encode(\"utf-8\")+b\"cc461\").hexdigest() == \"a170f5d0292d586ae590d1baca2f13355fb1c178\", \"boolean value of new_obs is None is not correct\"\n\nassert sha1(str(type(new_obs)).encode(\"utf-8\")+b\"cc462\").hexdigest() == \"46dce56999c53d1950618cc8e55690c524892a78\", \"type of type(new_obs) is not correct\"\n\nassert sha1(str(type(new_obs.Symmetry.values)).encode(\"utf-8\")+b\"cc463\").hexdigest() == \"113c0758f0ab20fd6712d816469f64cbc4243a12\", \"type of new_obs.Symmetry.values is not correct\"\nassert sha1(str(new_obs.Symmetry.values).encode(\"utf-8\")+b\"cc463\").hexdigest() == \"a41abcf000cf506078cdf4e306ca8121772da0f6\", \"value of new_obs.Symmetry.values is not correct\"\n\nassert sha1(str(type(new_obs.Radius.values)).encode(\"utf-8\")+b\"cc464\").hexdigest() == \"ca2beddfd89352b63fc45c70ca25260e7271c928\", \"type of new_obs.Radius.values is not correct\"\nassert sha1(str(new_obs.Radius.values).encode(\"utf-8\")+b\"cc464\").hexdigest() == \"195b0a370675aea18bf01c541c099428f4e248bd\", \"value of new_obs.Radius.values is not correct\"\n\nassert sha1(str(type(class_prediction is None)).encode(\"utf-8\")+b\"cc465\").hexdigest() == \"1ebd9c72926a2430221b10ae35a374b95040d55f\", \"type of class_prediction is None is not bool. class_prediction is None should be a bool\"\nassert sha1(str(class_prediction is None).encode(\"utf-8\")+b\"cc465\").hexdigest() == \"883aa082b8853226ec339ff84fd4744558d7fd3b\", \"boolean value of class_prediction is None is not correct\"\n\nassert sha1(str(type(class_prediction)).encode(\"utf-8\")+b\"cc466\").hexdigest() == \"a0d1780e9e6b14b3a86e32b907408ae8253ceec5\", \"type of class_prediction is not correct\"\nassert sha1(str(class_prediction).encode(\"utf-8\")+b\"cc466\").hexdigest() == \"2704eb31133cad075ccb97f6c93ca707487c9642\", \"value of class_prediction is not correct\"\n\nprint('Success!')\n\nQuestion 2.4  {points: 1}\nLet‚Äôs perform K-nearest neighbour classification again, but with three predictors. Use the scikit-learn package and K = 7 to classify a new observation where we measure Symmetry = 1, Radius = 0 and Concavity = 1. Use the scaffolding from Questions 2.2 and 2.3 to help you.\n\nPass the same knn_spec from before to fit, but this time specify Symmetry, Radius, and Concavity as the predictors. Save the predictor as X_2 and the target as y_2. Store the output in knn_fit_2.\nStore the new observation values in an object called new_obs_2.\nStore the output of predict in an object called class_prediction_2.\n\n\n# your code here\nraise NotImplementedError\nclass_prediction_2\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_fit_2 is None)).encode(\"utf-8\")+b\"1c52c\").hexdigest() == \"56fcaa2be4db81bcc9472911429b629cba6101f3\", \"type of knn_fit_2 is None is not bool. knn_fit_2 is None should be a bool\"\nassert sha1(str(knn_fit_2 is None).encode(\"utf-8\")+b\"1c52c\").hexdigest() == \"99533bc9488848c2608ec11993eb5cfe3033cdf5\", \"boolean value of knn_fit_2 is None is not correct\"\n\nassert sha1(str(type(knn_fit_2.kneighbors)).encode(\"utf-8\")+b\"1c52d\").hexdigest() == \"3f30af90ec8c1a811004f1225ae54fbd83681019\", \"type of knn_fit_2.kneighbors is not correct\"\nassert sha1(str(knn_fit_2.kneighbors).encode(\"utf-8\")+b\"1c52d\").hexdigest() == \"55cb84f4cc156f43e6f8f2b5aa8379f34d7d579d\", \"value of knn_fit_2.kneighbors is not correct\"\n\nassert sha1(str(type(knn_fit_2.effective_metric_)).encode(\"utf-8\")+b\"1c52e\").hexdigest() == \"4a071b6a39a1366f1c50cfff6bf5bafe83bd944b\", \"type of knn_fit_2.effective_metric_ is not str. knn_fit_2.effective_metric_ should be an str\"\nassert sha1(str(len(knn_fit_2.effective_metric_)).encode(\"utf-8\")+b\"1c52e\").hexdigest() == \"6949133fe0bc9d23f3f2391308a85eeac883a74a\", \"length of knn_fit_2.effective_metric_ is not correct\"\nassert sha1(str(knn_fit_2.effective_metric_.lower()).encode(\"utf-8\")+b\"1c52e\").hexdigest() == \"fed6a9fc14d942c80421836e802e4f459699e299\", \"value of knn_fit_2.effective_metric_ is not correct\"\nassert sha1(str(knn_fit_2.effective_metric_).encode(\"utf-8\")+b\"1c52e\").hexdigest() == \"fed6a9fc14d942c80421836e802e4f459699e299\", \"correct string value of knn_fit_2.effective_metric_ but incorrect case of letters\"\n\nassert sha1(str(type(type(knn_fit_2))).encode(\"utf-8\")+b\"1c52f\").hexdigest() == \"c0688283eb347b3a716716a2762a46dfa6000cbd\", \"type of type(knn_fit_2) is not correct\"\nassert sha1(str(type(knn_fit_2)).encode(\"utf-8\")+b\"1c52f\").hexdigest() == \"0704b8be28fd8da7859dd6fc8e5148535b75399d\", \"value of type(knn_fit_2) is not correct\"\n\nassert sha1(str(type(knn_fit_2.n_features_in_)).encode(\"utf-8\")+b\"1c530\").hexdigest() == \"3ac6a926c4de23bcae7e9a759aca7cd489f4cad2\", \"type of knn_fit_2.n_features_in_ is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(knn_fit_2.n_features_in_).encode(\"utf-8\")+b\"1c530\").hexdigest() == \"a032550f3adad26547a9e2adaae58acd93a5b36e\", \"value of knn_fit_2.n_features_in_ is not correct\"\n\nassert sha1(str(type(X_2.columns.values)).encode(\"utf-8\")+b\"1c531\").hexdigest() == \"a7b153af47464cec438aaac3a5398422f15c739c\", \"type of X_2.columns.values is not correct\"\nassert sha1(str(X_2.columns.values).encode(\"utf-8\")+b\"1c531\").hexdigest() == \"c3fd80e7a523addffb5f040e8560caf25c601447\", \"value of X_2.columns.values is not correct\"\n\nassert sha1(str(type(y_2.name)).encode(\"utf-8\")+b\"1c532\").hexdigest() == \"6ecd85411910b1c1e49f8fb6a6c446122031f3a3\", \"type of y_2.name is not str. y_2.name should be an str\"\nassert sha1(str(len(y_2.name)).encode(\"utf-8\")+b\"1c532\").hexdigest() == \"ebad5f09e153e6397258881e828d3d859c85154e\", \"length of y_2.name is not correct\"\nassert sha1(str(y_2.name.lower()).encode(\"utf-8\")+b\"1c532\").hexdigest() == \"7c29dd51d86bde999e3497b24a52324ffa88d80b\", \"value of y_2.name is not correct\"\nassert sha1(str(y_2.name).encode(\"utf-8\")+b\"1c532\").hexdigest() == \"ff10a06265a833d1274bee055d40fe11d6c1de8f\", \"correct string value of y_2.name but incorrect case of letters\"\n\nassert sha1(str(type(y_2.values)).encode(\"utf-8\")+b\"1c533\").hexdigest() == \"6701ed81a4e9d4e3d605bff672f33abc4cab49ab\", \"type of y_2.values is not correct\"\nassert sha1(str(y_2.values).encode(\"utf-8\")+b\"1c533\").hexdigest() == \"1eeb2c2596b3d6987df53c982b8921d368bfa856\", \"value of y_2.values is not correct\"\n\nassert sha1(str(type(sum(X_2.Symmetry))).encode(\"utf-8\")+b\"1c534\").hexdigest() == \"401b25657e3ccc6f373b04c3f73b718954f2f87f\", \"type of sum(X_2.Symmetry) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(X_2.Symmetry), 2)).encode(\"utf-8\")+b\"1c534\").hexdigest() == \"10b6bfc8d4273cf2cda252fff3ead0fcdb4a13cd\", \"value of sum(X_2.Symmetry) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(sum(X_2.Radius))).encode(\"utf-8\")+b\"1c535\").hexdigest() == \"bc045aaaa3e654827d701f1a154d82ce3bf159de\", \"type of sum(X_2.Radius) is not float. Please make sure it is float and not np.float64, etc. You can cast your value into a float using float()\"\nassert sha1(str(round(sum(X_2.Radius), 2)).encode(\"utf-8\")+b\"1c535\").hexdigest() == \"ba710607201f7fb546dfbff2bbf8125c43eb507a\", \"value of sum(X_2.Radius) is not correct (rounded to 2 decimal places)\"\n\nassert sha1(str(type(new_obs_2 is None)).encode(\"utf-8\")+b\"1c537\").hexdigest() == \"0e6db0897dec3a5a0be1727ee2174838fed8392d\", \"type of new_obs_2 is None is not bool. new_obs_2 is None should be a bool\"\nassert sha1(str(new_obs_2 is None).encode(\"utf-8\")+b\"1c537\").hexdigest() == \"3bd0c39ffd477eef3d20295014e9603c292a36db\", \"boolean value of new_obs_2 is None is not correct\"\n\nassert sha1(str(type(new_obs_2)).encode(\"utf-8\")+b\"1c538\").hexdigest() == \"dbd7bddf2ba93297c398200c3aba22eee791cc2b\", \"type of type(new_obs_2) is not correct\"\n\nassert sha1(str(type(new_obs_2.Symmetry.values)).encode(\"utf-8\")+b\"1c539\").hexdigest() == \"f025231cf463eb1b53a6eb94ed504ae8eacbd2b3\", \"type of new_obs_2.Symmetry.values is not correct\"\nassert sha1(str(new_obs_2.Symmetry.values).encode(\"utf-8\")+b\"1c539\").hexdigest() == \"ee3ccc35dc76e76f3d3f26ce2af31aba1c00475d\", \"value of new_obs_2.Symmetry.values is not correct\"\n\nassert sha1(str(type(new_obs_2.Radius.values)).encode(\"utf-8\")+b\"1c53a\").hexdigest() == \"88a9bfa1595cca275e5f844a75c8fd0e7e660b85\", \"type of new_obs_2.Radius.values is not correct\"\nassert sha1(str(new_obs_2.Radius.values).encode(\"utf-8\")+b\"1c53a\").hexdigest() == \"71bc38e24dca943a89fdba0e143c0c93a97d9dbd\", \"value of new_obs_2.Radius.values is not correct\"\n\nassert sha1(str(type(new_obs_2.Concavity.values)).encode(\"utf-8\")+b\"1c53b\").hexdigest() == \"581e0fa5a3a53d9c88d6df38ab3e022f128d41bb\", \"type of new_obs_2.Concavity.values is not correct\"\nassert sha1(str(new_obs_2.Concavity.values).encode(\"utf-8\")+b\"1c53b\").hexdigest() == \"6366ed5e9216d5cf0dab850bfd420217af90487c\", \"value of new_obs_2.Concavity.values is not correct\"\n\nassert sha1(str(type(class_prediction_2 is None)).encode(\"utf-8\")+b\"1c53c\").hexdigest() == \"102656310fdb351c06ebfcd34be208a380387416\", \"type of class_prediction_2 is None is not bool. class_prediction_2 is None should be a bool\"\nassert sha1(str(class_prediction_2 is None).encode(\"utf-8\")+b\"1c53c\").hexdigest() == \"2a4bb29e14b26cb4fba6321e00f5ea7264414756\", \"boolean value of class_prediction_2 is None is not correct\"\n\nassert sha1(str(type(class_prediction_2)).encode(\"utf-8\")+b\"1c53d\").hexdigest() == \"0d077a418cd3754263e26adf0dd15e657d98c01a\", \"type of class_prediction_2 is not correct\"\nassert sha1(str(class_prediction_2).encode(\"utf-8\")+b\"1c53d\").hexdigest() == \"fa0463880a9a71ddc10bfae3b027f9f4a6da0038\", \"value of class_prediction_2 is not correct\"\n\nprint('Success!')\n\nQuestion 2.5 {points: 1}\nFinally, we will perform K-nearest neighbour classification again, using the scikit-learn package and K = 7 to classify a new observation where we use all the predictors in our data set (we give you the values in the code below).\nBut we first have to do one important thing: we need to remove the ID variable from the analysis (it‚Äôs not a numerical measurement that we should use for classification). Thankfully, scikit-learn provides a nice way of combining data preprocessing and training into a single consistent pipeline.\nWe will first create a preprocessor to remove the ID variable using the drop preprocessing step. Since we aren‚Äôt doing any preprocessing to other columns, we will set the remainder parameter to passthrough. Do so below using the provided scaffolding. Name the preprocessor object knn_preprocessor.\n\n# ___ = make_column_transformer(\n#     (\"drop\", [___]),\n#     remainder=___\n# )\n\n# your code here\nraise NotImplementedError\nknn_preprocessor\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_preprocessor is None)).encode(\"utf-8\")+b\"c9190\").hexdigest() == \"1aa697ad8121d7a033d091dd1dde11fd9a9048c0\", \"type of knn_preprocessor is None is not bool. knn_preprocessor is None should be a bool\"\nassert sha1(str(knn_preprocessor is None).encode(\"utf-8\")+b\"c9190\").hexdigest() == \"fa97b74142ab349c61853efd4aed109620ea4419\", \"boolean value of knn_preprocessor is None is not correct\"\n\nassert sha1(str(type(type(knn_preprocessor))).encode(\"utf-8\")+b\"c9191\").hexdigest() == \"1109d30d9e1570f804f20d5d028ef6afce897841\", \"type of type(knn_preprocessor) is not correct\"\nassert sha1(str(type(knn_preprocessor)).encode(\"utf-8\")+b\"c9191\").hexdigest() == \"b7f41df01f0ad5de7c336fbe446f33229e39ffba\", \"value of type(knn_preprocessor) is not correct\"\n\nassert sha1(str(type(knn_preprocessor.get_feature_names_out)).encode(\"utf-8\")+b\"c9192\").hexdigest() == \"bb33b66fca24d58fe1cd06f2a2985674ee8f0698\", \"type of knn_preprocessor.get_feature_names_out is not correct\"\nassert sha1(str(knn_preprocessor.get_feature_names_out).encode(\"utf-8\")+b\"c9192\").hexdigest() == \"ee35e154ea1785364b8455f97248689b3f960199\", \"value of knn_preprocessor.get_feature_names_out is not correct\"\n\nprint('Success!')\n\nQuestion 2.6  {points: 1}\nCreate a pipeline that includes the new preprocessor (knn_preprocessor) and the model specification (knn_spec) using the scaffolding below. Name the pipeline object knn_pipeline.\n\n# ___ = make_pipeline(___, ___)\n\n# your code here\nraise NotImplementedError\nknn_pipeline\n\n\nfrom hashlib import sha1\nassert sha1(str(type(knn_pipeline is None)).encode(\"utf-8\")+b\"570b0\").hexdigest() == \"5229d62594055e066352668824c5c3908bb29f74\", \"type of knn_pipeline is None is not bool. knn_pipeline is None should be a bool\"\nassert sha1(str(knn_pipeline is None).encode(\"utf-8\")+b\"570b0\").hexdigest() == \"1d2994bb8a065a370b5d478a8087d37e3841c353\", \"boolean value of knn_pipeline is None is not correct\"\n\nassert sha1(str(type(type(knn_pipeline))).encode(\"utf-8\")+b\"570b1\").hexdigest() == \"4b6b2b6e8dcd91c384d5e7b3ffaa5df5ed5fac76\", \"type of type(knn_pipeline) is not correct\"\nassert sha1(str(type(knn_pipeline)).encode(\"utf-8\")+b\"570b1\").hexdigest() == \"7cff373362fd36faa9d06c119ebecf6820b812f0\", \"value of type(knn_pipeline) is not correct\"\n\nassert sha1(str(type(knn_pipeline.named_steps.kneighborsclassifier.n_neighbors)).encode(\"utf-8\")+b\"570b2\").hexdigest() == \"c0cf487b545aea403a76ddbbedba6f508f1e6b8f\", \"type of knn_pipeline.named_steps.kneighborsclassifier.n_neighbors is not int. Please make sure it is int and not np.int64, etc. You can cast your value into an int using int()\"\nassert sha1(str(knn_pipeline.named_steps.kneighborsclassifier.n_neighbors).encode(\"utf-8\")+b\"570b2\").hexdigest() == \"0c675a0ab79e6e0c763779b13443212fca47db27\", \"value of knn_pipeline.named_steps.kneighborsclassifier.n_neighbors is not correct\"\n\nprint('Success!')\n\nQuestion 2.7 {points: 1}\nFinally, fit the pipeline and predict the class label for the new observation named new_obs_all. Name the fit object knn_fit_all, and the class prediction class_prediction_all. Name the new predictor as X_3 and the new target as y_3.\n\nnew_obs_all = pd.DataFrame(\n    [[None, 0, 0, 0, 0, 0.5, 0, 1, 0, 1, 0]],\n    columns=[\n        \"ID\",\n        \"Radius\",\n        \"Texture\",\n        \"Perimeter\",\n        \"Area\",\n        \"Smoothness\",\n        \"Compactness\",\n        \"Concavity\",\n        \"Concave_points\",\n        \"Symmetry\",\n        \"Fractal_dimension\",\n    ],\n)\n# X_3 = cancer.drop(columns=[___])\n# y_3 = cancer[___]\n# ___ = knn_pipeline.fit(___, ___)\n# ___ = knn_fit_all.____(____)\n\n# your code here\nraise NotImplementedError\nclass_prediction_all\n\n\nfrom hashlib import sha1\nassert sha1(str(type(class_prediction_all)).encode(\"utf-8\")+b\"bdab9\").hexdigest() == \"6a39e4695fdfb5df240814e4336bf420375f8ab0\", \"type of class_prediction_all is not correct\"\nassert sha1(str(class_prediction_all).encode(\"utf-8\")+b\"bdab9\").hexdigest() == \"f3a6c0bfa60af2a0f2398b36ffe3f27359ac2478\", \"value of class_prediction_all is not correct\"\n\nprint('Success!')"
  },
  {
    "objectID": "materials/slides/classification2.html#session-learning-objectives",
    "href": "materials/slides/classification2.html#session-learning-objectives",
    "title": "Classification II: evaluation & tuning",
    "section": "Session learning objectives",
    "text": "Session learning objectives\n\n\n\n\n\n\nBy the end of the session, learners will be able to do the following:\n\nDescribe what training, validation, and test data sets are and how they are used in classification.\nSplit data into training, validation, and test data sets.\nDescribe what a random seed is and its importance in reproducible data analysis.\nSet the random seed in Python using the numpy.random.seed function.\nDescribe and interpret accuracy, precision, recall, and confusion matrices.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#session-learning-objectives-contd",
    "href": "materials/slides/classification2.html#session-learning-objectives-contd",
    "title": "Classification II: evaluation & tuning",
    "section": "Session learning objectives cont‚Äôd",
    "text": "Session learning objectives cont‚Äôd\nBy the end of the session, learners will be able to do the following:\n\nEvaluate classification accuracy, precision, and recall in Python using a test set, a single validation set, and cross-validation.\nProduce a confusion matrix in Python.\nChoose the number of neighbors in a K-nearest neighbors classifier by maximizing estimated cross-validation accuracy.\nDescribe underfitting and overfitting, and relate it to the number of neighbors in K-nearest neighbors classification.\nDescribe the advantages and disadvantages of the K-nearest neighbors classification algorithm.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#evaluating-performance",
    "href": "materials/slides/classification2.html#evaluating-performance",
    "title": "Classification II: evaluation & tuning",
    "section": "Evaluating performance",
    "text": "Evaluating performance\n\nSometimes our classifier might make the wrong prediction.\nA classifier does not need to be right 100% of the time to be useful, though we don‚Äôt want the classifier to make too many wrong predictions.\nHow do we measure how ‚Äúgood‚Äù our classifier is?",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#data-splitting",
    "href": "materials/slides/classification2.html#data-splitting",
    "title": "Classification II: evaluation & tuning",
    "section": "Data splitting",
    "text": "Data splitting\n\nThe trick is to split the data into a training set and test set.\nOnly the training set when building the classifier.\nTo evaluate the performance of the classifier, we first set aside the labels from the test set, and then use the classifier to predict the labels in the test set.\nIf our predictions match the actual labels for the observations in the test set, then we have some confidence that our classifier might also accurately predict the class labels for new observations without known class labels.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#splitting-the-data-into-training-and-testing-sets",
    "href": "materials/slides/classification2.html#splitting-the-data-into-training-and-testing-sets",
    "title": "Classification II: evaluation & tuning",
    "section": "Splitting the data into training and testing sets",
    "text": "Splitting the data into training and testing sets",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#prediction-accuracy",
    "href": "materials/slides/classification2.html#prediction-accuracy",
    "title": "Classification II: evaluation & tuning",
    "section": "Prediction accuracy",
    "text": "Prediction accuracy",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#is-knowing-accuracy-enough",
    "href": "materials/slides/classification2.html#is-knowing-accuracy-enough",
    "title": "Classification II: evaluation & tuning",
    "section": "Is knowing accuracy enough?",
    "text": "Is knowing accuracy enough?\n\nExample accuracy calculation:\n\n\\[\\mathrm{accuracy} = \\frac{\\mathrm{number \\; of  \\; correct  \\; predictions}}{\\mathrm{total \\;  number \\;  of  \\; predictions}} = \\frac{58}{65} = 0.892\\]\n\nPrediction accuracy only tells us how often the classifier makes mistakes in general, but does not tell us anything about the kinds of mistakes the classifier makes.\nThe confusion matrix tells a more complete story.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#example-confusion-matrix-for-the-breast-cancer-data",
    "href": "materials/slides/classification2.html#example-confusion-matrix-for-the-breast-cancer-data",
    "title": "Classification II: evaluation & tuning",
    "section": "Example confusion matrix for the breast cancer data:",
    "text": "Example confusion matrix for the breast cancer data:\n\n\n\n\n\n\n\n\n\nPredicted Malignant\nPredicted Benign\n\n\n\n\nActually Malignant\n1\n3\n\n\nActually Benign\n4\n57\n\n\n\n\nTrue Positive: A malignant observation that was classified as malignant (top left).\nFalse Positive: A benign observation that was classified as malignant (bottom left).\nTrue Negative: A benign observation that was classified as benign (bottom right).\nFalse Negative: A malignant observation that was classified as benign (top right).",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#precision-recall",
    "href": "materials/slides/classification2.html#precision-recall",
    "title": "Classification II: evaluation & tuning",
    "section": "Precision & recall",
    "text": "Precision & recall\n\nPrecision quantifies how many of the positive predictions the classifier made were actually positive.\n\n\\[\\mathrm{precision} = \\frac{\\mathrm{number \\; of  \\; correct \\; positive \\; predictions}}{\\mathrm{total \\;  number \\;  of \\; positive  \\; predictions}}\\]\n\nRecall quantifies how many of the positive observations in the test set were identified as positive.\n\n\\[\\mathrm{recall} = \\frac{\\mathrm{number \\; of  \\; correct  \\; positive \\; predictions}}{\\mathrm{total \\;  number \\;  of  \\; positive \\; test \\; set \\; observations}}\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#precision-and-recall-for-the-breast-cancer-data-set-example",
    "href": "materials/slides/classification2.html#precision-and-recall-for-the-breast-cancer-data-set-example",
    "title": "Classification II: evaluation & tuning",
    "section": "Precision and recall for the breast cancer data set example",
    "text": "Precision and recall for the breast cancer data set example\n\n\n\n\n\n\n\n\n\nPredicted Malignant\nPredicted Benign\n\n\n\n\nActually Malignant\n1\n3\n\n\nActually Benign\n4\n57\n\n\n\n\\[\\mathrm{precision} = \\frac{1}{1+4} = 0.20, \\quad \\mathrm{recall} = \\frac{1}{1+3} = 0.25\\]\n\nSo even with an accuracy of 89%, the precision and recall of the classifier were both relatively low. For this data analysis context, recall is particularly important: if someone has a malignant tumor, we certainly want to identify it. A recall of just 25% would likely be unacceptable!",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#randomness-and-seeds",
    "href": "materials/slides/classification2.html#randomness-and-seeds",
    "title": "Classification II: evaluation & tuning",
    "section": "Randomness and seeds",
    "text": "Randomness and seeds\n\nOur data analyses will often involve the use of randomness\nWe use randomness any time we need to make a decision in our analysis that needs to be fair, unbiased, and not influenced by human input (e.g., splitting into training and test sets).\nHowever, the use of randomness runs counter to one of the main tenets of good data analysis practice: reproducibility‚Ä¶\nThe trick is that in Python‚Äîand other programming languages‚Äîrandomness is not actually random! Instead, Python uses a random number generator that produces a sequence of numbers that are completely determined by a seed value.\nOnce you set the seed value, everything after that point may look random, but is actually totally reproducible.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#setting-the-seed-in-python",
    "href": "materials/slides/classification2.html#setting-the-seed-in-python",
    "title": "Classification II: evaluation & tuning",
    "section": "Setting the seed in Python",
    "text": "Setting the seed in Python\nLet‚Äôs say we want to make a series object containing the integers from 0 to 9. And then we want to randomly pick 10 numbers from that list, but we want it to be reproducible.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#setting-the-seed-in-python-contd",
    "href": "materials/slides/classification2.html#setting-the-seed-in-python-contd",
    "title": "Classification II: evaluation & tuning",
    "section": "Setting the seed in Python (cont‚Äôd)",
    "text": "Setting the seed in Python (cont‚Äôd)\nReminder of the list of numbers we just generated:\n\n\n\n\n\n\n\nIf we run the sample method again, we will get a fresh batch of 10 numbers that also look random.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#setting-the-seed-in-python-contd-1",
    "href": "materials/slides/classification2.html#setting-the-seed-in-python-contd-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Setting the seed in Python (cont‚Äôd)",
    "text": "Setting the seed in Python (cont‚Äôd)\nLet‚Äôs regenerate original set of 10 random numbers by setting the seed to 1:\n\n\n\n\n\n\n\nIf we choose a different value for the seed‚Äîsay, 4235‚Äîwe obtain a different sequence of random numbers:",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#back-to-the-breast-cancer-data-set-example",
    "href": "materials/slides/classification2.html#back-to-the-breast-cancer-data-set-example",
    "title": "Classification II: evaluation & tuning",
    "section": "Back to the breast cancer data set example",
    "text": "Back to the breast cancer data set example\nLoad packages and set seed:",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#create-the-train-test-split",
    "href": "materials/slides/classification2.html#create-the-train-test-split",
    "title": "Classification II: evaluation & tuning",
    "section": "Create the train / test split",
    "text": "Create the train / test split\n\nBefore fitting any models, or doing exploratory data analysis, it is critical that you split the data into training and test sets.\nTypically, the training set is between 50% and 95% of the data, while the test set is the remaining 5% to 50%.\nThe train_test_split function from scikit-learn handles the procedure of splitting the data for us.\nUse shuffle=True to remove the influence of order in the data set.\nSet the stratify parameter to be the response variable to ensure the same proportion of each class ends up in both the training and testing sets.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#splitting-the-breast-cancer-data-set",
    "href": "materials/slides/classification2.html#splitting-the-breast-cancer-data-set",
    "title": "Classification II: evaluation & tuning",
    "section": "Splitting the breast cancer data set",
    "text": "Splitting the breast cancer data set\n\nSplit the data so 75% are in the training set, and 25% in the test set\nData are shuffled\nSplit is stratified on the Class variable",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#checking-the-splits",
    "href": "materials/slides/classification2.html#checking-the-splits",
    "title": "Classification II: evaluation & tuning",
    "section": "Checking the splits",
    "text": "Checking the splits\n\nLet‚Äôs look at the training and test splits (in practice you look at both)\nWe can see our class proportions were roughly preserved when we split the data.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#preprocessing-with-data-splitting",
    "href": "materials/slides/classification2.html#preprocessing-with-data-splitting",
    "title": "Classification II: evaluation & tuning",
    "section": "Preprocessing with data splitting",
    "text": "Preprocessing with data splitting\n\nMany machine learning models are sensitive to the scale of the predictors, and even if not, comparison of importance of features for prediction after fitting requires scaling.\nWhen preprocessing the data (scaling is part of this), it is critical that we use only the training set in creating the mathematical function to do this.\nIf this is not done, we will get overly optimistic test accuracy, as our test data will have influenced our model.\nAfter creating the preprocessing function, we can then apply it separately to both the training and test data sets.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#preprocessing-with-scikit-learn",
    "href": "materials/slides/classification2.html#preprocessing-with-scikit-learn",
    "title": "Classification II: evaluation & tuning",
    "section": "Preprocessing with scikit-learn",
    "text": "Preprocessing with scikit-learn\n\nscikit-learn helps us handle this properly as long as we wrap our analysis steps in a Pipeline.\nSpecifically, we construct and prepare the preprocessor using make_column_transformer, specifying the type of tranformation we want to apply, as well as which columns to apply it to:",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#train-the-classifier",
    "href": "materials/slides/classification2.html#train-the-classifier",
    "title": "Classification II: evaluation & tuning",
    "section": "Train the classifier",
    "text": "Train the classifier\n\nNow we can create our K-nearest neighbors classifier with only the training set.\nFor simplicity, we will just choose \\(K\\) = 3, and use only the concavity and smoothness predictors.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#predict-the-labels-in-the-test-set",
    "href": "materials/slides/classification2.html#predict-the-labels-in-the-test-set",
    "title": "Classification II: evaluation & tuning",
    "section": "Predict the labels in the test set",
    "text": "Predict the labels in the test set\nNow that we have a K-nearest neighbors classifier object, we can use it to predict the class labels for our test set:",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#evaluate-performance",
    "href": "materials/slides/classification2.html#evaluate-performance",
    "title": "Classification II: evaluation & tuning",
    "section": "Evaluate performance",
    "text": "Evaluate performance\nTo evaluate the model, we will look at:\n\naccuracy\nprecision\nrecall\nconfusion matrix\ncompare to baseline model (majority classifier)\n\nAll of these together, will help us develop a fuller picture of how the model is performing, as opposed to only evaluating the model based on a single metric or table.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#accuracy",
    "href": "materials/slides/classification2.html#accuracy",
    "title": "Classification II: evaluation & tuning",
    "section": "Accuracy",
    "text": "Accuracy",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#precision",
    "href": "materials/slides/classification2.html#precision",
    "title": "Classification II: evaluation & tuning",
    "section": "Precision",
    "text": "Precision",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#recall",
    "href": "materials/slides/classification2.html#recall",
    "title": "Classification II: evaluation & tuning",
    "section": "Recall",
    "text": "Recall",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#confusion-matrix",
    "href": "materials/slides/classification2.html#confusion-matrix",
    "title": "Classification II: evaluation & tuning",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nThe Pandas crosstab function takes two arguments: the actual labels first, then the predicted labels second.\nNote that crosstab orders its columns alphabetically, but the positive label is still Malignant, even if it is not in the top left corner as in the table shown earlier.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#critically-analyze-performance",
    "href": "materials/slides/classification2.html#critically-analyze-performance",
    "title": "Classification II: evaluation & tuning",
    "section": "Critically analyze performance",
    "text": "Critically analyze performance\n\nIs 90% accuracy, a precision of 83% and a recall of 91% good enough?\nTo get a sense of scale, we often compare our model to a baseline model. In the case of classification, this would be the majority classifier (always guesses the majority class label from the training data).\nFor the breast cancer training data, the baseline classifier‚Äôs accuracy would be 63%",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#critically-analyze-performance-contd",
    "href": "materials/slides/classification2.html#critically-analyze-performance-contd",
    "title": "Classification II: evaluation & tuning",
    "section": "Critically analyze performance (cont‚Äôd)",
    "text": "Critically analyze performance (cont‚Äôd)\n\nSo we do see that our model is doing a LOT better than the baseline, which is great, but considering our application domain is in cancer diagnosis, we still have a ways to go‚Ä¶\nAnalyzing model performance really depends on your application!",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#tuning-the-classifier",
    "href": "materials/slides/classification2.html#tuning-the-classifier",
    "title": "Classification II: evaluation & tuning",
    "section": "Tuning the classifier",
    "text": "Tuning the classifier\n\nMost predictive models in statistics and machine learning have parameters (a number you have to pick in advance that determines some aspect of how the model behaves).\nFor our working example, \\(K\\)-nearest neighbors classification algorithm, \\(K\\) is a parameter that we have to pick that determines how many neighbors participate in the class vote.\nHow do we choose \\(K\\), or any parameter for other models?\nData splitting!",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#validation-set",
    "href": "materials/slides/classification2.html#validation-set",
    "title": "Classification II: evaluation & tuning",
    "section": "Validation set",
    "text": "Validation set\n\nCannot use the test set to choose the parameter!\nBut we can split the training set into two partitions, a traning set and a validation set.\nFor each parameter value we want to assess, we can fit on the training set, and evaluate on the validation set.\nThen after we find the best value for our parameter, we can refit the model with the best parameter on the entire training set and then evaluate our model on the test set.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#validation-set-1",
    "href": "materials/slides/classification2.html#validation-set-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Validation set",
    "text": "Validation set",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#can-we-do-better",
    "href": "materials/slides/classification2.html#can-we-do-better",
    "title": "Classification II: evaluation & tuning",
    "section": "Can we do better?",
    "text": "Can we do better?\n\nDepending on how we split the data into the training and validation sets, we might get a lucky split (or an unlucky one) that doesn‚Äôt give us a good estimate of the model‚Äôs true accuracy.\nIn many cases, we can do better by making many splits, and averaging the accuracy scores to get a better estimate.\nWe call this cross-validation.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#cross-validation-with-five-folds",
    "href": "materials/slides/classification2.html#cross-validation-with-five-folds",
    "title": "Classification II: evaluation & tuning",
    "section": "Cross-validation with five folds",
    "text": "Cross-validation with five folds",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#cross-validation-in-scikit-learn",
    "href": "materials/slides/classification2.html#cross-validation-in-scikit-learn",
    "title": "Classification II: evaluation & tuning",
    "section": "Cross-validation in scikit-learn",
    "text": "Cross-validation in scikit-learn\n\nUse the scikit-learn cross_validate function.\nNeed to specify:\n\na modelling Pipeline as the estimator argument,\nthe number of folds as the cv argument,\nthe training data predictors as the X argument\nthe labels as the y arguments.\n\nNote that the cross_validate function handles stratifying the classes in each train and validate fold automatically.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#cross-validation-in-scikit-learn-contd",
    "href": "materials/slides/classification2.html#cross-validation-in-scikit-learn-contd",
    "title": "Classification II: evaluation & tuning",
    "section": "Cross-validation in scikit-learn (cont‚Äôd)",
    "text": "Cross-validation in scikit-learn (cont‚Äôd)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#cross-validation-in-scikit-learn-contd-1",
    "href": "materials/slides/classification2.html#cross-validation-in-scikit-learn-contd-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Cross-validation in scikit-learn (cont‚Äôd)",
    "text": "Cross-validation in scikit-learn (cont‚Äôd)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#parameter-value-selection",
    "href": "materials/slides/classification2.html#parameter-value-selection",
    "title": "Classification II: evaluation & tuning",
    "section": "Parameter value selection",
    "text": "Parameter value selection\n\nSince cross-validation helps us evaluate the accuracy of our classifier, we can use cross-validation to calculate an accuracy for each value of our parameter, here \\(K\\), in a reasonable range.\nThen we pick the value of \\(K\\) that gives us the best accuracy, and refit the model with our parameter on the training data, and then evaluate on the test data.\nThe scikit-learn package collection provides built-in functionality, named GridSearchCV, to automatically handle the details for us.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#parameter-value-selection-1",
    "href": "materials/slides/classification2.html#parameter-value-selection-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Parameter value selection",
    "text": "Parameter value selection",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#parameter-value-selection-2",
    "href": "materials/slides/classification2.html#parameter-value-selection-2",
    "title": "Classification II: evaluation & tuning",
    "section": "Parameter value selection",
    "text": "Parameter value selection\nNow we use the fit method on the GridSearchCV object to begin the tuning process.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#parameter-value-selection-3",
    "href": "materials/slides/classification2.html#parameter-value-selection-3",
    "title": "Classification II: evaluation & tuning",
    "section": "Parameter value selection",
    "text": "Parameter value selection",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#visualize-paramter-value-selection",
    "href": "materials/slides/classification2.html#visualize-paramter-value-selection",
    "title": "Classification II: evaluation & tuning",
    "section": "Visualize paramter value selection",
    "text": "Visualize paramter value selection\n\naccuracy_vs_k = (\n    alt.Chart(accuracies_grid)\n    .mark_line(point=True)\n    .encode(\n        x=alt.X(\"n_neighbors\")\n        .title(\"Neighbors\"),\n        y=alt.Y(\"mean_test_score\")\n        .scale(zero=False)\n        .title(\"Accuracy estimate\")\n    )\n)",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#visualize-paramter-value-selection-1",
    "href": "materials/slides/classification2.html#visualize-paramter-value-selection-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Visualize paramter value selection",
    "text": "Visualize paramter value selection",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#best-parameter-value",
    "href": "materials/slides/classification2.html#best-parameter-value",
    "title": "Classification II: evaluation & tuning",
    "section": "Best parameter value",
    "text": "Best parameter value\nWe can also obtain the number of neighbours with the highest accuracy programmatically by accessing the best_params_ attribute of the fit GridSearchCV object.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#best-parameter-value-1",
    "href": "materials/slides/classification2.html#best-parameter-value-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Best parameter value",
    "text": "Best parameter value\nDo we use \\(K\\) = 36?\nGenerally, when selecting a parameters, we are looking for a value where:\n\nwe get roughly optimal accuracy\nchanging the value to a nearby one doesn‚Äôt change the accuracy too much\nthe cost of training the model is not prohibitive",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#best-parameter-value-2",
    "href": "materials/slides/classification2.html#best-parameter-value-2",
    "title": "Classification II: evaluation & tuning",
    "section": "Best parameter value",
    "text": "Best parameter value",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#underoverfitting",
    "href": "materials/slides/classification2.html#underoverfitting",
    "title": "Classification II: evaluation & tuning",
    "section": "Under/Overfitting",
    "text": "Under/Overfitting\n\nWhat happens if we keep increasing the number of neighbors \\(K\\)?\nThe cross-validation accuracy estimate actually starts to decrease!",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#evaluating-on-the-test-set",
    "href": "materials/slides/classification2.html#evaluating-on-the-test-set",
    "title": "Classification II: evaluation & tuning",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\n\nBefore we evaluate on the test set, we need to refit the model using the best parameter(s) on the entire training set\nLuckily, scikit-learn does it for us automatically!\nTo make predictions and assess the estimated accuracy of the best model on the test data, we can use the score and predict methods of the fit GridSearchCV object.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#evaluating-on-the-test-set-1",
    "href": "materials/slides/classification2.html#evaluating-on-the-test-set-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Evaluating on the test set",
    "text": "Evaluating on the test set\nHow well might our classifier do on unseen data?\nTo find out we can then pass those predictions to the precision, recall, and crosstab functions to assess the estimated precision and recall, and print a confusion matrix.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#accuracy-1",
    "href": "materials/slides/classification2.html#accuracy-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Accuracy",
    "text": "Accuracy",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#precision-1",
    "href": "materials/slides/classification2.html#precision-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Precision",
    "text": "Precision",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#recall-1",
    "href": "materials/slides/classification2.html#recall-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Recall",
    "text": "Recall",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#confusion-matrix-1",
    "href": "materials/slides/classification2.html#confusion-matrix-1",
    "title": "Classification II: evaluation & tuning",
    "section": "Confusion matrix",
    "text": "Confusion matrix",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#summary",
    "href": "materials/slides/classification2.html#summary",
    "title": "Classification II: evaluation & tuning",
    "section": "Summary",
    "text": "Summary",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#k-nearest-neighbors-classification-algorithm",
    "href": "materials/slides/classification2.html#k-nearest-neighbors-classification-algorithm",
    "title": "Classification II: evaluation & tuning",
    "section": "K-nearest neighbors classification algorithm",
    "text": "K-nearest neighbors classification algorithm\nStrengths: K-nearest neighbors classification\n\nis a simple, intuitive algorithm,\nrequires few assumptions about what the data must look like, and\nworks for binary (two-class) and multi-class (more than 2 classes) classification problems.\n\nWeaknesses: K-nearest neighbors classification\n\nbecomes very slow as the training data gets larger,\nmay not perform well with a large number of predictors, and\nmay not perform well when classes are imbalanced.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#other-classification-algorithms",
    "href": "materials/slides/classification2.html#other-classification-algorithms",
    "title": "Classification II: evaluation & tuning",
    "section": "Other classification algorithms",
    "text": "Other classification algorithms\n\nscikit-learn classification documentation: https://scikit-learn.org/stable/supervised_learning.html",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#additional-resources",
    "href": "materials/slides/classification2.html#additional-resources",
    "title": "Classification II: evaluation & tuning",
    "section": "Additional resources",
    "text": "Additional resources\n\nThe Classification II: evaluation & tuning chapter of Data Science: A First Introduction (Python Edition) by Tiffany Timbers, Trevor Campbell, Melissa Lee, Joel Ostblom, Lindsey Heagy contains all the content presented here with a detailed narrative.\nThe scikit-learn website is an excellent reference for more details on, and advanced usage of, the functions and packages in the past two chapters. Aside from that, it also offers many useful tutorials to get you started.\nAn Introduction to Statistical Learning provides a great next stop in the process of learning about classification. Chapter 4 discusses additional basic techniques for classification that we do not cover, such as logistic regression, linear discriminant analysis, and naive Bayes.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/classification2.html#references",
    "href": "materials/slides/classification2.html#references",
    "title": "Classification II: evaluation & tuning",
    "section": "References",
    "text": "References\nEvelyn Martin Lansdowne Beale, Maurice George Kendall, and David Mann. The discarding of variables in multivariate analysis. Biometrika, 54(3-4):357‚Äì366, 1967.\nNorman Draper and Harry Smith. Applied Regression Analysis. Wiley, 1966.\nM. Eforymson. Stepwise regression‚Äîa backward and forward look. In Eastern Regional Meetings of the Institute of Mathematical Statistics. 1966.\nRonald Hocking and R. N. Leslie. Selection of the best subset in regression analysis. Technometrics, 9(4):531‚Äì540, 1967.\nGareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning. Springer, 1st edition, 2013. URL: https://www.statlearning.com/.\nWes McKinney. Python for data analysis: Data wrangling with Pandas, NumPy, and IPython. ‚Äù O‚ÄôReilly Media, Inc.‚Äù, 2012.\nWilliam Nick Street, William Wolberg, and Olvi Mangasarian. Nuclear feature extraction for breast tumor diagnosis. In International Symposium on Electronic Imaging: Science and Technology. 1993.",
    "crumbs": [
      "Home",
      "Slides",
      "Classification II: evaluation & tuning"
    ]
  },
  {
    "objectID": "materials/slides/intro.html#introductions",
    "href": "materials/slides/intro.html#introductions",
    "title": "Welcome",
    "section": "Introductions",
    "text": "Introductions\n\n\n\nTiffany Timbers\nUniversity of British Columbia\n\n\nDaniel Chen\nUniversity of British Columbia"
  },
  {
    "objectID": "materials/slides/intro.html#introduce-yourself",
    "href": "materials/slides/intro.html#introduce-yourself",
    "title": "Welcome",
    "section": "Introduce yourself",
    "text": "Introduce yourself\nWe wont go around the room, but take the next couple of minutes to introduce yourself to your neighbors.\nSome suggested topics:\n\nWhat is your name\nWhere you are coming from\nWhy you are interested in learning ML in Python"
  },
  {
    "objectID": "materials/slides/intro.html#materials",
    "href": "materials/slides/intro.html#materials",
    "title": "Welcome",
    "section": "Materials",
    "text": "Materials\n\n\nWebsite:\n\nhttps://posit-conf-2024.github.io/ml-python/\n\n\nGitHub:\n\nhttps://github.com/posit-conf-2024/ml-python"
  },
  {
    "objectID": "materials/slides/intro.html#schedule",
    "href": "materials/slides/intro.html#schedule",
    "title": "Welcome",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n09:00 - 10:30\nSession 1 - Introduction to workshop and intro to machine learning (via Classification I)\n\n\n10:30 - 11:00\nCoffee break\n\n\n11:00 - 12:30\nSession 2 - Classification II\n\n\n12:30 - 13:30\nLunch break\n\n\n13:30 - 15:00\nSession 3 - Regression\n\n\n15:00 - 15:30\nCoffee break\n\n\n15:30 - 17:00\nSession 4 - Tree-based and ensemble methods"
  },
  {
    "objectID": "materials/slides/intro.html#wifi",
    "href": "materials/slides/intro.html#wifi",
    "title": "Welcome",
    "section": "WiFi",
    "text": "WiFi\n\n\nUsername:\n\nPosit Conf 2024\n\nPassword:\n\nconf2024\n\n\n\nIf you have any difficulty with your connection please let us (myself and or the TAs) know so we can escalate issues if needed."
  },
  {
    "objectID": "materials/slides/intro.html#code-of-conduct",
    "href": "materials/slides/intro.html#code-of-conduct",
    "title": "Welcome",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nThe Code of Conduct can be found at posit.co/code-of-conduct.\nPlease review it carefully.\nYou can report Code of Conduct violations in person, by email, or by phone.\nPlease see the policy linked above for contact information."
  },
  {
    "objectID": "materials/slides/intro.html#other-useful-info",
    "href": "materials/slides/intro.html#other-useful-info",
    "title": "Welcome",
    "section": "Other useful info",
    "text": "Other useful info\n\nThere are gender-neutral bathrooms located on floors 3, 4, 5, 6, and 7.\nThe meditation and prayer room is Room 503, it is available Mon & Tues 7am - 7pm, and Wed 7am - 5pm.\nThe lactation room is located in 509, with the same timings as above.\nParticipants who do not wish to be photographed have red lanyards; please note everyone‚Äôs lanyard colors before taking a photo and respect their choices."
  },
  {
    "objectID": "materials/slides/intro.html#asking-for-help-stickies",
    "href": "materials/slides/intro.html#asking-for-help-stickies",
    "title": "Welcome",
    "section": "Asking for help (Stickies)",
    "text": "Asking for help (Stickies)\n\n\n\n\n\nI‚Äôm working\n\n\n\n\n\nI‚Äôm stuck\n\n\n\n\n\nI‚Äôm done"
  },
  {
    "objectID": "materials/slides/intro.html#other-communication-discord",
    "href": "materials/slides/intro.html#other-communication-discord",
    "title": "Welcome",
    "section": "Other communication (Discord)",
    "text": "Other communication (Discord)\nYou should have received an email with an invitation and instructions for joining the conference‚Äôs discord server.\nThis workshop has a private channel under Workshops,\n\n#workshop-ml-python\n\nThis is a great place to ask questions, post resources, memes, or most anything else before, during, and after the workshop."
  },
  {
    "objectID": "materials/slides/intro.html#rstudio-cloud",
    "href": "materials/slides/intro.html#rstudio-cloud",
    "title": "Welcome",
    "section": "RStudio Cloud",
    "text": "RStudio Cloud\nLink in your welcome email (from last night!) and/or discord (pinned/first comment)\nOnce you have joined you‚Äôre ready to go! We will work with these at the end of each lecture component.\n\n\n\n\n\n\nwhich should then create a copy of all materials and launch a cloud session for you."
  },
  {
    "objectID": "materials/slides/intro.html#cloud-session",
    "href": "materials/slides/intro.html#cloud-session",
    "title": "Welcome",
    "section": "Cloud session",
    "text": "Cloud session\nIf everything is working you should see something very close to the following,"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 ‚Äì Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter‚Äôs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 ‚Äì Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor ‚Äì Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. __Additional offer from the Licensor ‚Äì Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter‚Äôs License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 ‚Äì License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter‚Äôs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter‚Äôs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter‚Äôs License You apply.\n\n\n\nSection 4 ‚Äì Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 ‚Äì Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 ‚Äì Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 ‚Äì Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the ‚ÄúLicensor.‚Äù Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark ‚ÄúCreative Commons‚Äù or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  }
]